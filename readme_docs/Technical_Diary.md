# ğŸ“˜ Technical Diary â€” Fuel Intelligence Report

This document captures the **technical journey**, **challenges**, and **solutions** during the Fuel Intelligence Report build. It's intended for recruiters, collaborators, or certification reviewers to understand the depth of knowledge demonstrated in this project.

---

## ğŸ“… Day 1â€“2: Setting Up the Environment

### âœ… Actions Taken:

* Created 4 source CSVs: `clients.csv`, `drivers.csv`, `fuels.csv`, `pump_loads.csv`
* Uploaded to Azure Data Lake Storage Gen2 (`fuelintelligencestorage` â†’ `fueldata` container)

### âš ï¸ Challenge:

Couldnâ€™t validate the uploaded files directly through Synapse â€” needed preview.

### ğŸ’¡ Solution:

Opened in Storage Explorer first, then validated using `OPENROWSET` SQL after transformation.

---

## ğŸ“… Day 3: Azure Databricks Setup and Notebook Writing

### âœ… Actions Taken:

* Created cluster (`Standard_DS3_v2`)
* Authenticated Azure Data Lake Gen2 with App Registration

### âš ï¸ Challenge:

Could not read from storage â€” error `403: This request is not authorized`

### ğŸ’¡ Solution:

* Created an App Registration `FuelDatabricksAccess`
* Added `Storage Blob Data Contributor` role to the service principal
* Configured Spark with:

```python
spark.conf.set("fs.azure.account.auth.type...", "OAuth")
spark.conf.set("fs.azure.account.oauth2.client.id", "<client_id>")
... etc
```

* Used OAuth token endpoint for proper identity

---

## ğŸ“… Day 4: Data Transformation in Notebook

### âœ… Data Joined and Written to:

* `/fueldata/final/fuel_report.parquet`

### ğŸ“„ Parquet Validation:

* Created `validate_parquet.sql`:

```sql
SELECT * FROM OPENROWSET(... FORMAT='PARQUET')
```

### âœ… Curated Outputs:

* `top_drivers.parquet`
* `fuel_cost_per_client.parquet`
* `fuel_type_usage.parquet`

---

## ğŸ“… Day 5: Automating With Synapse + Pipelines

### âœ… Created:

* Azure Synapse pipeline `Refresh_FuelReport_Weekly`
* Used **Web Activity** to call **Databricks REST API**
* Scheduled with **Weekly Trigger**

### âš ï¸ Challenge:

Couldnâ€™t use direct Synapse â†’ Databricks activity (incompatibility with runtime 15.4)

### ğŸ’¡ Solution:

Used manual `POST` call to:

```
https://<workspace-url>/api/2.1/jobs/run-now
```

With bearer token from service principal

---

## ğŸ“· Screenshots Captured for Evidence:

* âœ… `upload_to_datalake.png`
* âœ… `notebook_execution.png`
* âœ… `job_trigger_synapse.png`

---

## âœ… Summary of Outputs

* 1 Notebook: `1_ingest_to_datalake.ipynb`
* 4 SQLs in `/queries` folder
* 3 Curated Parquet files in `/curated/`
* Full working Synapse pipeline
* Real OAuth + Azure Role-based Access

---

## ğŸ“Œ Next Steps

* Load curated Parquet files into Snowflake warehouse
* Continue Power Platform integrations:

  * Power Apps dashboards for executives
  * Power Automate flows for fuel alerts
  * Canva summaries for CEOs

---

## ğŸ‘¨â€ğŸ’» Author

**Movlan Aliyev**
GitHub: \[Your Profile] | Contact: [robert.movlan@outlook.com](mailto:robert.movlan@outlook.com)

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a45efcca-c7a6-47c8-9f55-97edcaff2ffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mExecutionError\u001b[0m                            Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-7454861199995879>, line 16\u001b[0m\n",
       "\u001b[1;32m     10\u001b[0m spark\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mset(\n",
       "\u001b[1;32m     11\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfs.azure.account.key.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstorage_account_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.blob.core.windows.net\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
       "\u001b[1;32m     12\u001b[0m     storage_account_key\n",
       "\u001b[1;32m     13\u001b[0m )\n",
       "\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# âœ… Mount the blob container to DBFS (Databricks File System)\u001b[39;00m\n",
       "\u001b[0;32m---> 16\u001b[0m dbutils\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mmount(\n",
       "\u001b[1;32m     17\u001b[0m     source \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwasbs://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontainer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstorage_account_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.blob.core.windows.net/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
       "\u001b[1;32m     18\u001b[0m     mount_point \u001b[38;5;241m=\u001b[39m mount_point,\n",
       "\u001b[1;32m     19\u001b[0m     extra_configs \u001b[38;5;241m=\u001b[39m {\n",
       "\u001b[1;32m     20\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfs.azure.account.key.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstorage_account_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.blob.core.windows.net\u001b[39m\u001b[38;5;124m\"\u001b[39m: storage_account_key\n",
       "\u001b[1;32m     21\u001b[0m     }\n",
       "\u001b[1;32m     22\u001b[0m )\n",
       "\n",
       "File \u001b[0;32m/databricks/python_shell/lib/dbruntime/dbutils.py:158\u001b[0m, in \u001b[0;36mprettify_exception_message.<locals>.f_with_exception_handling\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m    156\u001b[0m exc\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[1;32m    157\u001b[0m exc\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
       "\n",
       "\u001b[0;31mExecutionError\u001b[0m: An error occurred while calling o425.mount.\n",
       ": java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/fueldata; nested exception is: \n",
       "\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/fueldata\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:152)\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:74)\n",
       "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1205)\n",
       "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1231)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:528)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:633)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:656)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:76)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:76)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:628)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:537)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:76)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:529)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:495)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:76)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:140)\n",
       "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1225)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/fueldata\n",
       "\tat scala.Predef$.require(Predef.scala:281)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:812)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$2(MetadataManager.scala:1194)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:967)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:1183)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:820)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:131)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive(DbfsRequestHandler.scala:16)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive$(DbfsRequestHandler.scala:15)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:39)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:54)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:53)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:53)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.$anonfun$applyOrElse$10(DbfsServerBackend.scala:482)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:482)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:371)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1033)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:953)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:548)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:513)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$13(ActivityContextFactory.scala:831)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:831)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:794)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:776)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:285)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:285)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:513)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:408)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.lang.Thread.run(Thread.java:840)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ExecutionError",
        "evalue": "An error occurred while calling o425.mount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/fueldata; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/fueldata\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:152)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:74)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1205)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1231)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:528)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:633)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:656)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:76)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:76)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:628)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:537)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:76)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:529)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:495)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:76)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:140)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1225)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/fueldata\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:812)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$2(MetadataManager.scala:1194)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:967)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:1183)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:820)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:131)\n\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive(DbfsRequestHandler.scala:16)\n\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive$(DbfsRequestHandler.scala:15)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:39)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:54)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:53)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:53)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.$anonfun$applyOrElse$10(DbfsServerBackend.scala:482)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:482)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:371)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1033)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:953)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:548)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:513)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$13(ActivityContextFactory.scala:831)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:831)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:794)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:776)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:285)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:285)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:513)\n\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:408)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n\tat java.lang.Thread.run(Thread.java:840)\n"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>ExecutionError</span>: An error occurred while calling o425.mount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/fueldata; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/fueldata\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:152)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:74)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1205)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1231)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:528)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:633)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:656)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:76)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:76)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:628)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:537)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:76)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:529)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:495)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:76)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:140)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1225)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/fueldata\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:812)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$2(MetadataManager.scala:1194)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:967)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:1183)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:820)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:131)\n\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive(DbfsRequestHandler.scala:16)\n\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive$(DbfsRequestHandler.scala:15)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:39)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:54)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:53)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:53)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.$anonfun$applyOrElse$10(DbfsServerBackend.scala:482)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:482)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:371)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1033)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:953)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:548)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:513)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$13(ActivityContextFactory.scala:831)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:831)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:794)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:776)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:285)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:285)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:513)\n\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:408)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n\tat java.lang.Thread.run(Thread.java:840)\n"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mExecutionError\u001b[0m                            Traceback (most recent call last)",
        "File \u001b[0;32m<command-7454861199995879>, line 16\u001b[0m\n\u001b[1;32m     10\u001b[0m spark\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mset(\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfs.azure.account.key.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstorage_account_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.blob.core.windows.net\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     storage_account_key\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# âœ… Mount the blob container to DBFS (Databricks File System)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m dbutils\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mmount(\n\u001b[1;32m     17\u001b[0m     source \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwasbs://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontainer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstorage_account_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.blob.core.windows.net/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m     mount_point \u001b[38;5;241m=\u001b[39m mount_point,\n\u001b[1;32m     19\u001b[0m     extra_configs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfs.azure.account.key.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstorage_account_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.blob.core.windows.net\u001b[39m\u001b[38;5;124m\"\u001b[39m: storage_account_key\n\u001b[1;32m     21\u001b[0m     }\n\u001b[1;32m     22\u001b[0m )\n",
        "File \u001b[0;32m/databricks/python_shell/lib/dbruntime/dbutils.py:158\u001b[0m, in \u001b[0;36mprettify_exception_message.<locals>.f_with_exception_handling\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m exc\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    157\u001b[0m exc\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
        "\u001b[0;31mExecutionError\u001b[0m: An error occurred while calling o425.mount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/fueldata; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/fueldata\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:152)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:74)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1205)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1231)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:528)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:633)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:656)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:76)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:76)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:628)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:537)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:76)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:529)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:495)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:76)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:140)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1225)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/fueldata\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:812)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$2(MetadataManager.scala:1194)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:967)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:1183)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:820)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:131)\n\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive(DbfsRequestHandler.scala:16)\n\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive$(DbfsRequestHandler.scala:15)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:39)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:54)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:53)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:53)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.$anonfun$applyOrElse$10(DbfsServerBackend.scala:482)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:482)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:371)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1033)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:953)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:548)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:513)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$13(ActivityContextFactory.scala:831)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:831)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:794)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:776)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:285)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:285)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:513)\n\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:408)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n\tat java.lang.Thread.run(Thread.java:840)\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# âœ… Replace with your actual values\n",
    "storage_account_name = \"fuelintelligencestorage\"\n",
    "container_name = \"fueldata\"  # âœ… Actual container name from your screenshot\n",
    "mount_point = f\"/mnt/{container_name}\"\n",
    "\n",
    "# ðŸ” Storage account key from Azure Portal > Storage Account > Access Keys\n",
    "storage_account_key = \"<CLIENT_SECRET>\"  # â† Replace this securely (never hardcode in production)\n",
    "\n",
    "# âœ… Set Spark config to allow access to the storage account\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\",\n",
    "    storage_account_key\n",
    ")\n",
    "\n",
    "# âœ… Mount the blob container to DBFS (Databricks File System)\n",
    "dbutils.fs.mount(\n",
    "    source = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/\",\n",
    "    mount_point = mount_point,\n",
    "    extra_configs = {\n",
    "        f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\": storage_account_key\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c84d3e30-1cd6-476d-99e2-a221aab33501",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>client_id</th><th>name</th><th>country</th><th>state</th><th>outstanding_amount</th></tr></thead><tbody><tr><td>C001</td><td>Rodriguez, Hensley and Tucker</td><td>Canada</td><td>NY</td><td>2432.89</td></tr><tr><td>C002</td><td>Brown-Carlson</td><td>Canada</td><td>FL</td><td>1141.32</td></tr><tr><td>C003</td><td>Rubio, Patterson and Williams</td><td>Mexico</td><td>FL</td><td>2261.34</td></tr><tr><td>C004</td><td>Malone-Sanchez</td><td>Canada</td><td>FL</td><td>7998.23</td></tr><tr><td>C005</td><td>Beck-Nichols</td><td>Canada</td><td>NY</td><td>8282.1</td></tr><tr><td>C006</td><td>Atkinson, Kaiser and Gregory</td><td>Mexico</td><td>CA</td><td>2845.9</td></tr><tr><td>C007</td><td>Charles-Ortega</td><td>USA</td><td>FL</td><td>7409.77</td></tr><tr><td>C008</td><td>Holmes, Walker and Williams</td><td>Mexico</td><td>CA</td><td>3827.22</td></tr><tr><td>C009</td><td>Young-Cole</td><td>Canada</td><td>MA</td><td>1555.85</td></tr><tr><td>C010</td><td>White-Daniels</td><td>Canada</td><td>MA</td><td>4901.15</td></tr><tr><td>C011</td><td>Richardson and Sons</td><td>USA</td><td>NY</td><td>3927.15</td></tr><tr><td>C012</td><td>Warner, Moore and Pennington</td><td>USA</td><td>FL</td><td>5704.65</td></tr><tr><td>C013</td><td>Parker-Thomas</td><td>Canada</td><td>NY</td><td>8683.76</td></tr><tr><td>C014</td><td>Knox, Davis and Chase</td><td>Mexico</td><td>MA</td><td>5287.4</td></tr><tr><td>C015</td><td>Smith, Patrick and Roth</td><td>USA</td><td>CA</td><td>4454.04</td></tr><tr><td>C016</td><td>Bowman-Campbell</td><td>Mexico</td><td>FL</td><td>6077.78</td></tr><tr><td>C017</td><td>Erickson LLC</td><td>Mexico</td><td>CA</td><td>1493.34</td></tr><tr><td>C018</td><td>Roberts-Hill</td><td>Canada</td><td>MA</td><td>5784.92</td></tr><tr><td>C019</td><td>Howell, Cruz and Smith</td><td>USA</td><td>CA</td><td>9207.77</td></tr><tr><td>C020</td><td>Jones, Parrish and Riddle</td><td>Mexico</td><td>MA</td><td>9831.63</td></tr><tr><td>C021</td><td>Baker, Henson and Bradley</td><td>Mexico</td><td>CA</td><td>4179.05</td></tr><tr><td>C022</td><td>Blackwell, Hall and Stark</td><td>Canada</td><td>TX</td><td>2646.33</td></tr><tr><td>C023</td><td>Robinson-Hunter</td><td>USA</td><td>NY</td><td>9267.01</td></tr><tr><td>C024</td><td>Spears-Rocha</td><td>USA</td><td>NY</td><td>8319.36</td></tr><tr><td>C025</td><td>Jenkins, Castro and Moore</td><td>Canada</td><td>MA</td><td>6949.0</td></tr><tr><td>C026</td><td>Holland, Lynn and Cantu</td><td>USA</td><td>CA</td><td>2990.56</td></tr><tr><td>C027</td><td>Ellis Ltd</td><td>USA</td><td>FL</td><td>3959.42</td></tr><tr><td>C028</td><td>Morse and Sons</td><td>Canada</td><td>NY</td><td>7633.34</td></tr><tr><td>C029</td><td>Johnson, Ramirez and Walton</td><td>Canada</td><td>TX</td><td>9078.27</td></tr><tr><td>C030</td><td>Carney-Tran</td><td>USA</td><td>NY</td><td>8098.36</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "C001",
         "Rodriguez, Hensley and Tucker",
         "Canada",
         "NY",
         "2432.89"
        ],
        [
         "C002",
         "Brown-Carlson",
         "Canada",
         "FL",
         "1141.32"
        ],
        [
         "C003",
         "Rubio, Patterson and Williams",
         "Mexico",
         "FL",
         "2261.34"
        ],
        [
         "C004",
         "Malone-Sanchez",
         "Canada",
         "FL",
         "7998.23"
        ],
        [
         "C005",
         "Beck-Nichols",
         "Canada",
         "NY",
         "8282.1"
        ],
        [
         "C006",
         "Atkinson, Kaiser and Gregory",
         "Mexico",
         "CA",
         "2845.9"
        ],
        [
         "C007",
         "Charles-Ortega",
         "USA",
         "FL",
         "7409.77"
        ],
        [
         "C008",
         "Holmes, Walker and Williams",
         "Mexico",
         "CA",
         "3827.22"
        ],
        [
         "C009",
         "Young-Cole",
         "Canada",
         "MA",
         "1555.85"
        ],
        [
         "C010",
         "White-Daniels",
         "Canada",
         "MA",
         "4901.15"
        ],
        [
         "C011",
         "Richardson and Sons",
         "USA",
         "NY",
         "3927.15"
        ],
        [
         "C012",
         "Warner, Moore and Pennington",
         "USA",
         "FL",
         "5704.65"
        ],
        [
         "C013",
         "Parker-Thomas",
         "Canada",
         "NY",
         "8683.76"
        ],
        [
         "C014",
         "Knox, Davis and Chase",
         "Mexico",
         "MA",
         "5287.4"
        ],
        [
         "C015",
         "Smith, Patrick and Roth",
         "USA",
         "CA",
         "4454.04"
        ],
        [
         "C016",
         "Bowman-Campbell",
         "Mexico",
         "FL",
         "6077.78"
        ],
        [
         "C017",
         "Erickson LLC",
         "Mexico",
         "CA",
         "1493.34"
        ],
        [
         "C018",
         "Roberts-Hill",
         "Canada",
         "MA",
         "5784.92"
        ],
        [
         "C019",
         "Howell, Cruz and Smith",
         "USA",
         "CA",
         "9207.77"
        ],
        [
         "C020",
         "Jones, Parrish and Riddle",
         "Mexico",
         "MA",
         "9831.63"
        ],
        [
         "C021",
         "Baker, Henson and Bradley",
         "Mexico",
         "CA",
         "4179.05"
        ],
        [
         "C022",
         "Blackwell, Hall and Stark",
         "Canada",
         "TX",
         "2646.33"
        ],
        [
         "C023",
         "Robinson-Hunter",
         "USA",
         "NY",
         "9267.01"
        ],
        [
         "C024",
         "Spears-Rocha",
         "USA",
         "NY",
         "8319.36"
        ],
        [
         "C025",
         "Jenkins, Castro and Moore",
         "Canada",
         "MA",
         "6949.0"
        ],
        [
         "C026",
         "Holland, Lynn and Cantu",
         "USA",
         "CA",
         "2990.56"
        ],
        [
         "C027",
         "Ellis Ltd",
         "USA",
         "FL",
         "3959.42"
        ],
        [
         "C028",
         "Morse and Sons",
         "Canada",
         "NY",
         "7633.34"
        ],
        [
         "C029",
         "Johnson, Ramirez and Walton",
         "Canada",
         "TX",
         "9078.27"
        ],
        [
         "C030",
         "Carney-Tran",
         "USA",
         "NY",
         "8098.36"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "client_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "outstanding_amount",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>driver_id</th><th>name</th><th>license_number</th><th>experience_years</th></tr></thead><tbody><tr><td>D001</td><td>Mitchell Erickson</td><td>uL533961</td><td>2</td></tr><tr><td>D002</td><td>Eric Maldonado</td><td>Cf347080</td><td>8</td></tr><tr><td>D003</td><td>Jenny Kelly</td><td>sa456491</td><td>13</td></tr><tr><td>D004</td><td>Ruben Gonzalez</td><td>vF178322</td><td>11</td></tr><tr><td>D005</td><td>Robert Lambert</td><td>pW399436</td><td>19</td></tr><tr><td>D006</td><td>Ana Baker</td><td>vV188768</td><td>2</td></tr><tr><td>D007</td><td>Michael Lamb</td><td>oY666058</td><td>3</td></tr><tr><td>D008</td><td>Christina Jones</td><td>FG458396</td><td>19</td></tr><tr><td>D009</td><td>Lawrence Flores</td><td>yD846849</td><td>13</td></tr><tr><td>D010</td><td>Jacob Flowers</td><td>Qs404480</td><td>7</td></tr><tr><td>D011</td><td>Alexander Anderson</td><td>WG182160</td><td>9</td></tr><tr><td>D012</td><td>James Schmidt</td><td>wl226169</td><td>18</td></tr><tr><td>D013</td><td>Ryan Myers</td><td>EN382710</td><td>6</td></tr><tr><td>D014</td><td>Jennifer Allen</td><td>jU336695</td><td>20</td></tr><tr><td>D015</td><td>Phillip Morales</td><td>QO966263</td><td>20</td></tr><tr><td>D016</td><td>Tanner Morales</td><td>vP248328</td><td>3</td></tr><tr><td>D017</td><td>Teresa Reynolds</td><td>SK309094</td><td>15</td></tr><tr><td>D018</td><td>Carolyn Jones</td><td>YZ016037</td><td>14</td></tr><tr><td>D019</td><td>Jared Howell Jr.</td><td>ZP938622</td><td>10</td></tr><tr><td>D020</td><td>Vincent Carr</td><td>Sk649981</td><td>14</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "D001",
         "Mitchell Erickson",
         "uL533961",
         "2"
        ],
        [
         "D002",
         "Eric Maldonado",
         "Cf347080",
         "8"
        ],
        [
         "D003",
         "Jenny Kelly",
         "sa456491",
         "13"
        ],
        [
         "D004",
         "Ruben Gonzalez",
         "vF178322",
         "11"
        ],
        [
         "D005",
         "Robert Lambert",
         "pW399436",
         "19"
        ],
        [
         "D006",
         "Ana Baker",
         "vV188768",
         "2"
        ],
        [
         "D007",
         "Michael Lamb",
         "oY666058",
         "3"
        ],
        [
         "D008",
         "Christina Jones",
         "FG458396",
         "19"
        ],
        [
         "D009",
         "Lawrence Flores",
         "yD846849",
         "13"
        ],
        [
         "D010",
         "Jacob Flowers",
         "Qs404480",
         "7"
        ],
        [
         "D011",
         "Alexander Anderson",
         "WG182160",
         "9"
        ],
        [
         "D012",
         "James Schmidt",
         "wl226169",
         "18"
        ],
        [
         "D013",
         "Ryan Myers",
         "EN382710",
         "6"
        ],
        [
         "D014",
         "Jennifer Allen",
         "jU336695",
         "20"
        ],
        [
         "D015",
         "Phillip Morales",
         "QO966263",
         "20"
        ],
        [
         "D016",
         "Tanner Morales",
         "vP248328",
         "3"
        ],
        [
         "D017",
         "Teresa Reynolds",
         "SK309094",
         "15"
        ],
        [
         "D018",
         "Carolyn Jones",
         "YZ016037",
         "14"
        ],
        [
         "D019",
         "Jared Howell Jr.",
         "ZP938622",
         "10"
        ],
        [
         "D020",
         "Vincent Carr",
         "Sk649981",
         "14"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "driver_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "license_number",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "experience_years",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>fuel_id</th><th>fuel_type</th><th>price_per_gallon</th></tr></thead><tbody><tr><td>F001</td><td>Diesel</td><td>4.73</td></tr><tr><td>F002</td><td>Petrol</td><td>2.77</td></tr><tr><td>F003</td><td>Ethanol</td><td>4.78</td></tr><tr><td>F004</td><td>Biofuel</td><td>3.58</td></tr><tr><td>F005</td><td>Kerosene</td><td>2.67</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "F001",
         "Diesel",
         "4.73"
        ],
        [
         "F002",
         "Petrol",
         "2.77"
        ],
        [
         "F003",
         "Ethanol",
         "4.78"
        ],
        [
         "F004",
         "Biofuel",
         "3.58"
        ],
        [
         "F005",
         "Kerosene",
         "2.67"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "fuel_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "fuel_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "price_per_gallon",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>load_id</th><th>driver_id</th><th>client_id</th><th>fuel_id</th><th>gallons_loaded</th><th>state_tax</th><th>excise_tax</th><th>note</th><th>timestamp</th></tr></thead><tbody><tr><td>7b5f1dd0-cf2c-468c-8df6-c9a6d59d510e</td><td>D008</td><td>C013</td><td>F002</td><td>503.6</td><td>25.18</td><td>50.36</td><td>Likely wrong forget.</td><td>2025-03-23T21:47:00</td></tr><tr><td>4177127b-1679-494d-b738-39c7c9e0c4eb</td><td>D016</td><td>C027</td><td>F002</td><td>839.26</td><td>41.96</td><td>83.93</td><td>Across to arrive condition human his enter.</td><td>2025-02-03T10:44:56</td></tr><tr><td>c60d3a27-7750-494d-80a6-303ab9aeb3d5</td><td>D009</td><td>C014</td><td>F002</td><td>857.19</td><td>42.86</td><td>85.72</td><td>Hand green miss may and.</td><td>2025-02-09T21:59:10</td></tr><tr><td>90160447-47cb-4815-8f2d-c9f09e88cbe7</td><td>D003</td><td>C026</td><td>F004</td><td>770.04</td><td>38.5</td><td>77.0</td><td>Pick quite story where cover again wall.</td><td>2025-03-07T11:48:34</td></tr><tr><td>54112fce-7195-4166-9af2-7e56c4dd0b68</td><td>D007</td><td>C016</td><td>F002</td><td>982.39</td><td>49.12</td><td>98.24</td><td>Even total material entire.</td><td>2025-02-07T23:36:04</td></tr><tr><td>ce26049b-6bad-4bfe-aed6-787e64fa02b2</td><td>D019</td><td>C020</td><td>F001</td><td>182.13</td><td>9.11</td><td>18.21</td><td>Black work which receive air subject argue.</td><td>2025-01-04T20:06:20</td></tr><tr><td>9dcd1af1-16ad-451d-ab2f-3d6d9e2c2b8b</td><td>D018</td><td>C009</td><td>F002</td><td>173.1</td><td>8.65</td><td>17.31</td><td>World itself direction second rise relationship purpose wait.</td><td>2025-01-30T04:51:10</td></tr><tr><td>0d1a99bb-250c-421b-aaf2-c274cfcad711</td><td>D003</td><td>C017</td><td>F003</td><td>201.33</td><td>10.07</td><td>20.13</td><td>Building marriage two worker also cover.</td><td>2025-01-06T10:59:12</td></tr><tr><td>4f56056e-18d1-4737-869b-c36130387781</td><td>D004</td><td>C025</td><td>F004</td><td>992.14</td><td>49.61</td><td>99.21</td><td>Particularly plant only card.</td><td>2025-02-12T12:46:29</td></tr><tr><td>cffbc1da-521d-41d3-b5c7-bce3eaae7185</td><td>D002</td><td>C006</td><td>F005</td><td>973.23</td><td>48.66</td><td>97.32</td><td>Difference about dark after value book.</td><td>2025-02-19T08:04:27</td></tr><tr><td>be9b8abf-3767-4ad0-9f42-40b5fd99ca2b</td><td>D015</td><td>C008</td><td>F004</td><td>209.79</td><td>10.49</td><td>20.98</td><td>Boy me inside admit build.</td><td>2025-05-02T03:32:10</td></tr><tr><td>190cf34f-938a-4a4c-a9fb-e8bad79ecd14</td><td>D015</td><td>C030</td><td>F001</td><td>874.43</td><td>43.72</td><td>87.44</td><td>Short pull several expert court.</td><td>2025-01-16T15:02:33</td></tr><tr><td>2f3a1f7a-7b9c-418c-ad72-1d73bed07f95</td><td>D020</td><td>C030</td><td>F005</td><td>354.68</td><td>17.73</td><td>35.47</td><td>Fire alone not indicate.</td><td>2025-04-09T05:45:29</td></tr><tr><td>f34a3cdd-63d8-4dac-b394-824810f38cbe</td><td>D017</td><td>C025</td><td>F001</td><td>511.16</td><td>25.56</td><td>51.12</td><td>Able language house enough.</td><td>2025-02-08T17:14:43</td></tr><tr><td>ef5c27bb-03a7-4f8c-ad80-fe74b562ed8c</td><td>D020</td><td>C011</td><td>F005</td><td>557.94</td><td>27.9</td><td>55.79</td><td>Almost money arm.</td><td>2025-03-25T14:25:38</td></tr><tr><td>85bda5e0-551d-4f36-a434-fe61ed176f04</td><td>D008</td><td>C012</td><td>F001</td><td>658.35</td><td>32.92</td><td>65.84</td><td>Coach account director current serve cost standard.</td><td>2025-04-27T00:48:42</td></tr><tr><td>7351261e-520e-45ed-829f-855aad8c020c</td><td>D003</td><td>C029</td><td>F005</td><td>743.12</td><td>37.16</td><td>74.31</td><td>Join hospital leg wait.</td><td>2025-02-25T08:50:37</td></tr><tr><td>c1e2f65d-7e71-4d6c-a3d6-1ab410032fdb</td><td>D012</td><td>C003</td><td>F001</td><td>329.97</td><td>16.5</td><td>33.0</td><td>Southern again identify must happen.</td><td>2025-02-24T03:49:38</td></tr><tr><td>9832cff8-91ac-4ab6-a081-64c2d89dc594</td><td>D004</td><td>C030</td><td>F002</td><td>139.67</td><td>6.98</td><td>13.97</td><td>Purpose must party.</td><td>2025-03-26T00:44:36</td></tr><tr><td>937a0229-d655-4f56-b8e2-d98696415793</td><td>D012</td><td>C013</td><td>F002</td><td>440.75</td><td>22.04</td><td>44.08</td><td>The energy training national option less late.</td><td>2025-03-10T08:49:41</td></tr><tr><td>4a7f1a29-e440-46e3-b9e2-fa1c95129702</td><td>D006</td><td>C026</td><td>F003</td><td>781.95</td><td>39.1</td><td>78.2</td><td>Focus hotel weight large say.</td><td>2025-04-22T00:19:48</td></tr><tr><td>9b6aba5b-0efa-487a-b334-b91b4ef17d88</td><td>D015</td><td>C006</td><td>F004</td><td>463.62</td><td>23.18</td><td>46.36</td><td>Write fight discuss play economic executive father.</td><td>2025-04-28T04:38:12</td></tr><tr><td>be845ea9-ba9c-4c29-ae1b-a0ea32f9b4cf</td><td>D003</td><td>C012</td><td>F002</td><td>943.27</td><td>47.16</td><td>94.33</td><td>Such general seek.</td><td>2025-04-23T10:01:36</td></tr><tr><td>a68fa48a-77fd-4d76-a6d3-f69a513aec89</td><td>D002</td><td>C021</td><td>F005</td><td>796.91</td><td>39.85</td><td>79.69</td><td>Paper notice nothing best store particular country.</td><td>2025-02-23T06:10:58</td></tr><tr><td>b985331f-ab88-41ea-a426-66043567eb11</td><td>D005</td><td>C018</td><td>F002</td><td>399.66</td><td>19.98</td><td>39.97</td><td>Glass writer modern people.</td><td>2025-01-22T07:14:26</td></tr><tr><td>77569c2f-4eea-4495-9522-b5a8cdf20f0e</td><td>D010</td><td>C029</td><td>F001</td><td>428.21</td><td>21.41</td><td>42.82</td><td>Audience discover party dark.</td><td>2025-02-04T16:32:51</td></tr><tr><td>54018d64-7a63-4b69-a5ee-9a3f4c770760</td><td>D012</td><td>C029</td><td>F003</td><td>444.85</td><td>22.24</td><td>44.49</td><td>Site knowledge perhaps century position edge clear.</td><td>2025-01-23T12:20:05</td></tr><tr><td>916d7d71-6a01-4896-bcd9-1834dc01bdec</td><td>D001</td><td>C007</td><td>F005</td><td>611.42</td><td>30.57</td><td>61.14</td><td>Like property social myself during produce office plant.</td><td>2025-02-20T18:52:21</td></tr><tr><td>08005542-80f2-4ed9-b388-c4ccb9e0d572</td><td>D018</td><td>C026</td><td>F003</td><td>184.12</td><td>9.21</td><td>18.41</td><td>Lay budget stop image it per education.</td><td>2025-02-02T13:13:17</td></tr><tr><td>e8a0ffab-c75e-4cca-ba29-df80227f1ab2</td><td>D011</td><td>C018</td><td>F005</td><td>624.01</td><td>31.2</td><td>62.4</td><td>Create machine party me give.</td><td>2025-04-05T22:43:45</td></tr><tr><td>64927ab8-62cd-4f13-911c-628c602c1dae</td><td>D003</td><td>C030</td><td>F001</td><td>663.63</td><td>33.18</td><td>66.36</td><td>Animal reach couple against.</td><td>2025-04-15T13:26:07</td></tr><tr><td>679fe366-92cf-47d9-97be-35f69f8b1fac</td><td>D017</td><td>C017</td><td>F004</td><td>966.03</td><td>48.3</td><td>96.6</td><td>South senior deep fact economic use.</td><td>2025-02-03T18:00:53</td></tr><tr><td>98c1c7b1-d725-40fe-ad53-2823572c4c41</td><td>D004</td><td>C026</td><td>F001</td><td>723.05</td><td>36.15</td><td>72.3</td><td>Season once ahead painting.</td><td>2025-02-04T14:01:57</td></tr><tr><td>b9f09a78-98df-48eb-a027-08f5f7f4c59b</td><td>D010</td><td>C013</td><td>F001</td><td>805.46</td><td>40.27</td><td>80.55</td><td>Machine community professor without stand money finally collection.</td><td>2025-01-20T16:03:59</td></tr><tr><td>9cccc597-82d4-4a59-968a-539d7edb12ce</td><td>D015</td><td>C029</td><td>F002</td><td>489.35</td><td>24.47</td><td>48.94</td><td>Some local occur.</td><td>2025-01-21T18:32:18</td></tr><tr><td>2b8df8b1-5567-47a9-a149-318db3ec0e27</td><td>D004</td><td>C001</td><td>F005</td><td>601.14</td><td>30.06</td><td>60.11</td><td>Contain machine three will.</td><td>2025-02-13T15:15:56</td></tr><tr><td>bcd35a07-8263-49df-b484-6118753ce60a</td><td>D002</td><td>C019</td><td>F001</td><td>624.78</td><td>31.24</td><td>62.48</td><td>Difference particularly name course keep.</td><td>2025-02-12T09:17:48</td></tr><tr><td>504c7f93-60e1-4a29-9a90-034195ee2a6d</td><td>D018</td><td>C021</td><td>F004</td><td>626.11</td><td>31.31</td><td>62.61</td><td>Together avoid business sign.</td><td>2025-04-09T16:17:14</td></tr><tr><td>4c61a761-0d91-485f-bc68-cbde3b67eedf</td><td>D008</td><td>C001</td><td>F001</td><td>725.27</td><td>36.26</td><td>72.53</td><td>Live professor training seat beyond also late.</td><td>2025-04-11T03:56:37</td></tr><tr><td>d17c65b7-639f-4722-91c3-b70c1f8e1f21</td><td>D001</td><td>C009</td><td>F005</td><td>882.8</td><td>44.14</td><td>88.28</td><td>Bring generation some administration defense oil hand.</td><td>2025-04-04T11:03:45</td></tr><tr><td>66c0f6d8-2d00-48d2-bf51-8d37c7a2f9d1</td><td>D003</td><td>C030</td><td>F002</td><td>562.83</td><td>28.14</td><td>56.28</td><td>Paper including opportunity.</td><td>2025-01-16T03:33:48</td></tr><tr><td>c225fc9e-6188-4425-8be7-035f4d7f3be5</td><td>D020</td><td>C012</td><td>F002</td><td>452.4</td><td>22.62</td><td>45.24</td><td>Table stand represent between keep.</td><td>2025-03-10T15:34:30</td></tr><tr><td>6588aeaf-5593-48cb-91a7-fa61556b2b7c</td><td>D006</td><td>C004</td><td>F005</td><td>183.96</td><td>9.2</td><td>18.4</td><td>Drop fish hair.</td><td>2025-03-30T22:22:33</td></tr><tr><td>748a09ee-293c-48a8-bee9-9697dcc5b856</td><td>D015</td><td>C008</td><td>F004</td><td>923.47</td><td>46.17</td><td>92.35</td><td>Direction back box property life event short marriage.</td><td>2025-04-16T08:55:44</td></tr><tr><td>b547ddd9-27e6-488e-beb9-12631b01b8ec</td><td>D006</td><td>C026</td><td>F005</td><td>520.04</td><td>26.0</td><td>52.0</td><td>Great until name behind last.</td><td>2025-01-22T22:37:34</td></tr><tr><td>0da2af49-1554-45f9-9829-d13c3823a394</td><td>D012</td><td>C015</td><td>F005</td><td>310.28</td><td>15.51</td><td>31.03</td><td>Real as force home pattern.</td><td>2025-01-08T04:24:46</td></tr><tr><td>09098dcb-e141-4bce-af0d-e1d2a1bb521a</td><td>D002</td><td>C013</td><td>F002</td><td>141.88</td><td>7.09</td><td>14.19</td><td>Consider include herself three yes explain throw must.</td><td>2025-01-05T13:31:28</td></tr><tr><td>7685022d-1dad-441e-a482-af9e3f52aa08</td><td>D013</td><td>C017</td><td>F004</td><td>894.89</td><td>44.74</td><td>89.49</td><td>Issue federal type wish effort different film.</td><td>2025-03-25T18:27:12</td></tr><tr><td>3b82769f-0440-4154-abf6-5fe4b42283c2</td><td>D007</td><td>C020</td><td>F005</td><td>965.93</td><td>48.3</td><td>96.59</td><td>Town evening person campaign.</td><td>2025-02-21T23:21:19</td></tr><tr><td>5e576714-0ac8-4b2f-8c2f-796b62171ad6</td><td>D007</td><td>C003</td><td>F001</td><td>881.52</td><td>44.08</td><td>88.15</td><td>Discuss theory subject agent research would head.</td><td>2025-02-07T09:18:06</td></tr><tr><td>696b0148-3cd1-44e1-b4ee-51369e75a31f</td><td>D005</td><td>C021</td><td>F001</td><td>600.1</td><td>30.01</td><td>60.01</td><td>Reach piece trouble have fast positive best.</td><td>2025-02-18T03:04:44</td></tr><tr><td>3d96c4a7-1840-4d38-b9fc-532c022e5e59</td><td>D003</td><td>C017</td><td>F003</td><td>295.69</td><td>14.78</td><td>29.57</td><td>Our to along run.</td><td>2025-04-15T12:14:51</td></tr><tr><td>f35190c0-7874-41f3-bef6-010efc718914</td><td>D011</td><td>C023</td><td>F005</td><td>626.1</td><td>31.31</td><td>62.61</td><td>Include city few commercial nothing either just.</td><td>2025-04-05T09:34:36</td></tr><tr><td>d4c2b66f-320e-440d-a22b-a4bbc6bcbff9</td><td>D009</td><td>C001</td><td>F005</td><td>887.78</td><td>44.39</td><td>88.78</td><td>Certain kitchen real issue memory morning.</td><td>2025-01-03T17:00:06</td></tr><tr><td>a0a40acc-ac18-4ccb-ba58-5d460b580709</td><td>D010</td><td>C005</td><td>F001</td><td>616.79</td><td>30.84</td><td>61.68</td><td>Enjoy describe stand reflect car stage win.</td><td>2025-02-03T04:31:27</td></tr><tr><td>de35f987-20e3-4d7b-a288-ff81477ee092</td><td>D017</td><td>C010</td><td>F002</td><td>168.35</td><td>8.42</td><td>16.84</td><td>Common local full enjoy race heart more.</td><td>2025-01-24T05:16:45</td></tr><tr><td>fc9966a6-bd32-49d1-8db4-6743599be83b</td><td>D016</td><td>C008</td><td>F003</td><td>402.62</td><td>20.13</td><td>40.26</td><td>May deal answer three.</td><td>2025-01-20T19:36:05</td></tr><tr><td>fbfefd7e-c02c-4318-9fdb-41bd2848c46a</td><td>D010</td><td>C016</td><td>F004</td><td>409.89</td><td>20.49</td><td>40.99</td><td>Generation know magazine ground week.</td><td>2025-03-25T17:23:44</td></tr><tr><td>54fce3aa-8b33-4bca-a564-9a2d5a409717</td><td>D011</td><td>C030</td><td>F002</td><td>382.21</td><td>19.11</td><td>38.22</td><td>Successful start amount word.</td><td>2025-03-26T15:15:51</td></tr><tr><td>6f170ce8-25c9-4187-8a7c-36487eba4f78</td><td>D007</td><td>C020</td><td>F001</td><td>773.72</td><td>38.69</td><td>77.37</td><td>Bill month push begin reality performance.</td><td>2025-02-18T07:09:41</td></tr><tr><td>ea35f95b-b3d7-4b5d-a3f2-5514bee909ab</td><td>D007</td><td>C008</td><td>F001</td><td>947.35</td><td>47.37</td><td>94.74</td><td>Old teacher happy.</td><td>2025-04-08T22:01:05</td></tr><tr><td>f06f53b1-8fe8-4789-94bf-2009eab1fd5d</td><td>D011</td><td>C001</td><td>F003</td><td>862.25</td><td>43.11</td><td>86.23</td><td>Against back quickly size science like standard especially.</td><td>2025-03-10T16:20:38</td></tr><tr><td>887c2385-7251-4d9f-9c8a-f2d7ab28160e</td><td>D005</td><td>C002</td><td>F003</td><td>298.23</td><td>14.91</td><td>29.82</td><td>Father can drive certainly happen set.</td><td>2025-03-03T13:27:39</td></tr><tr><td>cc21317a-b024-474d-a7b3-1c9651d75c6c</td><td>D014</td><td>C019</td><td>F002</td><td>975.96</td><td>48.8</td><td>97.6</td><td>American care science if away.</td><td>2025-04-11T10:10:24</td></tr><tr><td>2f624787-6c32-436d-8912-82d56441334c</td><td>D009</td><td>C023</td><td>F004</td><td>727.38</td><td>36.37</td><td>72.74</td><td>Feel imagine region one popular.</td><td>2025-01-25T02:26:06</td></tr><tr><td>b088514b-7eb4-4549-8a12-f2795aeb74e2</td><td>D010</td><td>C017</td><td>F004</td><td>755.91</td><td>37.8</td><td>75.59</td><td>Opportunity who hard line shake respond visit.</td><td>2025-01-12T11:20:28</td></tr><tr><td>32b99a74-b141-4cdb-881a-a5bbf0240f24</td><td>D006</td><td>C021</td><td>F001</td><td>465.39</td><td>23.27</td><td>46.54</td><td>Find partner various score agree lay.</td><td>2025-04-23T11:40:24</td></tr><tr><td>5d656490-4f1e-4dff-bce9-a168f8539cff</td><td>D002</td><td>C003</td><td>F003</td><td>558.43</td><td>27.92</td><td>55.84</td><td>Call ago sister suffer Democrat upon.</td><td>2025-04-19T12:13:24</td></tr><tr><td>2c683d09-6904-464d-8d96-ce122c7f9431</td><td>D002</td><td>C015</td><td>F005</td><td>632.78</td><td>31.64</td><td>63.28</td><td>Walk item action mother mouth theory.</td><td>2025-03-07T14:39:43</td></tr><tr><td>b1b596b0-9134-4fa2-8295-de66e3a147ae</td><td>D015</td><td>C029</td><td>F004</td><td>953.07</td><td>47.65</td><td>95.31</td><td>Attack citizen plan strategy opportunity much.</td><td>2025-01-09T20:31:39</td></tr><tr><td>dcf3ef47-3857-4b62-ae21-aac1414082f6</td><td>D016</td><td>C023</td><td>F003</td><td>594.5</td><td>29.73</td><td>59.45</td><td>Community onto beyond keep reveal.</td><td>2025-03-18T04:55:46</td></tr><tr><td>12ab2ca3-c0de-4f9f-83f2-1c0bdece4733</td><td>D016</td><td>C029</td><td>F003</td><td>692.48</td><td>34.62</td><td>69.25</td><td>Bit fast gas difficult indeed financial.</td><td>2025-04-05T00:01:41</td></tr><tr><td>16a427d9-063d-48ac-b52d-f321fa12d7cd</td><td>D007</td><td>C009</td><td>F003</td><td>675.49</td><td>33.77</td><td>67.55</td><td>Move position significant parent upon put many.</td><td>2025-03-12T13:29:55</td></tr><tr><td>62d2acc7-b622-459e-8a65-1b3765d2fa3c</td><td>D008</td><td>C020</td><td>F005</td><td>444.13</td><td>22.21</td><td>44.41</td><td>Teach program mean.</td><td>2025-05-03T21:36:19</td></tr><tr><td>8407cdda-718a-484e-b110-40b88c00e0cb</td><td>D003</td><td>C012</td><td>F005</td><td>637.07</td><td>31.85</td><td>63.71</td><td>Half see then attorney before central care.</td><td>2025-02-03T11:41:06</td></tr><tr><td>afdd75e1-d9ad-43e1-8b32-2db9beee24aa</td><td>D019</td><td>C025</td><td>F004</td><td>975.42</td><td>48.77</td><td>97.54</td><td>Painting simply risk simply.</td><td>2025-03-16T17:20:20</td></tr><tr><td>06b6f24c-4387-454b-a242-09c8634eec06</td><td>D015</td><td>C011</td><td>F001</td><td>487.42</td><td>24.37</td><td>48.74</td><td>Commercial maybe describe.</td><td>2025-02-27T20:15:54</td></tr><tr><td>cc4f9c61-0456-4c13-bdd9-c876adf77565</td><td>D012</td><td>C030</td><td>F001</td><td>199.41</td><td>9.97</td><td>19.94</td><td>Fear visit mother yeah.</td><td>2025-02-26T14:45:21</td></tr><tr><td>774fbf66-61e6-44c5-9f7d-ce344627fd41</td><td>D005</td><td>C011</td><td>F002</td><td>463.69</td><td>23.18</td><td>46.37</td><td>Image themselves public pattern now prepare.</td><td>2025-02-16T22:15:19</td></tr><tr><td>e4bf27e0-9266-499f-95b6-a7384f266ad6</td><td>D016</td><td>C004</td><td>F004</td><td>389.94</td><td>19.5</td><td>38.99</td><td>Moment pick sister admit month condition.</td><td>2025-02-07T17:43:34</td></tr><tr><td>31b168bc-cc95-42ee-9b1d-5d7af1d2990d</td><td>D013</td><td>C006</td><td>F004</td><td>967.02</td><td>48.35</td><td>96.7</td><td>Ground Democrat in.</td><td>2025-04-18T03:04:20</td></tr><tr><td>c783107c-2800-42d3-bb39-f33b774e2218</td><td>D009</td><td>C005</td><td>F001</td><td>650.11</td><td>32.51</td><td>65.01</td><td>World me card people they them environment.</td><td>2025-02-16T06:58:15</td></tr><tr><td>9c91cf9f-4625-4690-a999-d6662bd6c0ef</td><td>D011</td><td>C002</td><td>F004</td><td>457.92</td><td>22.9</td><td>45.79</td><td>Music social possible action success growth kid.</td><td>2025-04-03T16:58:14</td></tr><tr><td>3b1c23eb-b24a-436f-8d3a-8b2b650fb4b0</td><td>D008</td><td>C019</td><td>F002</td><td>865.12</td><td>43.26</td><td>86.51</td><td>Political choose practice.</td><td>2025-04-05T14:01:11</td></tr><tr><td>f9b797c5-a0f6-4e06-ba16-53c82fa7c026</td><td>D002</td><td>C009</td><td>F003</td><td>822.38</td><td>41.12</td><td>82.24</td><td>Loss management everyone.</td><td>2025-02-05T13:21:59</td></tr><tr><td>877f9df3-9c8b-463e-959a-f88312f9ac2d</td><td>D007</td><td>C011</td><td>F001</td><td>133.99</td><td>6.7</td><td>13.4</td><td>Themselves eat rise production push interest.</td><td>2025-04-25T12:00:27</td></tr><tr><td>80cafb2a-b335-46dd-a3aa-48c64b834966</td><td>D010</td><td>C003</td><td>F003</td><td>387.27</td><td>19.36</td><td>38.73</td><td>Education evidence improve toward.</td><td>2025-02-28T12:08:47</td></tr><tr><td>023711c9-7b79-4d39-8a7b-3d9afab4831f</td><td>D007</td><td>C003</td><td>F002</td><td>924.9</td><td>46.25</td><td>92.49</td><td>Exactly matter tax her plant appear unit.</td><td>2025-04-23T06:25:52</td></tr><tr><td>6d5df8b0-2977-4899-be49-b09799860522</td><td>D004</td><td>C030</td><td>F002</td><td>266.56</td><td>13.33</td><td>26.66</td><td>Film large another project new always tough.</td><td>2025-02-01T13:54:26</td></tr><tr><td>c32e7311-10cf-4e27-b744-1aae96fbd701</td><td>D002</td><td>C024</td><td>F004</td><td>512.1</td><td>25.61</td><td>51.21</td><td>Image generation significant organization want low star.</td><td>2025-01-18T14:57:42</td></tr><tr><td>b22d1756-2586-48b7-8daf-13e08d935e00</td><td>D008</td><td>C027</td><td>F003</td><td>854.22</td><td>42.71</td><td>85.42</td><td>Sign them easy eight hear power rather.</td><td>2025-03-05T12:58:10</td></tr><tr><td>aa1751fd-6dbb-4b3c-8319-2bcbd5e60143</td><td>D011</td><td>C012</td><td>F005</td><td>941.84</td><td>47.09</td><td>94.18</td><td>Son party region responsibility.</td><td>2025-01-06T01:16:21</td></tr><tr><td>3366f46b-1c38-4b22-b7f8-72a763cfc5d6</td><td>D019</td><td>C016</td><td>F002</td><td>670.19</td><td>33.51</td><td>67.02</td><td>Admit prepare generation than general say available.</td><td>2025-03-18T16:56:24</td></tr><tr><td>7973f749-e05b-4459-a9fb-f4d647a70fd0</td><td>D001</td><td>C005</td><td>F004</td><td>189.22</td><td>9.46</td><td>18.92</td><td>Major key remember watch whether cultural white.</td><td>2025-03-03T17:12:33</td></tr><tr><td>8324cc49-e0da-4fe8-b3ee-09a43bb8ad02</td><td>D012</td><td>C014</td><td>F004</td><td>403.76</td><td>20.19</td><td>40.38</td><td>Morning end glass difference level put.</td><td>2025-01-15T03:23:10</td></tr><tr><td>289cb7fb-7c4d-46f5-bcbb-cd036ae428d6</td><td>D006</td><td>C022</td><td>F001</td><td>550.51</td><td>27.53</td><td>55.05</td><td>Game ago then value source provide.</td><td>2025-01-04T12:05:28</td></tr><tr><td>16ceacb7-e622-4b31-99d7-7adab4f36864</td><td>D009</td><td>C006</td><td>F004</td><td>793.01</td><td>39.65</td><td>79.3</td><td>Doctor rise per hear.</td><td>2025-02-26T10:24:22</td></tr><tr><td>a96d9052-ec81-4587-bdf6-78b9a1475ac9</td><td>D003</td><td>C023</td><td>F002</td><td>968.98</td><td>48.45</td><td>96.9</td><td>Development eye just.</td><td>2025-01-27T18:57:16</td></tr><tr><td>6641999d-cc98-4556-9f5b-eb39f822d37c</td><td>D014</td><td>C026</td><td>F004</td><td>355.16</td><td>17.76</td><td>35.52</td><td>Gas service benefit.</td><td>2025-01-15T18:32:11</td></tr><tr><td>83c68623-4664-4067-8257-96e4e2e58980</td><td>D014</td><td>C005</td><td>F001</td><td>255.32</td><td>12.77</td><td>25.53</td><td>Lose pull decide watch.</td><td>2025-01-04T00:27:04</td></tr><tr><td>2b55c97b-a913-4559-90fd-14ad9a138a9d</td><td>D007</td><td>C025</td><td>F002</td><td>653.5</td><td>32.68</td><td>65.35</td><td>Shake kid common then term race.</td><td>2025-02-18T03:48:10</td></tr><tr><td>153be433-7a21-40b4-964a-a47981c0e91e</td><td>D013</td><td>C006</td><td>F004</td><td>295.09</td><td>14.75</td><td>29.51</td><td>Become possible allow past.</td><td>2025-01-23T14:10:10</td></tr><tr><td>252747b5-6147-4e01-a734-159a480c676c</td><td>D016</td><td>C020</td><td>F001</td><td>135.35</td><td>6.77</td><td>13.54</td><td>Toward enter pay past field.</td><td>2025-01-31T18:47:54</td></tr><tr><td>528b6c21-6daa-44bb-9302-3d82491e16a6</td><td>D010</td><td>C010</td><td>F003</td><td>843.68</td><td>42.18</td><td>84.37</td><td>Look hope science site window.</td><td>2025-03-11T04:03:34</td></tr><tr><td>2eefba32-3082-46d6-b5d7-7bf66c00b126</td><td>D015</td><td>C012</td><td>F002</td><td>126.2</td><td>6.31</td><td>12.62</td><td>Happen not second inside.</td><td>2025-01-26T08:24:17</td></tr><tr><td>292f8c04-aea0-437a-a20d-b8c67b13da07</td><td>D015</td><td>C022</td><td>F003</td><td>682.22</td><td>34.11</td><td>68.22</td><td>Simply with place turn.</td><td>2025-04-14T20:04:08</td></tr><tr><td>1c16fd1d-15c8-44d0-a5d6-181b90716751</td><td>D003</td><td>C017</td><td>F001</td><td>352.82</td><td>17.64</td><td>35.28</td><td>Attention form cut daughter series.</td><td>2025-04-15T04:53:34</td></tr><tr><td>6c113539-350a-455a-92bd-a0e88efc64ea</td><td>D018</td><td>C027</td><td>F005</td><td>664.96</td><td>33.25</td><td>66.5</td><td>Opportunity treat government though and.</td><td>2025-03-26T09:28:38</td></tr><tr><td>c5e9e6c7-96d3-4c9c-8b0d-16b86b46b8cd</td><td>D011</td><td>C006</td><td>F001</td><td>435.22</td><td>21.76</td><td>43.52</td><td>Cause fine research heart enter.</td><td>2025-03-23T04:56:21</td></tr><tr><td>421cc1a8-7fe8-4e86-a7ab-373587cd3cff</td><td>D008</td><td>C011</td><td>F004</td><td>664.67</td><td>33.23</td><td>66.47</td><td>Staff right me quite floor pretty goal push.</td><td>2025-01-25T07:06:27</td></tr><tr><td>b58aca8a-02ce-44e7-a006-0f0ba8c94dbd</td><td>D010</td><td>C016</td><td>F004</td><td>798.3</td><td>39.91</td><td>79.83</td><td>Address culture player may.</td><td>2025-02-15T15:17:14</td></tr><tr><td>21f2f468-5f75-40a4-8bc1-753a42b9ef48</td><td>D010</td><td>C017</td><td>F005</td><td>521.3</td><td>26.06</td><td>52.13</td><td>No clear house worry better drop into.</td><td>2025-01-29T15:13:07</td></tr><tr><td>f3375df1-6933-4d6e-a002-328c53b1687e</td><td>D016</td><td>C024</td><td>F005</td><td>726.07</td><td>36.3</td><td>72.61</td><td>Attorney strategy indicate.</td><td>2025-01-16T21:33:27</td></tr><tr><td>1969c237-bdef-4cdd-b4e4-82766dfc7d13</td><td>D008</td><td>C029</td><td>F002</td><td>879.35</td><td>43.97</td><td>87.94</td><td>Federal adult executive recently strong.</td><td>2025-01-13T19:48:09</td></tr><tr><td>00cf826c-4e7b-4278-98bb-04edf7914cbc</td><td>D009</td><td>C024</td><td>F003</td><td>547.76</td><td>27.39</td><td>54.78</td><td>Energy ten once agreement.</td><td>2025-01-04T02:15:37</td></tr><tr><td>8c908729-994c-4f5b-8857-d7bc3d12cf14</td><td>D020</td><td>C029</td><td>F001</td><td>917.99</td><td>45.9</td><td>91.8</td><td>Per task agent produce.</td><td>2025-03-06T01:19:44</td></tr><tr><td>ed682d65-430a-414a-8919-9a63dc1d86a8</td><td>D010</td><td>C026</td><td>F001</td><td>390.92</td><td>19.55</td><td>39.09</td><td>Just program within true shake them rock school.</td><td>2025-01-25T07:10:55</td></tr><tr><td>421dc8be-8b83-493f-9c0b-1ae8825d0050</td><td>D006</td><td>C027</td><td>F003</td><td>402.57</td><td>20.13</td><td>40.26</td><td>Certain both least building white.</td><td>2025-01-30T08:28:16</td></tr><tr><td>29b53e28-0e28-4452-a940-bc7174467aa2</td><td>D018</td><td>C023</td><td>F003</td><td>487.03</td><td>24.35</td><td>48.7</td><td>Yet school street leg threat interest.</td><td>2025-01-02T03:25:40</td></tr><tr><td>807c9a5f-7a78-4f53-bd27-e854871c9ba5</td><td>D010</td><td>C006</td><td>F002</td><td>522.04</td><td>26.1</td><td>52.2</td><td>Guess day wind father value science provide.</td><td>2025-01-08T12:37:50</td></tr><tr><td>99934e57-8e89-4fa2-babb-bee4278f2c8b</td><td>D012</td><td>C002</td><td>F002</td><td>105.31</td><td>5.27</td><td>10.53</td><td>May nothing tend arrive tell edge.</td><td>2025-04-23T11:20:24</td></tr><tr><td>52bc7780-abd5-4530-bb2d-f5304fe50afd</td><td>D007</td><td>C018</td><td>F001</td><td>569.69</td><td>28.48</td><td>56.97</td><td>Tell sort easy give bank music customer half.</td><td>2025-03-05T13:00:16</td></tr><tr><td>a42ddf7e-c25e-421c-aaca-d861869cc55c</td><td>D003</td><td>C007</td><td>F001</td><td>166.93</td><td>8.35</td><td>16.69</td><td>Activity source wonder carry become front conference cost.</td><td>2025-02-26T00:39:56</td></tr><tr><td>58dfe33e-1ef3-42dc-96bc-674bf0efef5b</td><td>D005</td><td>C005</td><td>F002</td><td>166.63</td><td>8.33</td><td>16.66</td><td>Describe image not reach toward sing entire.</td><td>2025-02-19T20:45:59</td></tr><tr><td>b0cd40cb-c7b0-48ff-908c-9d5ae0a5bfd7</td><td>D018</td><td>C008</td><td>F005</td><td>127.84</td><td>6.39</td><td>12.78</td><td>Ok local election.</td><td>2025-02-04T13:12:25</td></tr><tr><td>88afc54d-bdc7-4507-ad77-19c9917460e0</td><td>D002</td><td>C002</td><td>F004</td><td>873.09</td><td>43.65</td><td>87.31</td><td>Air compare here or speech.</td><td>2025-04-06T14:16:25</td></tr><tr><td>04bcb483-0e10-442d-917f-02c2a793fead</td><td>D015</td><td>C023</td><td>F005</td><td>432.79</td><td>21.64</td><td>43.28</td><td>Economy college to able minute deep history.</td><td>2025-03-15T09:56:40</td></tr><tr><td>c26e45dc-a480-445a-9e66-0b39b291cd46</td><td>D015</td><td>C006</td><td>F005</td><td>545.06</td><td>27.25</td><td>54.51</td><td>Where all some none money mean administration heavy.</td><td>2025-04-30T08:58:33</td></tr><tr><td>beb3db9a-d1af-4e53-90c4-14739f1429b7</td><td>D018</td><td>C009</td><td>F005</td><td>368.96</td><td>18.45</td><td>36.9</td><td>Call newspaper common result book next.</td><td>2025-04-18T08:05:48</td></tr><tr><td>6eb334ff-0bc7-4d7a-889e-872fa92cede8</td><td>D014</td><td>C013</td><td>F003</td><td>192.75</td><td>9.64</td><td>19.28</td><td>Help country without specific.</td><td>2025-04-14T09:54:35</td></tr><tr><td>dcbedb39-8785-42e7-b72f-e133be7f34db</td><td>D003</td><td>C007</td><td>F003</td><td>148.22</td><td>7.41</td><td>14.82</td><td>North in become best keep toward.</td><td>2025-02-03T07:30:05</td></tr><tr><td>1339070b-58c2-4fa2-a424-4f34b02b4ff8</td><td>D007</td><td>C002</td><td>F003</td><td>170.46</td><td>8.52</td><td>17.05</td><td>Finally billion people traditional writer safe air ask.</td><td>2025-05-01T05:17:41</td></tr><tr><td>a7569219-4cdf-41ab-a25b-9c02a3392615</td><td>D010</td><td>C015</td><td>F002</td><td>614.87</td><td>30.74</td><td>61.49</td><td>Measure international project because but.</td><td>2025-04-13T00:58:24</td></tr><tr><td>f566cc55-6cc3-4adb-82b7-9dbcd9af1c94</td><td>D016</td><td>C025</td><td>F004</td><td>834.45</td><td>41.72</td><td>83.45</td><td>Run nor color.</td><td>2025-03-22T16:24:59</td></tr><tr><td>4a92ab44-9399-47c3-93cf-d97b1dc866e4</td><td>D013</td><td>C007</td><td>F001</td><td>996.97</td><td>49.85</td><td>99.7</td><td>Speech Mrs fall include detail.</td><td>2025-04-10T02:46:15</td></tr><tr><td>2465c743-4d3e-454a-aab9-e57727d62743</td><td>D017</td><td>C004</td><td>F004</td><td>148.39</td><td>7.42</td><td>14.84</td><td>Money reality subject but Republican consumer care eat.</td><td>2025-04-01T00:30:44</td></tr><tr><td>76a9525e-2f35-49ec-8c87-5e3fa70544b6</td><td>D006</td><td>C028</td><td>F002</td><td>135.22</td><td>6.76</td><td>13.52</td><td>What involve more media worker pretty.</td><td>2025-01-17T04:08:30</td></tr><tr><td>0d5474fe-9d08-462d-ad70-9bd47cb4a4a2</td><td>D014</td><td>C020</td><td>F003</td><td>606.23</td><td>30.31</td><td>60.62</td><td>Soldier range goal yourself family theory doctor.</td><td>2025-02-23T21:22:53</td></tr><tr><td>ea17aefc-6510-4379-9c0b-ac4f41b72432</td><td>D004</td><td>C015</td><td>F003</td><td>861.58</td><td>43.08</td><td>86.16</td><td>Water probably table manage here million later.</td><td>2025-02-09T20:05:55</td></tr><tr><td>479cf616-5901-4743-ac37-2eb651f07e7f</td><td>D001</td><td>C027</td><td>F004</td><td>271.08</td><td>13.55</td><td>27.11</td><td>Night no measure management call try run stand.</td><td>2025-01-04T13:23:51</td></tr><tr><td>48d5fd6e-7792-4a32-9f6b-51d7d5c425f9</td><td>D013</td><td>C001</td><td>F002</td><td>337.56</td><td>16.88</td><td>33.76</td><td>Itself heavy top south across.</td><td>2025-03-07T01:43:41</td></tr><tr><td>9e68c495-6fbe-414a-9dcd-77ebadca6cf7</td><td>D009</td><td>C007</td><td>F004</td><td>444.18</td><td>22.21</td><td>44.42</td><td>Media describe father occur ten likely ask.</td><td>2025-01-22T23:00:27</td></tr><tr><td>dcf921da-5f3e-4346-81fd-ca63e3024cf0</td><td>D014</td><td>C028</td><td>F003</td><td>145.29</td><td>7.26</td><td>14.53</td><td>Company compare sound minute computer condition various.</td><td>2025-01-23T07:59:00</td></tr><tr><td>e4ef4fb1-11af-4164-a95d-4245e868fc7f</td><td>D018</td><td>C012</td><td>F003</td><td>758.33</td><td>37.92</td><td>75.83</td><td>Break involve other computer another area.</td><td>2025-02-12T01:45:12</td></tr><tr><td>d5153b0d-6380-4302-ace4-425e914384cd</td><td>D002</td><td>C012</td><td>F005</td><td>577.63</td><td>28.88</td><td>57.76</td><td>Along blue about rock full individual.</td><td>2025-04-28T15:21:00</td></tr><tr><td>afab9bf9-01b3-4b64-9a67-a2a2f73cc2a3</td><td>D018</td><td>C026</td><td>F002</td><td>297.38</td><td>14.87</td><td>29.74</td><td>Probably edge officer wide he size management.</td><td>2025-04-08T11:00:14</td></tr><tr><td>7ac84269-c851-4175-91b2-400e975d6370</td><td>D010</td><td>C012</td><td>F002</td><td>245.03</td><td>12.25</td><td>24.5</td><td>Down become yes report.</td><td>2025-01-01T15:03:33</td></tr><tr><td>3c869021-d070-465a-9e20-0e761fe9f0c5</td><td>D015</td><td>C007</td><td>F003</td><td>232.35</td><td>11.62</td><td>23.23</td><td>Despite free man director.</td><td>2025-04-22T01:38:52</td></tr><tr><td>c148df81-fdb9-4fb3-bbc2-f8eb2f17f77a</td><td>D017</td><td>C004</td><td>F005</td><td>367.59</td><td>18.38</td><td>36.76</td><td>Something these or.</td><td>2025-02-11T02:37:40</td></tr><tr><td>dcd12f6b-954e-46c9-940f-d4527d1f392f</td><td>D019</td><td>C002</td><td>F002</td><td>968.72</td><td>48.44</td><td>96.87</td><td>Data apply Mr future popular.</td><td>2025-01-31T05:32:54</td></tr><tr><td>71f626c5-6802-4a1d-a316-8db3982699ff</td><td>D011</td><td>C012</td><td>F005</td><td>186.81</td><td>9.34</td><td>18.68</td><td>Group piece writer ability stage visit.</td><td>2025-03-21T15:56:27</td></tr><tr><td>8dba4c49-de31-4441-89fb-65417d3993bf</td><td>D017</td><td>C010</td><td>F001</td><td>809.75</td><td>40.49</td><td>80.98</td><td>General go room away human.</td><td>2025-01-17T00:25:43</td></tr><tr><td>e9846661-340e-4d74-a394-fcd904ad36a9</td><td>D012</td><td>C013</td><td>F002</td><td>602.2</td><td>30.11</td><td>60.22</td><td>Foot high just structure decade.</td><td>2025-02-15T03:46:13</td></tr><tr><td>7714873d-8688-4980-89d9-145479320b04</td><td>D010</td><td>C029</td><td>F002</td><td>457.53</td><td>22.88</td><td>45.75</td><td>Go hear detail voice.</td><td>2025-04-07T03:16:31</td></tr><tr><td>3d34b9d3-5775-4b3c-bdf2-a4689c252b2f</td><td>D007</td><td>C001</td><td>F003</td><td>287.2</td><td>14.36</td><td>28.72</td><td>Arm girl population hit author sound already.</td><td>2025-03-03T09:18:26</td></tr><tr><td>527947ed-91d6-4ee5-95cd-d3cc01eb9ccc</td><td>D019</td><td>C008</td><td>F003</td><td>181.01</td><td>9.05</td><td>18.1</td><td>Company citizen pass Mr senior high protect.</td><td>2025-02-21T13:22:17</td></tr><tr><td>66b53845-1c36-4946-9cbc-45f8a04bb6e0</td><td>D013</td><td>C007</td><td>F001</td><td>731.34</td><td>36.57</td><td>73.13</td><td>Hot everything rest while probably act cause.</td><td>2025-04-29T01:56:55</td></tr><tr><td>7038601d-afbe-4a8d-b9a3-ec0092a7519b</td><td>D015</td><td>C029</td><td>F001</td><td>429.95</td><td>21.5</td><td>43.0</td><td>Spring before town wait.</td><td>2025-03-12T16:00:03</td></tr><tr><td>01ce6326-dbbf-408d-a35d-692cfe80bf66</td><td>D006</td><td>C015</td><td>F002</td><td>506.37</td><td>25.32</td><td>50.64</td><td>Lawyer during cost however focus realize identify.</td><td>2025-02-10T20:07:16</td></tr><tr><td>b57db7ed-e2de-4b96-9375-8cfbea373165</td><td>D015</td><td>C015</td><td>F002</td><td>633.5</td><td>31.68</td><td>63.35</td><td>Lose draw growth throw prove say chance yourself.</td><td>2025-02-13T01:50:05</td></tr><tr><td>58a33a9c-6978-4a50-8cc3-36fad76b6fcb</td><td>D012</td><td>C016</td><td>F002</td><td>328.77</td><td>16.44</td><td>32.88</td><td>Nation when least sort.</td><td>2025-02-08T18:19:14</td></tr><tr><td>a85a36c7-cb00-4682-9728-1aa182333bc6</td><td>D019</td><td>C009</td><td>F001</td><td>944.77</td><td>47.24</td><td>94.48</td><td>Bar wide really grow person garden firm through.</td><td>2025-02-06T12:51:36</td></tr><tr><td>84a43384-7483-4b2e-869b-f5f95c15ecea</td><td>D017</td><td>C002</td><td>F005</td><td>509.67</td><td>25.48</td><td>50.97</td><td>Source coach thing meeting lay son.</td><td>2025-01-23T06:08:15</td></tr><tr><td>63843662-706d-48d2-85cf-c02396ef42ca</td><td>D007</td><td>C015</td><td>F003</td><td>334.29</td><td>16.71</td><td>33.43</td><td>Stand kid debate.</td><td>2025-01-03T03:43:12</td></tr><tr><td>b0e5d322-3ab4-4f8f-947e-939ce0256f0b</td><td>D017</td><td>C029</td><td>F001</td><td>500.33</td><td>25.02</td><td>50.03</td><td>Role store single benefit technology world.</td><td>2025-01-12T16:30:28</td></tr><tr><td>445083be-e0ad-44d9-91fa-dd18b04719be</td><td>D015</td><td>C013</td><td>F001</td><td>101.83</td><td>5.09</td><td>10.18</td><td>Design yes human than.</td><td>2025-04-05T06:16:48</td></tr><tr><td>9b510699-b96e-454d-9cb3-501870aeb30a</td><td>D006</td><td>C006</td><td>F004</td><td>358.03</td><td>17.9</td><td>35.8</td><td>See benefit individual care TV three course.</td><td>2025-01-16T05:40:28</td></tr><tr><td>91c35e54-87a9-468d-be0b-3aecd0b6a5b0</td><td>D003</td><td>C006</td><td>F002</td><td>796.18</td><td>39.81</td><td>79.62</td><td>Board enjoy direction system blue.</td><td>2025-02-01T06:55:16</td></tr><tr><td>6d87f888-b126-4438-82db-2c07a4ce06bf</td><td>D018</td><td>C006</td><td>F004</td><td>116.95</td><td>5.85</td><td>11.7</td><td>Thank quality together most wait.</td><td>2025-03-26T23:55:08</td></tr><tr><td>0ff473f3-a68a-4947-a82b-e5ca08f1029e</td><td>D011</td><td>C024</td><td>F003</td><td>117.83</td><td>5.89</td><td>11.78</td><td>Town ask detail near never along we.</td><td>2025-02-08T01:02:22</td></tr><tr><td>012e5e36-9d6b-4155-8e1b-460b73a54410</td><td>D016</td><td>C002</td><td>F001</td><td>107.94</td><td>5.4</td><td>10.79</td><td>Rich your road.</td><td>2025-01-09T17:33:54</td></tr><tr><td>68e1fe9a-bed4-4803-b2ea-54370a8b6a83</td><td>D014</td><td>C001</td><td>F002</td><td>894.68</td><td>44.73</td><td>89.47</td><td>Wait worry human exactly whether.</td><td>2025-03-08T00:08:57</td></tr><tr><td>98613f79-3d7c-4805-9495-564969f5e46f</td><td>D010</td><td>C017</td><td>F001</td><td>887.8</td><td>44.39</td><td>88.78</td><td>Office PM both consumer size capital information.</td><td>2025-03-08T10:30:35</td></tr><tr><td>387da213-a938-44b0-9627-8352cf78bca0</td><td>D007</td><td>C018</td><td>F005</td><td>444.44</td><td>22.22</td><td>44.44</td><td>Subject avoid read second here.</td><td>2025-04-22T23:01:43</td></tr><tr><td>5ae11472-4817-4c1a-b656-039f5013e331</td><td>D003</td><td>C008</td><td>F002</td><td>552.78</td><td>27.64</td><td>55.28</td><td>Camera same and possible wife few.</td><td>2025-01-26T02:04:46</td></tr><tr><td>0c9df3e9-2daf-4a6a-89b8-5593fdf1d012</td><td>D004</td><td>C016</td><td>F005</td><td>945.13</td><td>47.26</td><td>94.51</td><td>Tax rock there customer price glass.</td><td>2025-01-18T22:50:56</td></tr><tr><td>4c1a19e7-e6bf-407e-876f-05585e6e377f</td><td>D004</td><td>C028</td><td>F001</td><td>123.28</td><td>6.16</td><td>12.33</td><td>Up on Republican mother eight.</td><td>2025-03-31T06:55:59</td></tr><tr><td>425c38ab-11c3-45cb-aa68-a77174868f9d</td><td>D020</td><td>C025</td><td>F003</td><td>399.56</td><td>19.98</td><td>39.96</td><td>Part person watch put.</td><td>2025-02-17T12:49:19</td></tr><tr><td>6eb0ef06-bbb9-44f6-88d0-d6af11ad1623</td><td>D017</td><td>C022</td><td>F003</td><td>597.06</td><td>29.85</td><td>59.71</td><td>Now decision western watch ahead.</td><td>2025-02-01T23:26:22</td></tr><tr><td>327c5086-90e2-45cd-934d-3a8d928a51a2</td><td>D002</td><td>C021</td><td>F002</td><td>511.6</td><td>25.58</td><td>51.16</td><td>Small see son single hundred.</td><td>2025-04-17T08:53:10</td></tr><tr><td>f04385d5-dce9-42e2-b27e-c10a5d7c0ae1</td><td>D001</td><td>C016</td><td>F001</td><td>750.86</td><td>37.54</td><td>75.09</td><td>Vote condition bill couple.</td><td>2025-04-20T00:36:52</td></tr><tr><td>508cbaa3-1ff3-4468-a65c-f81905811612</td><td>D018</td><td>C012</td><td>F005</td><td>356.89</td><td>17.84</td><td>35.69</td><td>Customer wear issue adult good reason.</td><td>2025-04-20T21:58:05</td></tr><tr><td>e517d310-4f85-42cb-afa8-f4fdcd9ee261</td><td>D019</td><td>C005</td><td>F004</td><td>228.17</td><td>11.41</td><td>22.82</td><td>Evening page American hard voice argue behavior true.</td><td>2025-03-22T07:45:30</td></tr><tr><td>079ded85-6b03-47e5-a776-e7c4df791185</td><td>D008</td><td>C017</td><td>F003</td><td>101.39</td><td>5.07</td><td>10.14</td><td>Teacher civil big eight almost.</td><td>2025-04-02T00:05:05</td></tr><tr><td>5c520987-9701-4c94-acff-87df26e0549a</td><td>D012</td><td>C017</td><td>F002</td><td>696.86</td><td>34.84</td><td>69.69</td><td>Think inside huge language lose economic sit.</td><td>2025-04-12T05:22:58</td></tr><tr><td>5242b89f-7ac3-4ac0-b8af-175c0cf17331</td><td>D001</td><td>C006</td><td>F004</td><td>625.82</td><td>31.29</td><td>62.58</td><td>Level statement ahead country finally.</td><td>2025-02-22T03:53:56</td></tr><tr><td>b4e4201a-78e1-4553-a544-ab3d6bcad374</td><td>D007</td><td>C003</td><td>F005</td><td>245.35</td><td>12.27</td><td>24.54</td><td>Way same staff around physical plan game commercial.</td><td>2025-03-24T05:03:14</td></tr><tr><td>eafa13cf-0999-44c3-b2eb-fef7064f144f</td><td>D015</td><td>C030</td><td>F002</td><td>731.97</td><td>36.6</td><td>73.2</td><td>Seem suffer very or allow week month.</td><td>2025-03-09T14:36:10</td></tr><tr><td>1dd3cea0-fffe-442a-84d4-3f3722e5daae</td><td>D006</td><td>C005</td><td>F002</td><td>909.97</td><td>45.5</td><td>91.0</td><td>Huge stop parent step return effort.</td><td>2025-02-15T00:56:11</td></tr><tr><td>c767d446-4eba-4cb5-9eb3-966bd43e5c77</td><td>D002</td><td>C013</td><td>F002</td><td>610.72</td><td>30.54</td><td>61.07</td><td>Community magazine than movie green for four news.</td><td>2025-01-26T00:17:40</td></tr><tr><td>a3ba3fd7-e9c4-420c-b409-7c2e1901eb31</td><td>D017</td><td>C016</td><td>F003</td><td>474.86</td><td>23.74</td><td>47.49</td><td>Charge address happen thus employee foot suddenly.</td><td>2025-02-19T17:21:56</td></tr><tr><td>575cb2f9-1cb4-4619-9ecb-ab79b5a99b05</td><td>D004</td><td>C022</td><td>F003</td><td>452.73</td><td>22.64</td><td>45.27</td><td>Remain measure in per fish audience bring.</td><td>2025-01-19T01:30:25</td></tr><tr><td>b292bde2-25db-4b41-ae37-5e16fed5cf5d</td><td>D015</td><td>C021</td><td>F002</td><td>518.0</td><td>25.9</td><td>51.8</td><td>Walk hard young door audience hospital.</td><td>2025-04-14T20:47:28</td></tr><tr><td>a5915bdf-d2ee-4fb6-9a2e-37df9ff21955</td><td>D012</td><td>C006</td><td>F003</td><td>168.49</td><td>8.42</td><td>16.85</td><td>Condition moment customer role response look class.</td><td>2025-04-06T19:23:01</td></tr><tr><td>cc071b63-06cf-4dac-be3b-2234307080d3</td><td>D019</td><td>C003</td><td>F003</td><td>309.96</td><td>15.5</td><td>31.0</td><td>Structure skill consumer artist certain even prepare.</td><td>2025-02-26T18:38:58</td></tr><tr><td>04b63c45-fb8c-42e5-9fc8-cabee89944d0</td><td>D011</td><td>C019</td><td>F003</td><td>845.08</td><td>42.25</td><td>84.51</td><td>Election trip sister best.</td><td>2025-04-11T10:31:15</td></tr><tr><td>be2414e4-fc64-446c-a4df-9a0c15f307ba</td><td>D001</td><td>C011</td><td>F004</td><td>688.02</td><td>34.4</td><td>68.8</td><td>Nice themselves natural also three behind glass.</td><td>2025-04-28T12:37:04</td></tr><tr><td>032fe7b8-fe92-4aa9-a091-8f1d3ba5cadb</td><td>D002</td><td>C024</td><td>F001</td><td>179.67</td><td>8.98</td><td>17.97</td><td>Page well actually final common as.</td><td>2025-03-27T17:13:41</td></tr><tr><td>4dda6239-afae-4fb0-aabd-9475ff94bdef</td><td>D006</td><td>C019</td><td>F002</td><td>822.19</td><td>41.11</td><td>82.22</td><td>Beautiful lose skin.</td><td>2025-03-24T08:32:55</td></tr><tr><td>23eae1e5-7373-46f7-b995-0a143a323c5e</td><td>D013</td><td>C020</td><td>F005</td><td>528.09</td><td>26.4</td><td>52.81</td><td>Give word serious try dream.</td><td>2025-04-13T19:14:32</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "7b5f1dd0-cf2c-468c-8df6-c9a6d59d510e",
         "D008",
         "C013",
         "F002",
         "503.6",
         "25.18",
         "50.36",
         "Likely wrong forget.",
         "2025-03-23T21:47:00"
        ],
        [
         "4177127b-1679-494d-b738-39c7c9e0c4eb",
         "D016",
         "C027",
         "F002",
         "839.26",
         "41.96",
         "83.93",
         "Across to arrive condition human his enter.",
         "2025-02-03T10:44:56"
        ],
        [
         "c60d3a27-7750-494d-80a6-303ab9aeb3d5",
         "D009",
         "C014",
         "F002",
         "857.19",
         "42.86",
         "85.72",
         "Hand green miss may and.",
         "2025-02-09T21:59:10"
        ],
        [
         "90160447-47cb-4815-8f2d-c9f09e88cbe7",
         "D003",
         "C026",
         "F004",
         "770.04",
         "38.5",
         "77.0",
         "Pick quite story where cover again wall.",
         "2025-03-07T11:48:34"
        ],
        [
         "54112fce-7195-4166-9af2-7e56c4dd0b68",
         "D007",
         "C016",
         "F002",
         "982.39",
         "49.12",
         "98.24",
         "Even total material entire.",
         "2025-02-07T23:36:04"
        ],
        [
         "ce26049b-6bad-4bfe-aed6-787e64fa02b2",
         "D019",
         "C020",
         "F001",
         "182.13",
         "9.11",
         "18.21",
         "Black work which receive air subject argue.",
         "2025-01-04T20:06:20"
        ],
        [
         "9dcd1af1-16ad-451d-ab2f-3d6d9e2c2b8b",
         "D018",
         "C009",
         "F002",
         "173.1",
         "8.65",
         "17.31",
         "World itself direction second rise relationship purpose wait.",
         "2025-01-30T04:51:10"
        ],
        [
         "0d1a99bb-250c-421b-aaf2-c274cfcad711",
         "D003",
         "C017",
         "F003",
         "201.33",
         "10.07",
         "20.13",
         "Building marriage two worker also cover.",
         "2025-01-06T10:59:12"
        ],
        [
         "4f56056e-18d1-4737-869b-c36130387781",
         "D004",
         "C025",
         "F004",
         "992.14",
         "49.61",
         "99.21",
         "Particularly plant only card.",
         "2025-02-12T12:46:29"
        ],
        [
         "cffbc1da-521d-41d3-b5c7-bce3eaae7185",
         "D002",
         "C006",
         "F005",
         "973.23",
         "48.66",
         "97.32",
         "Difference about dark after value book.",
         "2025-02-19T08:04:27"
        ],
        [
         "be9b8abf-3767-4ad0-9f42-40b5fd99ca2b",
         "D015",
         "C008",
         "F004",
         "209.79",
         "10.49",
         "20.98",
         "Boy me inside admit build.",
         "2025-05-02T03:32:10"
        ],
        [
         "190cf34f-938a-4a4c-a9fb-e8bad79ecd14",
         "D015",
         "C030",
         "F001",
         "874.43",
         "43.72",
         "87.44",
         "Short pull several expert court.",
         "2025-01-16T15:02:33"
        ],
        [
         "2f3a1f7a-7b9c-418c-ad72-1d73bed07f95",
         "D020",
         "C030",
         "F005",
         "354.68",
         "17.73",
         "35.47",
         "Fire alone not indicate.",
         "2025-04-09T05:45:29"
        ],
        [
         "f34a3cdd-63d8-4dac-b394-824810f38cbe",
         "D017",
         "C025",
         "F001",
         "511.16",
         "25.56",
         "51.12",
         "Able language house enough.",
         "2025-02-08T17:14:43"
        ],
        [
         "ef5c27bb-03a7-4f8c-ad80-fe74b562ed8c",
         "D020",
         "C011",
         "F005",
         "557.94",
         "27.9",
         "55.79",
         "Almost money arm.",
         "2025-03-25T14:25:38"
        ],
        [
         "85bda5e0-551d-4f36-a434-fe61ed176f04",
         "D008",
         "C012",
         "F001",
         "658.35",
         "32.92",
         "65.84",
         "Coach account director current serve cost standard.",
         "2025-04-27T00:48:42"
        ],
        [
         "7351261e-520e-45ed-829f-855aad8c020c",
         "D003",
         "C029",
         "F005",
         "743.12",
         "37.16",
         "74.31",
         "Join hospital leg wait.",
         "2025-02-25T08:50:37"
        ],
        [
         "c1e2f65d-7e71-4d6c-a3d6-1ab410032fdb",
         "D012",
         "C003",
         "F001",
         "329.97",
         "16.5",
         "33.0",
         "Southern again identify must happen.",
         "2025-02-24T03:49:38"
        ],
        [
         "9832cff8-91ac-4ab6-a081-64c2d89dc594",
         "D004",
         "C030",
         "F002",
         "139.67",
         "6.98",
         "13.97",
         "Purpose must party.",
         "2025-03-26T00:44:36"
        ],
        [
         "937a0229-d655-4f56-b8e2-d98696415793",
         "D012",
         "C013",
         "F002",
         "440.75",
         "22.04",
         "44.08",
         "The energy training national option less late.",
         "2025-03-10T08:49:41"
        ],
        [
         "4a7f1a29-e440-46e3-b9e2-fa1c95129702",
         "D006",
         "C026",
         "F003",
         "781.95",
         "39.1",
         "78.2",
         "Focus hotel weight large say.",
         "2025-04-22T00:19:48"
        ],
        [
         "9b6aba5b-0efa-487a-b334-b91b4ef17d88",
         "D015",
         "C006",
         "F004",
         "463.62",
         "23.18",
         "46.36",
         "Write fight discuss play economic executive father.",
         "2025-04-28T04:38:12"
        ],
        [
         "be845ea9-ba9c-4c29-ae1b-a0ea32f9b4cf",
         "D003",
         "C012",
         "F002",
         "943.27",
         "47.16",
         "94.33",
         "Such general seek.",
         "2025-04-23T10:01:36"
        ],
        [
         "a68fa48a-77fd-4d76-a6d3-f69a513aec89",
         "D002",
         "C021",
         "F005",
         "796.91",
         "39.85",
         "79.69",
         "Paper notice nothing best store particular country.",
         "2025-02-23T06:10:58"
        ],
        [
         "b985331f-ab88-41ea-a426-66043567eb11",
         "D005",
         "C018",
         "F002",
         "399.66",
         "19.98",
         "39.97",
         "Glass writer modern people.",
         "2025-01-22T07:14:26"
        ],
        [
         "77569c2f-4eea-4495-9522-b5a8cdf20f0e",
         "D010",
         "C029",
         "F001",
         "428.21",
         "21.41",
         "42.82",
         "Audience discover party dark.",
         "2025-02-04T16:32:51"
        ],
        [
         "54018d64-7a63-4b69-a5ee-9a3f4c770760",
         "D012",
         "C029",
         "F003",
         "444.85",
         "22.24",
         "44.49",
         "Site knowledge perhaps century position edge clear.",
         "2025-01-23T12:20:05"
        ],
        [
         "916d7d71-6a01-4896-bcd9-1834dc01bdec",
         "D001",
         "C007",
         "F005",
         "611.42",
         "30.57",
         "61.14",
         "Like property social myself during produce office plant.",
         "2025-02-20T18:52:21"
        ],
        [
         "08005542-80f2-4ed9-b388-c4ccb9e0d572",
         "D018",
         "C026",
         "F003",
         "184.12",
         "9.21",
         "18.41",
         "Lay budget stop image it per education.",
         "2025-02-02T13:13:17"
        ],
        [
         "e8a0ffab-c75e-4cca-ba29-df80227f1ab2",
         "D011",
         "C018",
         "F005",
         "624.01",
         "31.2",
         "62.4",
         "Create machine party me give.",
         "2025-04-05T22:43:45"
        ],
        [
         "64927ab8-62cd-4f13-911c-628c602c1dae",
         "D003",
         "C030",
         "F001",
         "663.63",
         "33.18",
         "66.36",
         "Animal reach couple against.",
         "2025-04-15T13:26:07"
        ],
        [
         "679fe366-92cf-47d9-97be-35f69f8b1fac",
         "D017",
         "C017",
         "F004",
         "966.03",
         "48.3",
         "96.6",
         "South senior deep fact economic use.",
         "2025-02-03T18:00:53"
        ],
        [
         "98c1c7b1-d725-40fe-ad53-2823572c4c41",
         "D004",
         "C026",
         "F001",
         "723.05",
         "36.15",
         "72.3",
         "Season once ahead painting.",
         "2025-02-04T14:01:57"
        ],
        [
         "b9f09a78-98df-48eb-a027-08f5f7f4c59b",
         "D010",
         "C013",
         "F001",
         "805.46",
         "40.27",
         "80.55",
         "Machine community professor without stand money finally collection.",
         "2025-01-20T16:03:59"
        ],
        [
         "9cccc597-82d4-4a59-968a-539d7edb12ce",
         "D015",
         "C029",
         "F002",
         "489.35",
         "24.47",
         "48.94",
         "Some local occur.",
         "2025-01-21T18:32:18"
        ],
        [
         "2b8df8b1-5567-47a9-a149-318db3ec0e27",
         "D004",
         "C001",
         "F005",
         "601.14",
         "30.06",
         "60.11",
         "Contain machine three will.",
         "2025-02-13T15:15:56"
        ],
        [
         "bcd35a07-8263-49df-b484-6118753ce60a",
         "D002",
         "C019",
         "F001",
         "624.78",
         "31.24",
         "62.48",
         "Difference particularly name course keep.",
         "2025-02-12T09:17:48"
        ],
        [
         "504c7f93-60e1-4a29-9a90-034195ee2a6d",
         "D018",
         "C021",
         "F004",
         "626.11",
         "31.31",
         "62.61",
         "Together avoid business sign.",
         "2025-04-09T16:17:14"
        ],
        [
         "4c61a761-0d91-485f-bc68-cbde3b67eedf",
         "D008",
         "C001",
         "F001",
         "725.27",
         "36.26",
         "72.53",
         "Live professor training seat beyond also late.",
         "2025-04-11T03:56:37"
        ],
        [
         "d17c65b7-639f-4722-91c3-b70c1f8e1f21",
         "D001",
         "C009",
         "F005",
         "882.8",
         "44.14",
         "88.28",
         "Bring generation some administration defense oil hand.",
         "2025-04-04T11:03:45"
        ],
        [
         "66c0f6d8-2d00-48d2-bf51-8d37c7a2f9d1",
         "D003",
         "C030",
         "F002",
         "562.83",
         "28.14",
         "56.28",
         "Paper including opportunity.",
         "2025-01-16T03:33:48"
        ],
        [
         "c225fc9e-6188-4425-8be7-035f4d7f3be5",
         "D020",
         "C012",
         "F002",
         "452.4",
         "22.62",
         "45.24",
         "Table stand represent between keep.",
         "2025-03-10T15:34:30"
        ],
        [
         "6588aeaf-5593-48cb-91a7-fa61556b2b7c",
         "D006",
         "C004",
         "F005",
         "183.96",
         "9.2",
         "18.4",
         "Drop fish hair.",
         "2025-03-30T22:22:33"
        ],
        [
         "748a09ee-293c-48a8-bee9-9697dcc5b856",
         "D015",
         "C008",
         "F004",
         "923.47",
         "46.17",
         "92.35",
         "Direction back box property life event short marriage.",
         "2025-04-16T08:55:44"
        ],
        [
         "b547ddd9-27e6-488e-beb9-12631b01b8ec",
         "D006",
         "C026",
         "F005",
         "520.04",
         "26.0",
         "52.0",
         "Great until name behind last.",
         "2025-01-22T22:37:34"
        ],
        [
         "0da2af49-1554-45f9-9829-d13c3823a394",
         "D012",
         "C015",
         "F005",
         "310.28",
         "15.51",
         "31.03",
         "Real as force home pattern.",
         "2025-01-08T04:24:46"
        ],
        [
         "09098dcb-e141-4bce-af0d-e1d2a1bb521a",
         "D002",
         "C013",
         "F002",
         "141.88",
         "7.09",
         "14.19",
         "Consider include herself three yes explain throw must.",
         "2025-01-05T13:31:28"
        ],
        [
         "7685022d-1dad-441e-a482-af9e3f52aa08",
         "D013",
         "C017",
         "F004",
         "894.89",
         "44.74",
         "89.49",
         "Issue federal type wish effort different film.",
         "2025-03-25T18:27:12"
        ],
        [
         "3b82769f-0440-4154-abf6-5fe4b42283c2",
         "D007",
         "C020",
         "F005",
         "965.93",
         "48.3",
         "96.59",
         "Town evening person campaign.",
         "2025-02-21T23:21:19"
        ],
        [
         "5e576714-0ac8-4b2f-8c2f-796b62171ad6",
         "D007",
         "C003",
         "F001",
         "881.52",
         "44.08",
         "88.15",
         "Discuss theory subject agent research would head.",
         "2025-02-07T09:18:06"
        ],
        [
         "696b0148-3cd1-44e1-b4ee-51369e75a31f",
         "D005",
         "C021",
         "F001",
         "600.1",
         "30.01",
         "60.01",
         "Reach piece trouble have fast positive best.",
         "2025-02-18T03:04:44"
        ],
        [
         "3d96c4a7-1840-4d38-b9fc-532c022e5e59",
         "D003",
         "C017",
         "F003",
         "295.69",
         "14.78",
         "29.57",
         "Our to along run.",
         "2025-04-15T12:14:51"
        ],
        [
         "f35190c0-7874-41f3-bef6-010efc718914",
         "D011",
         "C023",
         "F005",
         "626.1",
         "31.31",
         "62.61",
         "Include city few commercial nothing either just.",
         "2025-04-05T09:34:36"
        ],
        [
         "d4c2b66f-320e-440d-a22b-a4bbc6bcbff9",
         "D009",
         "C001",
         "F005",
         "887.78",
         "44.39",
         "88.78",
         "Certain kitchen real issue memory morning.",
         "2025-01-03T17:00:06"
        ],
        [
         "a0a40acc-ac18-4ccb-ba58-5d460b580709",
         "D010",
         "C005",
         "F001",
         "616.79",
         "30.84",
         "61.68",
         "Enjoy describe stand reflect car stage win.",
         "2025-02-03T04:31:27"
        ],
        [
         "de35f987-20e3-4d7b-a288-ff81477ee092",
         "D017",
         "C010",
         "F002",
         "168.35",
         "8.42",
         "16.84",
         "Common local full enjoy race heart more.",
         "2025-01-24T05:16:45"
        ],
        [
         "fc9966a6-bd32-49d1-8db4-6743599be83b",
         "D016",
         "C008",
         "F003",
         "402.62",
         "20.13",
         "40.26",
         "May deal answer three.",
         "2025-01-20T19:36:05"
        ],
        [
         "fbfefd7e-c02c-4318-9fdb-41bd2848c46a",
         "D010",
         "C016",
         "F004",
         "409.89",
         "20.49",
         "40.99",
         "Generation know magazine ground week.",
         "2025-03-25T17:23:44"
        ],
        [
         "54fce3aa-8b33-4bca-a564-9a2d5a409717",
         "D011",
         "C030",
         "F002",
         "382.21",
         "19.11",
         "38.22",
         "Successful start amount word.",
         "2025-03-26T15:15:51"
        ],
        [
         "6f170ce8-25c9-4187-8a7c-36487eba4f78",
         "D007",
         "C020",
         "F001",
         "773.72",
         "38.69",
         "77.37",
         "Bill month push begin reality performance.",
         "2025-02-18T07:09:41"
        ],
        [
         "ea35f95b-b3d7-4b5d-a3f2-5514bee909ab",
         "D007",
         "C008",
         "F001",
         "947.35",
         "47.37",
         "94.74",
         "Old teacher happy.",
         "2025-04-08T22:01:05"
        ],
        [
         "f06f53b1-8fe8-4789-94bf-2009eab1fd5d",
         "D011",
         "C001",
         "F003",
         "862.25",
         "43.11",
         "86.23",
         "Against back quickly size science like standard especially.",
         "2025-03-10T16:20:38"
        ],
        [
         "887c2385-7251-4d9f-9c8a-f2d7ab28160e",
         "D005",
         "C002",
         "F003",
         "298.23",
         "14.91",
         "29.82",
         "Father can drive certainly happen set.",
         "2025-03-03T13:27:39"
        ],
        [
         "cc21317a-b024-474d-a7b3-1c9651d75c6c",
         "D014",
         "C019",
         "F002",
         "975.96",
         "48.8",
         "97.6",
         "American care science if away.",
         "2025-04-11T10:10:24"
        ],
        [
         "2f624787-6c32-436d-8912-82d56441334c",
         "D009",
         "C023",
         "F004",
         "727.38",
         "36.37",
         "72.74",
         "Feel imagine region one popular.",
         "2025-01-25T02:26:06"
        ],
        [
         "b088514b-7eb4-4549-8a12-f2795aeb74e2",
         "D010",
         "C017",
         "F004",
         "755.91",
         "37.8",
         "75.59",
         "Opportunity who hard line shake respond visit.",
         "2025-01-12T11:20:28"
        ],
        [
         "32b99a74-b141-4cdb-881a-a5bbf0240f24",
         "D006",
         "C021",
         "F001",
         "465.39",
         "23.27",
         "46.54",
         "Find partner various score agree lay.",
         "2025-04-23T11:40:24"
        ],
        [
         "5d656490-4f1e-4dff-bce9-a168f8539cff",
         "D002",
         "C003",
         "F003",
         "558.43",
         "27.92",
         "55.84",
         "Call ago sister suffer Democrat upon.",
         "2025-04-19T12:13:24"
        ],
        [
         "2c683d09-6904-464d-8d96-ce122c7f9431",
         "D002",
         "C015",
         "F005",
         "632.78",
         "31.64",
         "63.28",
         "Walk item action mother mouth theory.",
         "2025-03-07T14:39:43"
        ],
        [
         "b1b596b0-9134-4fa2-8295-de66e3a147ae",
         "D015",
         "C029",
         "F004",
         "953.07",
         "47.65",
         "95.31",
         "Attack citizen plan strategy opportunity much.",
         "2025-01-09T20:31:39"
        ],
        [
         "dcf3ef47-3857-4b62-ae21-aac1414082f6",
         "D016",
         "C023",
         "F003",
         "594.5",
         "29.73",
         "59.45",
         "Community onto beyond keep reveal.",
         "2025-03-18T04:55:46"
        ],
        [
         "12ab2ca3-c0de-4f9f-83f2-1c0bdece4733",
         "D016",
         "C029",
         "F003",
         "692.48",
         "34.62",
         "69.25",
         "Bit fast gas difficult indeed financial.",
         "2025-04-05T00:01:41"
        ],
        [
         "16a427d9-063d-48ac-b52d-f321fa12d7cd",
         "D007",
         "C009",
         "F003",
         "675.49",
         "33.77",
         "67.55",
         "Move position significant parent upon put many.",
         "2025-03-12T13:29:55"
        ],
        [
         "62d2acc7-b622-459e-8a65-1b3765d2fa3c",
         "D008",
         "C020",
         "F005",
         "444.13",
         "22.21",
         "44.41",
         "Teach program mean.",
         "2025-05-03T21:36:19"
        ],
        [
         "8407cdda-718a-484e-b110-40b88c00e0cb",
         "D003",
         "C012",
         "F005",
         "637.07",
         "31.85",
         "63.71",
         "Half see then attorney before central care.",
         "2025-02-03T11:41:06"
        ],
        [
         "afdd75e1-d9ad-43e1-8b32-2db9beee24aa",
         "D019",
         "C025",
         "F004",
         "975.42",
         "48.77",
         "97.54",
         "Painting simply risk simply.",
         "2025-03-16T17:20:20"
        ],
        [
         "06b6f24c-4387-454b-a242-09c8634eec06",
         "D015",
         "C011",
         "F001",
         "487.42",
         "24.37",
         "48.74",
         "Commercial maybe describe.",
         "2025-02-27T20:15:54"
        ],
        [
         "cc4f9c61-0456-4c13-bdd9-c876adf77565",
         "D012",
         "C030",
         "F001",
         "199.41",
         "9.97",
         "19.94",
         "Fear visit mother yeah.",
         "2025-02-26T14:45:21"
        ],
        [
         "774fbf66-61e6-44c5-9f7d-ce344627fd41",
         "D005",
         "C011",
         "F002",
         "463.69",
         "23.18",
         "46.37",
         "Image themselves public pattern now prepare.",
         "2025-02-16T22:15:19"
        ],
        [
         "e4bf27e0-9266-499f-95b6-a7384f266ad6",
         "D016",
         "C004",
         "F004",
         "389.94",
         "19.5",
         "38.99",
         "Moment pick sister admit month condition.",
         "2025-02-07T17:43:34"
        ],
        [
         "31b168bc-cc95-42ee-9b1d-5d7af1d2990d",
         "D013",
         "C006",
         "F004",
         "967.02",
         "48.35",
         "96.7",
         "Ground Democrat in.",
         "2025-04-18T03:04:20"
        ],
        [
         "c783107c-2800-42d3-bb39-f33b774e2218",
         "D009",
         "C005",
         "F001",
         "650.11",
         "32.51",
         "65.01",
         "World me card people they them environment.",
         "2025-02-16T06:58:15"
        ],
        [
         "9c91cf9f-4625-4690-a999-d6662bd6c0ef",
         "D011",
         "C002",
         "F004",
         "457.92",
         "22.9",
         "45.79",
         "Music social possible action success growth kid.",
         "2025-04-03T16:58:14"
        ],
        [
         "3b1c23eb-b24a-436f-8d3a-8b2b650fb4b0",
         "D008",
         "C019",
         "F002",
         "865.12",
         "43.26",
         "86.51",
         "Political choose practice.",
         "2025-04-05T14:01:11"
        ],
        [
         "f9b797c5-a0f6-4e06-ba16-53c82fa7c026",
         "D002",
         "C009",
         "F003",
         "822.38",
         "41.12",
         "82.24",
         "Loss management everyone.",
         "2025-02-05T13:21:59"
        ],
        [
         "877f9df3-9c8b-463e-959a-f88312f9ac2d",
         "D007",
         "C011",
         "F001",
         "133.99",
         "6.7",
         "13.4",
         "Themselves eat rise production push interest.",
         "2025-04-25T12:00:27"
        ],
        [
         "80cafb2a-b335-46dd-a3aa-48c64b834966",
         "D010",
         "C003",
         "F003",
         "387.27",
         "19.36",
         "38.73",
         "Education evidence improve toward.",
         "2025-02-28T12:08:47"
        ],
        [
         "023711c9-7b79-4d39-8a7b-3d9afab4831f",
         "D007",
         "C003",
         "F002",
         "924.9",
         "46.25",
         "92.49",
         "Exactly matter tax her plant appear unit.",
         "2025-04-23T06:25:52"
        ],
        [
         "6d5df8b0-2977-4899-be49-b09799860522",
         "D004",
         "C030",
         "F002",
         "266.56",
         "13.33",
         "26.66",
         "Film large another project new always tough.",
         "2025-02-01T13:54:26"
        ],
        [
         "c32e7311-10cf-4e27-b744-1aae96fbd701",
         "D002",
         "C024",
         "F004",
         "512.1",
         "25.61",
         "51.21",
         "Image generation significant organization want low star.",
         "2025-01-18T14:57:42"
        ],
        [
         "b22d1756-2586-48b7-8daf-13e08d935e00",
         "D008",
         "C027",
         "F003",
         "854.22",
         "42.71",
         "85.42",
         "Sign them easy eight hear power rather.",
         "2025-03-05T12:58:10"
        ],
        [
         "aa1751fd-6dbb-4b3c-8319-2bcbd5e60143",
         "D011",
         "C012",
         "F005",
         "941.84",
         "47.09",
         "94.18",
         "Son party region responsibility.",
         "2025-01-06T01:16:21"
        ],
        [
         "3366f46b-1c38-4b22-b7f8-72a763cfc5d6",
         "D019",
         "C016",
         "F002",
         "670.19",
         "33.51",
         "67.02",
         "Admit prepare generation than general say available.",
         "2025-03-18T16:56:24"
        ],
        [
         "7973f749-e05b-4459-a9fb-f4d647a70fd0",
         "D001",
         "C005",
         "F004",
         "189.22",
         "9.46",
         "18.92",
         "Major key remember watch whether cultural white.",
         "2025-03-03T17:12:33"
        ],
        [
         "8324cc49-e0da-4fe8-b3ee-09a43bb8ad02",
         "D012",
         "C014",
         "F004",
         "403.76",
         "20.19",
         "40.38",
         "Morning end glass difference level put.",
         "2025-01-15T03:23:10"
        ],
        [
         "289cb7fb-7c4d-46f5-bcbb-cd036ae428d6",
         "D006",
         "C022",
         "F001",
         "550.51",
         "27.53",
         "55.05",
         "Game ago then value source provide.",
         "2025-01-04T12:05:28"
        ],
        [
         "16ceacb7-e622-4b31-99d7-7adab4f36864",
         "D009",
         "C006",
         "F004",
         "793.01",
         "39.65",
         "79.3",
         "Doctor rise per hear.",
         "2025-02-26T10:24:22"
        ],
        [
         "a96d9052-ec81-4587-bdf6-78b9a1475ac9",
         "D003",
         "C023",
         "F002",
         "968.98",
         "48.45",
         "96.9",
         "Development eye just.",
         "2025-01-27T18:57:16"
        ],
        [
         "6641999d-cc98-4556-9f5b-eb39f822d37c",
         "D014",
         "C026",
         "F004",
         "355.16",
         "17.76",
         "35.52",
         "Gas service benefit.",
         "2025-01-15T18:32:11"
        ],
        [
         "83c68623-4664-4067-8257-96e4e2e58980",
         "D014",
         "C005",
         "F001",
         "255.32",
         "12.77",
         "25.53",
         "Lose pull decide watch.",
         "2025-01-04T00:27:04"
        ],
        [
         "2b55c97b-a913-4559-90fd-14ad9a138a9d",
         "D007",
         "C025",
         "F002",
         "653.5",
         "32.68",
         "65.35",
         "Shake kid common then term race.",
         "2025-02-18T03:48:10"
        ],
        [
         "153be433-7a21-40b4-964a-a47981c0e91e",
         "D013",
         "C006",
         "F004",
         "295.09",
         "14.75",
         "29.51",
         "Become possible allow past.",
         "2025-01-23T14:10:10"
        ],
        [
         "252747b5-6147-4e01-a734-159a480c676c",
         "D016",
         "C020",
         "F001",
         "135.35",
         "6.77",
         "13.54",
         "Toward enter pay past field.",
         "2025-01-31T18:47:54"
        ],
        [
         "528b6c21-6daa-44bb-9302-3d82491e16a6",
         "D010",
         "C010",
         "F003",
         "843.68",
         "42.18",
         "84.37",
         "Look hope science site window.",
         "2025-03-11T04:03:34"
        ],
        [
         "2eefba32-3082-46d6-b5d7-7bf66c00b126",
         "D015",
         "C012",
         "F002",
         "126.2",
         "6.31",
         "12.62",
         "Happen not second inside.",
         "2025-01-26T08:24:17"
        ],
        [
         "292f8c04-aea0-437a-a20d-b8c67b13da07",
         "D015",
         "C022",
         "F003",
         "682.22",
         "34.11",
         "68.22",
         "Simply with place turn.",
         "2025-04-14T20:04:08"
        ],
        [
         "1c16fd1d-15c8-44d0-a5d6-181b90716751",
         "D003",
         "C017",
         "F001",
         "352.82",
         "17.64",
         "35.28",
         "Attention form cut daughter series.",
         "2025-04-15T04:53:34"
        ],
        [
         "6c113539-350a-455a-92bd-a0e88efc64ea",
         "D018",
         "C027",
         "F005",
         "664.96",
         "33.25",
         "66.5",
         "Opportunity treat government though and.",
         "2025-03-26T09:28:38"
        ],
        [
         "c5e9e6c7-96d3-4c9c-8b0d-16b86b46b8cd",
         "D011",
         "C006",
         "F001",
         "435.22",
         "21.76",
         "43.52",
         "Cause fine research heart enter.",
         "2025-03-23T04:56:21"
        ],
        [
         "421cc1a8-7fe8-4e86-a7ab-373587cd3cff",
         "D008",
         "C011",
         "F004",
         "664.67",
         "33.23",
         "66.47",
         "Staff right me quite floor pretty goal push.",
         "2025-01-25T07:06:27"
        ],
        [
         "b58aca8a-02ce-44e7-a006-0f0ba8c94dbd",
         "D010",
         "C016",
         "F004",
         "798.3",
         "39.91",
         "79.83",
         "Address culture player may.",
         "2025-02-15T15:17:14"
        ],
        [
         "21f2f468-5f75-40a4-8bc1-753a42b9ef48",
         "D010",
         "C017",
         "F005",
         "521.3",
         "26.06",
         "52.13",
         "No clear house worry better drop into.",
         "2025-01-29T15:13:07"
        ],
        [
         "f3375df1-6933-4d6e-a002-328c53b1687e",
         "D016",
         "C024",
         "F005",
         "726.07",
         "36.3",
         "72.61",
         "Attorney strategy indicate.",
         "2025-01-16T21:33:27"
        ],
        [
         "1969c237-bdef-4cdd-b4e4-82766dfc7d13",
         "D008",
         "C029",
         "F002",
         "879.35",
         "43.97",
         "87.94",
         "Federal adult executive recently strong.",
         "2025-01-13T19:48:09"
        ],
        [
         "00cf826c-4e7b-4278-98bb-04edf7914cbc",
         "D009",
         "C024",
         "F003",
         "547.76",
         "27.39",
         "54.78",
         "Energy ten once agreement.",
         "2025-01-04T02:15:37"
        ],
        [
         "8c908729-994c-4f5b-8857-d7bc3d12cf14",
         "D020",
         "C029",
         "F001",
         "917.99",
         "45.9",
         "91.8",
         "Per task agent produce.",
         "2025-03-06T01:19:44"
        ],
        [
         "ed682d65-430a-414a-8919-9a63dc1d86a8",
         "D010",
         "C026",
         "F001",
         "390.92",
         "19.55",
         "39.09",
         "Just program within true shake them rock school.",
         "2025-01-25T07:10:55"
        ],
        [
         "421dc8be-8b83-493f-9c0b-1ae8825d0050",
         "D006",
         "C027",
         "F003",
         "402.57",
         "20.13",
         "40.26",
         "Certain both least building white.",
         "2025-01-30T08:28:16"
        ],
        [
         "29b53e28-0e28-4452-a940-bc7174467aa2",
         "D018",
         "C023",
         "F003",
         "487.03",
         "24.35",
         "48.7",
         "Yet school street leg threat interest.",
         "2025-01-02T03:25:40"
        ],
        [
         "807c9a5f-7a78-4f53-bd27-e854871c9ba5",
         "D010",
         "C006",
         "F002",
         "522.04",
         "26.1",
         "52.2",
         "Guess day wind father value science provide.",
         "2025-01-08T12:37:50"
        ],
        [
         "99934e57-8e89-4fa2-babb-bee4278f2c8b",
         "D012",
         "C002",
         "F002",
         "105.31",
         "5.27",
         "10.53",
         "May nothing tend arrive tell edge.",
         "2025-04-23T11:20:24"
        ],
        [
         "52bc7780-abd5-4530-bb2d-f5304fe50afd",
         "D007",
         "C018",
         "F001",
         "569.69",
         "28.48",
         "56.97",
         "Tell sort easy give bank music customer half.",
         "2025-03-05T13:00:16"
        ],
        [
         "a42ddf7e-c25e-421c-aaca-d861869cc55c",
         "D003",
         "C007",
         "F001",
         "166.93",
         "8.35",
         "16.69",
         "Activity source wonder carry become front conference cost.",
         "2025-02-26T00:39:56"
        ],
        [
         "58dfe33e-1ef3-42dc-96bc-674bf0efef5b",
         "D005",
         "C005",
         "F002",
         "166.63",
         "8.33",
         "16.66",
         "Describe image not reach toward sing entire.",
         "2025-02-19T20:45:59"
        ],
        [
         "b0cd40cb-c7b0-48ff-908c-9d5ae0a5bfd7",
         "D018",
         "C008",
         "F005",
         "127.84",
         "6.39",
         "12.78",
         "Ok local election.",
         "2025-02-04T13:12:25"
        ],
        [
         "88afc54d-bdc7-4507-ad77-19c9917460e0",
         "D002",
         "C002",
         "F004",
         "873.09",
         "43.65",
         "87.31",
         "Air compare here or speech.",
         "2025-04-06T14:16:25"
        ],
        [
         "04bcb483-0e10-442d-917f-02c2a793fead",
         "D015",
         "C023",
         "F005",
         "432.79",
         "21.64",
         "43.28",
         "Economy college to able minute deep history.",
         "2025-03-15T09:56:40"
        ],
        [
         "c26e45dc-a480-445a-9e66-0b39b291cd46",
         "D015",
         "C006",
         "F005",
         "545.06",
         "27.25",
         "54.51",
         "Where all some none money mean administration heavy.",
         "2025-04-30T08:58:33"
        ],
        [
         "beb3db9a-d1af-4e53-90c4-14739f1429b7",
         "D018",
         "C009",
         "F005",
         "368.96",
         "18.45",
         "36.9",
         "Call newspaper common result book next.",
         "2025-04-18T08:05:48"
        ],
        [
         "6eb334ff-0bc7-4d7a-889e-872fa92cede8",
         "D014",
         "C013",
         "F003",
         "192.75",
         "9.64",
         "19.28",
         "Help country without specific.",
         "2025-04-14T09:54:35"
        ],
        [
         "dcbedb39-8785-42e7-b72f-e133be7f34db",
         "D003",
         "C007",
         "F003",
         "148.22",
         "7.41",
         "14.82",
         "North in become best keep toward.",
         "2025-02-03T07:30:05"
        ],
        [
         "1339070b-58c2-4fa2-a424-4f34b02b4ff8",
         "D007",
         "C002",
         "F003",
         "170.46",
         "8.52",
         "17.05",
         "Finally billion people traditional writer safe air ask.",
         "2025-05-01T05:17:41"
        ],
        [
         "a7569219-4cdf-41ab-a25b-9c02a3392615",
         "D010",
         "C015",
         "F002",
         "614.87",
         "30.74",
         "61.49",
         "Measure international project because but.",
         "2025-04-13T00:58:24"
        ],
        [
         "f566cc55-6cc3-4adb-82b7-9dbcd9af1c94",
         "D016",
         "C025",
         "F004",
         "834.45",
         "41.72",
         "83.45",
         "Run nor color.",
         "2025-03-22T16:24:59"
        ],
        [
         "4a92ab44-9399-47c3-93cf-d97b1dc866e4",
         "D013",
         "C007",
         "F001",
         "996.97",
         "49.85",
         "99.7",
         "Speech Mrs fall include detail.",
         "2025-04-10T02:46:15"
        ],
        [
         "2465c743-4d3e-454a-aab9-e57727d62743",
         "D017",
         "C004",
         "F004",
         "148.39",
         "7.42",
         "14.84",
         "Money reality subject but Republican consumer care eat.",
         "2025-04-01T00:30:44"
        ],
        [
         "76a9525e-2f35-49ec-8c87-5e3fa70544b6",
         "D006",
         "C028",
         "F002",
         "135.22",
         "6.76",
         "13.52",
         "What involve more media worker pretty.",
         "2025-01-17T04:08:30"
        ],
        [
         "0d5474fe-9d08-462d-ad70-9bd47cb4a4a2",
         "D014",
         "C020",
         "F003",
         "606.23",
         "30.31",
         "60.62",
         "Soldier range goal yourself family theory doctor.",
         "2025-02-23T21:22:53"
        ],
        [
         "ea17aefc-6510-4379-9c0b-ac4f41b72432",
         "D004",
         "C015",
         "F003",
         "861.58",
         "43.08",
         "86.16",
         "Water probably table manage here million later.",
         "2025-02-09T20:05:55"
        ],
        [
         "479cf616-5901-4743-ac37-2eb651f07e7f",
         "D001",
         "C027",
         "F004",
         "271.08",
         "13.55",
         "27.11",
         "Night no measure management call try run stand.",
         "2025-01-04T13:23:51"
        ],
        [
         "48d5fd6e-7792-4a32-9f6b-51d7d5c425f9",
         "D013",
         "C001",
         "F002",
         "337.56",
         "16.88",
         "33.76",
         "Itself heavy top south across.",
         "2025-03-07T01:43:41"
        ],
        [
         "9e68c495-6fbe-414a-9dcd-77ebadca6cf7",
         "D009",
         "C007",
         "F004",
         "444.18",
         "22.21",
         "44.42",
         "Media describe father occur ten likely ask.",
         "2025-01-22T23:00:27"
        ],
        [
         "dcf921da-5f3e-4346-81fd-ca63e3024cf0",
         "D014",
         "C028",
         "F003",
         "145.29",
         "7.26",
         "14.53",
         "Company compare sound minute computer condition various.",
         "2025-01-23T07:59:00"
        ],
        [
         "e4ef4fb1-11af-4164-a95d-4245e868fc7f",
         "D018",
         "C012",
         "F003",
         "758.33",
         "37.92",
         "75.83",
         "Break involve other computer another area.",
         "2025-02-12T01:45:12"
        ],
        [
         "d5153b0d-6380-4302-ace4-425e914384cd",
         "D002",
         "C012",
         "F005",
         "577.63",
         "28.88",
         "57.76",
         "Along blue about rock full individual.",
         "2025-04-28T15:21:00"
        ],
        [
         "afab9bf9-01b3-4b64-9a67-a2a2f73cc2a3",
         "D018",
         "C026",
         "F002",
         "297.38",
         "14.87",
         "29.74",
         "Probably edge officer wide he size management.",
         "2025-04-08T11:00:14"
        ],
        [
         "7ac84269-c851-4175-91b2-400e975d6370",
         "D010",
         "C012",
         "F002",
         "245.03",
         "12.25",
         "24.5",
         "Down become yes report.",
         "2025-01-01T15:03:33"
        ],
        [
         "3c869021-d070-465a-9e20-0e761fe9f0c5",
         "D015",
         "C007",
         "F003",
         "232.35",
         "11.62",
         "23.23",
         "Despite free man director.",
         "2025-04-22T01:38:52"
        ],
        [
         "c148df81-fdb9-4fb3-bbc2-f8eb2f17f77a",
         "D017",
         "C004",
         "F005",
         "367.59",
         "18.38",
         "36.76",
         "Something these or.",
         "2025-02-11T02:37:40"
        ],
        [
         "dcd12f6b-954e-46c9-940f-d4527d1f392f",
         "D019",
         "C002",
         "F002",
         "968.72",
         "48.44",
         "96.87",
         "Data apply Mr future popular.",
         "2025-01-31T05:32:54"
        ],
        [
         "71f626c5-6802-4a1d-a316-8db3982699ff",
         "D011",
         "C012",
         "F005",
         "186.81",
         "9.34",
         "18.68",
         "Group piece writer ability stage visit.",
         "2025-03-21T15:56:27"
        ],
        [
         "8dba4c49-de31-4441-89fb-65417d3993bf",
         "D017",
         "C010",
         "F001",
         "809.75",
         "40.49",
         "80.98",
         "General go room away human.",
         "2025-01-17T00:25:43"
        ],
        [
         "e9846661-340e-4d74-a394-fcd904ad36a9",
         "D012",
         "C013",
         "F002",
         "602.2",
         "30.11",
         "60.22",
         "Foot high just structure decade.",
         "2025-02-15T03:46:13"
        ],
        [
         "7714873d-8688-4980-89d9-145479320b04",
         "D010",
         "C029",
         "F002",
         "457.53",
         "22.88",
         "45.75",
         "Go hear detail voice.",
         "2025-04-07T03:16:31"
        ],
        [
         "3d34b9d3-5775-4b3c-bdf2-a4689c252b2f",
         "D007",
         "C001",
         "F003",
         "287.2",
         "14.36",
         "28.72",
         "Arm girl population hit author sound already.",
         "2025-03-03T09:18:26"
        ],
        [
         "527947ed-91d6-4ee5-95cd-d3cc01eb9ccc",
         "D019",
         "C008",
         "F003",
         "181.01",
         "9.05",
         "18.1",
         "Company citizen pass Mr senior high protect.",
         "2025-02-21T13:22:17"
        ],
        [
         "66b53845-1c36-4946-9cbc-45f8a04bb6e0",
         "D013",
         "C007",
         "F001",
         "731.34",
         "36.57",
         "73.13",
         "Hot everything rest while probably act cause.",
         "2025-04-29T01:56:55"
        ],
        [
         "7038601d-afbe-4a8d-b9a3-ec0092a7519b",
         "D015",
         "C029",
         "F001",
         "429.95",
         "21.5",
         "43.0",
         "Spring before town wait.",
         "2025-03-12T16:00:03"
        ],
        [
         "01ce6326-dbbf-408d-a35d-692cfe80bf66",
         "D006",
         "C015",
         "F002",
         "506.37",
         "25.32",
         "50.64",
         "Lawyer during cost however focus realize identify.",
         "2025-02-10T20:07:16"
        ],
        [
         "b57db7ed-e2de-4b96-9375-8cfbea373165",
         "D015",
         "C015",
         "F002",
         "633.5",
         "31.68",
         "63.35",
         "Lose draw growth throw prove say chance yourself.",
         "2025-02-13T01:50:05"
        ],
        [
         "58a33a9c-6978-4a50-8cc3-36fad76b6fcb",
         "D012",
         "C016",
         "F002",
         "328.77",
         "16.44",
         "32.88",
         "Nation when least sort.",
         "2025-02-08T18:19:14"
        ],
        [
         "a85a36c7-cb00-4682-9728-1aa182333bc6",
         "D019",
         "C009",
         "F001",
         "944.77",
         "47.24",
         "94.48",
         "Bar wide really grow person garden firm through.",
         "2025-02-06T12:51:36"
        ],
        [
         "84a43384-7483-4b2e-869b-f5f95c15ecea",
         "D017",
         "C002",
         "F005",
         "509.67",
         "25.48",
         "50.97",
         "Source coach thing meeting lay son.",
         "2025-01-23T06:08:15"
        ],
        [
         "63843662-706d-48d2-85cf-c02396ef42ca",
         "D007",
         "C015",
         "F003",
         "334.29",
         "16.71",
         "33.43",
         "Stand kid debate.",
         "2025-01-03T03:43:12"
        ],
        [
         "b0e5d322-3ab4-4f8f-947e-939ce0256f0b",
         "D017",
         "C029",
         "F001",
         "500.33",
         "25.02",
         "50.03",
         "Role store single benefit technology world.",
         "2025-01-12T16:30:28"
        ],
        [
         "445083be-e0ad-44d9-91fa-dd18b04719be",
         "D015",
         "C013",
         "F001",
         "101.83",
         "5.09",
         "10.18",
         "Design yes human than.",
         "2025-04-05T06:16:48"
        ],
        [
         "9b510699-b96e-454d-9cb3-501870aeb30a",
         "D006",
         "C006",
         "F004",
         "358.03",
         "17.9",
         "35.8",
         "See benefit individual care TV three course.",
         "2025-01-16T05:40:28"
        ],
        [
         "91c35e54-87a9-468d-be0b-3aecd0b6a5b0",
         "D003",
         "C006",
         "F002",
         "796.18",
         "39.81",
         "79.62",
         "Board enjoy direction system blue.",
         "2025-02-01T06:55:16"
        ],
        [
         "6d87f888-b126-4438-82db-2c07a4ce06bf",
         "D018",
         "C006",
         "F004",
         "116.95",
         "5.85",
         "11.7",
         "Thank quality together most wait.",
         "2025-03-26T23:55:08"
        ],
        [
         "0ff473f3-a68a-4947-a82b-e5ca08f1029e",
         "D011",
         "C024",
         "F003",
         "117.83",
         "5.89",
         "11.78",
         "Town ask detail near never along we.",
         "2025-02-08T01:02:22"
        ],
        [
         "012e5e36-9d6b-4155-8e1b-460b73a54410",
         "D016",
         "C002",
         "F001",
         "107.94",
         "5.4",
         "10.79",
         "Rich your road.",
         "2025-01-09T17:33:54"
        ],
        [
         "68e1fe9a-bed4-4803-b2ea-54370a8b6a83",
         "D014",
         "C001",
         "F002",
         "894.68",
         "44.73",
         "89.47",
         "Wait worry human exactly whether.",
         "2025-03-08T00:08:57"
        ],
        [
         "98613f79-3d7c-4805-9495-564969f5e46f",
         "D010",
         "C017",
         "F001",
         "887.8",
         "44.39",
         "88.78",
         "Office PM both consumer size capital information.",
         "2025-03-08T10:30:35"
        ],
        [
         "387da213-a938-44b0-9627-8352cf78bca0",
         "D007",
         "C018",
         "F005",
         "444.44",
         "22.22",
         "44.44",
         "Subject avoid read second here.",
         "2025-04-22T23:01:43"
        ],
        [
         "5ae11472-4817-4c1a-b656-039f5013e331",
         "D003",
         "C008",
         "F002",
         "552.78",
         "27.64",
         "55.28",
         "Camera same and possible wife few.",
         "2025-01-26T02:04:46"
        ],
        [
         "0c9df3e9-2daf-4a6a-89b8-5593fdf1d012",
         "D004",
         "C016",
         "F005",
         "945.13",
         "47.26",
         "94.51",
         "Tax rock there customer price glass.",
         "2025-01-18T22:50:56"
        ],
        [
         "4c1a19e7-e6bf-407e-876f-05585e6e377f",
         "D004",
         "C028",
         "F001",
         "123.28",
         "6.16",
         "12.33",
         "Up on Republican mother eight.",
         "2025-03-31T06:55:59"
        ],
        [
         "425c38ab-11c3-45cb-aa68-a77174868f9d",
         "D020",
         "C025",
         "F003",
         "399.56",
         "19.98",
         "39.96",
         "Part person watch put.",
         "2025-02-17T12:49:19"
        ],
        [
         "6eb0ef06-bbb9-44f6-88d0-d6af11ad1623",
         "D017",
         "C022",
         "F003",
         "597.06",
         "29.85",
         "59.71",
         "Now decision western watch ahead.",
         "2025-02-01T23:26:22"
        ],
        [
         "327c5086-90e2-45cd-934d-3a8d928a51a2",
         "D002",
         "C021",
         "F002",
         "511.6",
         "25.58",
         "51.16",
         "Small see son single hundred.",
         "2025-04-17T08:53:10"
        ],
        [
         "f04385d5-dce9-42e2-b27e-c10a5d7c0ae1",
         "D001",
         "C016",
         "F001",
         "750.86",
         "37.54",
         "75.09",
         "Vote condition bill couple.",
         "2025-04-20T00:36:52"
        ],
        [
         "508cbaa3-1ff3-4468-a65c-f81905811612",
         "D018",
         "C012",
         "F005",
         "356.89",
         "17.84",
         "35.69",
         "Customer wear issue adult good reason.",
         "2025-04-20T21:58:05"
        ],
        [
         "e517d310-4f85-42cb-afa8-f4fdcd9ee261",
         "D019",
         "C005",
         "F004",
         "228.17",
         "11.41",
         "22.82",
         "Evening page American hard voice argue behavior true.",
         "2025-03-22T07:45:30"
        ],
        [
         "079ded85-6b03-47e5-a776-e7c4df791185",
         "D008",
         "C017",
         "F003",
         "101.39",
         "5.07",
         "10.14",
         "Teacher civil big eight almost.",
         "2025-04-02T00:05:05"
        ],
        [
         "5c520987-9701-4c94-acff-87df26e0549a",
         "D012",
         "C017",
         "F002",
         "696.86",
         "34.84",
         "69.69",
         "Think inside huge language lose economic sit.",
         "2025-04-12T05:22:58"
        ],
        [
         "5242b89f-7ac3-4ac0-b8af-175c0cf17331",
         "D001",
         "C006",
         "F004",
         "625.82",
         "31.29",
         "62.58",
         "Level statement ahead country finally.",
         "2025-02-22T03:53:56"
        ],
        [
         "b4e4201a-78e1-4553-a544-ab3d6bcad374",
         "D007",
         "C003",
         "F005",
         "245.35",
         "12.27",
         "24.54",
         "Way same staff around physical plan game commercial.",
         "2025-03-24T05:03:14"
        ],
        [
         "eafa13cf-0999-44c3-b2eb-fef7064f144f",
         "D015",
         "C030",
         "F002",
         "731.97",
         "36.6",
         "73.2",
         "Seem suffer very or allow week month.",
         "2025-03-09T14:36:10"
        ],
        [
         "1dd3cea0-fffe-442a-84d4-3f3722e5daae",
         "D006",
         "C005",
         "F002",
         "909.97",
         "45.5",
         "91.0",
         "Huge stop parent step return effort.",
         "2025-02-15T00:56:11"
        ],
        [
         "c767d446-4eba-4cb5-9eb3-966bd43e5c77",
         "D002",
         "C013",
         "F002",
         "610.72",
         "30.54",
         "61.07",
         "Community magazine than movie green for four news.",
         "2025-01-26T00:17:40"
        ],
        [
         "a3ba3fd7-e9c4-420c-b409-7c2e1901eb31",
         "D017",
         "C016",
         "F003",
         "474.86",
         "23.74",
         "47.49",
         "Charge address happen thus employee foot suddenly.",
         "2025-02-19T17:21:56"
        ],
        [
         "575cb2f9-1cb4-4619-9ecb-ab79b5a99b05",
         "D004",
         "C022",
         "F003",
         "452.73",
         "22.64",
         "45.27",
         "Remain measure in per fish audience bring.",
         "2025-01-19T01:30:25"
        ],
        [
         "b292bde2-25db-4b41-ae37-5e16fed5cf5d",
         "D015",
         "C021",
         "F002",
         "518.0",
         "25.9",
         "51.8",
         "Walk hard young door audience hospital.",
         "2025-04-14T20:47:28"
        ],
        [
         "a5915bdf-d2ee-4fb6-9a2e-37df9ff21955",
         "D012",
         "C006",
         "F003",
         "168.49",
         "8.42",
         "16.85",
         "Condition moment customer role response look class.",
         "2025-04-06T19:23:01"
        ],
        [
         "cc071b63-06cf-4dac-be3b-2234307080d3",
         "D019",
         "C003",
         "F003",
         "309.96",
         "15.5",
         "31.0",
         "Structure skill consumer artist certain even prepare.",
         "2025-02-26T18:38:58"
        ],
        [
         "04b63c45-fb8c-42e5-9fc8-cabee89944d0",
         "D011",
         "C019",
         "F003",
         "845.08",
         "42.25",
         "84.51",
         "Election trip sister best.",
         "2025-04-11T10:31:15"
        ],
        [
         "be2414e4-fc64-446c-a4df-9a0c15f307ba",
         "D001",
         "C011",
         "F004",
         "688.02",
         "34.4",
         "68.8",
         "Nice themselves natural also three behind glass.",
         "2025-04-28T12:37:04"
        ],
        [
         "032fe7b8-fe92-4aa9-a091-8f1d3ba5cadb",
         "D002",
         "C024",
         "F001",
         "179.67",
         "8.98",
         "17.97",
         "Page well actually final common as.",
         "2025-03-27T17:13:41"
        ],
        [
         "4dda6239-afae-4fb0-aabd-9475ff94bdef",
         "D006",
         "C019",
         "F002",
         "822.19",
         "41.11",
         "82.22",
         "Beautiful lose skin.",
         "2025-03-24T08:32:55"
        ],
        [
         "23eae1e5-7373-46f7-b995-0a143a323c5e",
         "D013",
         "C020",
         "F005",
         "528.09",
         "26.4",
         "52.81",
         "Give word serious try dream.",
         "2025-04-13T19:14:32"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "load_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "driver_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "client_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "fuel_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gallons_loaded",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state_tax",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "excise_tax",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "note",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the path once\n",
    "base_path = \"/mnt/fueldata\"\n",
    "\n",
    "# Read each CSV file\n",
    "df_clients = spark.read.option(\"header\", True).csv(f\"{base_path}/clients.csv\")\n",
    "df_drivers = spark.read.option(\"header\", True).csv(f\"{base_path}/drivers.csv\")\n",
    "df_fuels = spark.read.option(\"header\", True).csv(f\"{base_path}/fuels.csv\")\n",
    "df_pump_loads = spark.read.option(\"header\", True).csv(f\"{base_path}/pump_loads.csv\")\n",
    "\n",
    "# Show top rows from each DataFrame\n",
    "display(df_clients)\n",
    "display(df_drivers)\n",
    "display(df_fuels)\n",
    "display(df_pump_loads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa974ab7-7873-44c4-9c39-a4392f5acca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_clients.createOrReplaceTempView(\"clients\")\n",
    "df_drivers.createOrReplaceTempView(\"drivers\")\n",
    "df_fuels.createOrReplaceTempView(\"fuels\")\n",
    "df_pump_loads.createOrReplaceTempView(\"pump_loads\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48d908a3-474c-46ad-8fd4-683f0c49a383",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;36m  File \u001b[0;32m<command-7454861199995883>, line 2\u001b[0;36m\u001b[0m\n",
       "\u001b[0;31m    p.load_id,\u001b[0m\n",
       "\u001b[0m    ^\u001b[0m\n",
       "\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "IndentationError",
        "evalue": "unexpected indent (command-7454861199995883-461014461, line 2)"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>IndentationError</span>: unexpected indent (command-7454861199995883-461014461, line 2)"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001b[0;36m  File \u001b[0;32m<command-7454861199995883>, line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    p.load_id,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "SELECT \n",
    "  p.load_id,\n",
    "  d.driver_id,\n",
    "  d.name AS driver_name,\n",
    "  d.license_n,\n",
    "  d.experience_years,\n",
    "  f.fuel_type,\n",
    "  f.price_per_gallon,\n",
    "  p.gallons,\n",
    "  ROUND(p.gallons * f.price_per_gallon, 2) AS total_fuel_cost,\n",
    "  c.client_id,\n",
    "  c.name AS client_name,\n",
    "  c.country,\n",
    "  c.state,\n",
    "  c.outstanding_amount,\n",
    "  p.state_tax,\n",
    "  p.excise_tax,\n",
    "  p.note,\n",
    "  p.timestamp\n",
    "FROM pump_loads p\n",
    "JOIN drivers d ON p.driver_id = d.driver_id\n",
    "JOIN fuels f ON p.fuel_id = f.fuel_id\n",
    "JOIN clients c ON p.client_id = c.client_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f05140a5-f6f2-4715-b367-07bd96144803",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-7454861199995884>, line 1\u001b[0m\n",
       "\u001b[0;32m----> 1\u001b[0m fuel_report_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n",
       "\u001b[1;32m      2\u001b[0m \u001b[38;5;124mSELECT \u001b[39m\n",
       "\u001b[1;32m      3\u001b[0m \u001b[38;5;124m  p.load_id,\u001b[39m\n",
       "\u001b[1;32m      4\u001b[0m \u001b[38;5;124m  d.driver_id,\u001b[39m\n",
       "\u001b[1;32m      5\u001b[0m \u001b[38;5;124m  d.name AS driver_name,\u001b[39m\n",
       "\u001b[1;32m      6\u001b[0m \u001b[38;5;124m  d.license_n,\u001b[39m\n",
       "\u001b[1;32m      7\u001b[0m \u001b[38;5;124m  d.experience_years,\u001b[39m\n",
       "\u001b[1;32m      8\u001b[0m \u001b[38;5;124m  f.fuel_type,\u001b[39m\n",
       "\u001b[1;32m      9\u001b[0m \u001b[38;5;124m  f.price_per_gallon,\u001b[39m\n",
       "\u001b[1;32m     10\u001b[0m \u001b[38;5;124m  p.gallons,\u001b[39m\n",
       "\u001b[1;32m     11\u001b[0m \u001b[38;5;124m  ROUND(p.gallons * f.price_per_gallon, 2) AS total_fuel_cost,\u001b[39m\n",
       "\u001b[1;32m     12\u001b[0m \u001b[38;5;124m  c.client_id,\u001b[39m\n",
       "\u001b[1;32m     13\u001b[0m \u001b[38;5;124m  c.name AS client_name,\u001b[39m\n",
       "\u001b[1;32m     14\u001b[0m \u001b[38;5;124m  c.country,\u001b[39m\n",
       "\u001b[1;32m     15\u001b[0m \u001b[38;5;124m  c.state,\u001b[39m\n",
       "\u001b[1;32m     16\u001b[0m \u001b[38;5;124m  c.outstanding_amount,\u001b[39m\n",
       "\u001b[1;32m     17\u001b[0m \u001b[38;5;124m  p.state_tax,\u001b[39m\n",
       "\u001b[1;32m     18\u001b[0m \u001b[38;5;124m  p.excise_tax,\u001b[39m\n",
       "\u001b[1;32m     19\u001b[0m \u001b[38;5;124m  p.note,\u001b[39m\n",
       "\u001b[1;32m     20\u001b[0m \u001b[38;5;124m  p.timestamp\u001b[39m\n",
       "\u001b[1;32m     21\u001b[0m \u001b[38;5;124mFROM pump_loads p\u001b[39m\n",
       "\u001b[1;32m     22\u001b[0m \u001b[38;5;124mJOIN drivers d ON p.driver_id = d.driver_id\u001b[39m\n",
       "\u001b[1;32m     23\u001b[0m \u001b[38;5;124mJOIN fuels f ON p.fuel_id = f.fuel_id\u001b[39m\n",
       "\u001b[1;32m     24\u001b[0m \u001b[38;5;124mJOIN clients c ON p.client_id = c.client_id\u001b[39m\n",
       "\u001b[1;32m     25\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n",
       "\u001b[1;32m     27\u001b[0m fuel_report_df\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     45\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n",
       "\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m---> 47\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
       "\u001b[1;32m     48\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n",
       "\u001b[1;32m     49\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n",
       "\u001b[1;32m     50\u001b[0m     )\n",
       "\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/session.py:1830\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m   1825\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[1;32m   1826\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n",
       "\u001b[1;32m   1827\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINVALID_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
       "\u001b[1;32m   1828\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(args)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n",
       "\u001b[1;32m   1829\u001b[0m         )\n",
       "\u001b[0;32m-> 1830\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39msql(sqlQuery, litArgs), \u001b[38;5;28mself\u001b[39m)\n",
       "\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
       "\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1349\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1351\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1352\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n",
       "\u001b[1;32m   1354\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
       "\u001b[0;32m-> 1355\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n",
       "\u001b[1;32m   1356\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n",
       "\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
       "\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:261\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    257\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
       "\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n",
       "\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n",
       "\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n",
       "\u001b[0;32m--> 261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
       "\n",
       "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `d`.`license_n` cannot be resolved. Did you mean one of the following? [`d`.`license_number`, `c`.`name`, `d`.`name`, `p`.`note`, `c`.`client_id`]. SQLSTATE: 42703; line 6 pos 2;\n",
       "'Project [load_id#98, driver_id#50, name#51 AS driver_name#1424, 'd.license_n, experience_years#53, fuel_type#76, price_per_gallon#77, 'p.gallons, 'ROUND(('p.gallons * price_per_gallon#77), 2) AS total_fuel_cost#1425, client_id#23, name#24 AS client_name#1426, country#25, state#26, outstanding_amount#27, state_tax#103, excise_tax#104, note#105, timestamp#106]\n",
       "+- Join Inner, (client_id#100 = client_id#23)\n",
       "   :- Join Inner, (fuel_id#101 = fuel_id#75)\n",
       "   :  :- Join Inner, (driver_id#99 = driver_id#50)\n",
       "   :  :  :- SubqueryAlias p\n",
       "   :  :  :  +- SubqueryAlias pump_loads\n",
       "   :  :  :     +- View (`pump_loads`, [load_id#98, driver_id#99, client_id#100, fuel_id#101, gallons_loaded#102, state_tax#103, excise_tax#104, note#105, timestamp#106])\n",
       "   :  :  :        +- Relation [load_id#98,driver_id#99,client_id#100,fuel_id#101,gallons_loaded#102,state_tax#103,excise_tax#104,note#105,timestamp#106] csv\n",
       "   :  :  +- SubqueryAlias d\n",
       "   :  :     +- SubqueryAlias drivers\n",
       "   :  :        +- View (`drivers`, [driver_id#50, name#51, license_number#52, experience_years#53])\n",
       "   :  :           +- Relation [driver_id#50,name#51,license_number#52,experience_years#53] csv\n",
       "   :  +- SubqueryAlias f\n",
       "   :     +- SubqueryAlias fuels\n",
       "   :        +- View (`fuels`, [fuel_id#75, fuel_type#76, price_per_gallon#77])\n",
       "   :           +- Relation [fuel_id#75,fuel_type#76,price_per_gallon#77] csv\n",
       "   +- SubqueryAlias c\n",
       "      +- SubqueryAlias clients\n",
       "         +- View (`clients`, [client_id#23, name#24, country#25, state#26, outstanding_amount#27])\n",
       "            +- Relation [client_id#23,name#24,country#25,state#26,outstanding_amount#27] csv\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `d`.`license_n` cannot be resolved. Did you mean one of the following? [`d`.`license_number`, `c`.`name`, `d`.`name`, `p`.`note`, `c`.`client_id`]. SQLSTATE: 42703; line 6 pos 2;\n'Project [load_id#98, driver_id#50, name#51 AS driver_name#1424, 'd.license_n, experience_years#53, fuel_type#76, price_per_gallon#77, 'p.gallons, 'ROUND(('p.gallons * price_per_gallon#77), 2) AS total_fuel_cost#1425, client_id#23, name#24 AS client_name#1426, country#25, state#26, outstanding_amount#27, state_tax#103, excise_tax#104, note#105, timestamp#106]\n+- Join Inner, (client_id#100 = client_id#23)\n   :- Join Inner, (fuel_id#101 = fuel_id#75)\n   :  :- Join Inner, (driver_id#99 = driver_id#50)\n   :  :  :- SubqueryAlias p\n   :  :  :  +- SubqueryAlias pump_loads\n   :  :  :     +- View (`pump_loads`, [load_id#98, driver_id#99, client_id#100, fuel_id#101, gallons_loaded#102, state_tax#103, excise_tax#104, note#105, timestamp#106])\n   :  :  :        +- Relation [load_id#98,driver_id#99,client_id#100,fuel_id#101,gallons_loaded#102,state_tax#103,excise_tax#104,note#105,timestamp#106] csv\n   :  :  +- SubqueryAlias d\n   :  :     +- SubqueryAlias drivers\n   :  :        +- View (`drivers`, [driver_id#50, name#51, license_number#52, experience_years#53])\n   :  :           +- Relation [driver_id#50,name#51,license_number#52,experience_years#53] csv\n   :  +- SubqueryAlias f\n   :     +- SubqueryAlias fuels\n   :        +- View (`fuels`, [fuel_id#75, fuel_type#76, price_per_gallon#77])\n   :           +- Relation [fuel_id#75,fuel_type#76,price_per_gallon#77] csv\n   +- SubqueryAlias c\n      +- SubqueryAlias clients\n         +- View (`clients`, [client_id#23, name#24, country#25, state#26, outstanding_amount#27])\n            +- Relation [client_id#23,name#24,country#25,state#26,outstanding_amount#27] csv\n"
       },
       "metadata": {
        "errorSummary": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `d`.`license_n` cannot be resolved. Did you mean one of the following? [`d`.`license_number`, `c`.`name`, `d`.`name`, `p`.`note`, `c`.`client_id`]. SQLSTATE: 42703"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "UNRESOLVED_COLUMN.WITH_SUGGESTION",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "sqlState": "42703",
        "stackTrace": null,
        "startIndex": 64,
        "stopIndex": 74
       },
       "stackFrames": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
        "File \u001b[0;32m<command-7454861199995884>, line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fuel_report_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mSELECT \u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m  p.load_id,\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m  d.driver_id,\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m  d.name AS driver_name,\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m  d.license_n,\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m  d.experience_years,\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m  f.fuel_type,\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m  f.price_per_gallon,\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m  p.gallons,\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m  ROUND(p.gallons * f.price_per_gallon, 2) AS total_fuel_cost,\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m  c.client_id,\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m  c.name AS client_name,\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124m  c.country,\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124m  c.state,\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124m  c.outstanding_amount,\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124m  p.state_tax,\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124m  p.excise_tax,\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124m  p.note,\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124m  p.timestamp\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124mFROM pump_loads p\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124mJOIN drivers d ON p.driver_id = d.driver_id\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124mJOIN fuels f ON p.fuel_id = f.fuel_id\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124mJOIN clients c ON p.client_id = c.client_id\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m fuel_report_df\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     48\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n\u001b[1;32m     49\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n\u001b[1;32m     50\u001b[0m     )\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/sql/session.py:1830\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1825\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1826\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   1827\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINVALID_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1828\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(args)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   1829\u001b[0m         )\n\u001b[0;32m-> 1830\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39msql(sqlQuery, litArgs), \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
        "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1351\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1352\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1354\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1355\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1356\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:261\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    257\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
        "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `d`.`license_n` cannot be resolved. Did you mean one of the following? [`d`.`license_number`, `c`.`name`, `d`.`name`, `p`.`note`, `c`.`client_id`]. SQLSTATE: 42703; line 6 pos 2;\n'Project [load_id#98, driver_id#50, name#51 AS driver_name#1424, 'd.license_n, experience_years#53, fuel_type#76, price_per_gallon#77, 'p.gallons, 'ROUND(('p.gallons * price_per_gallon#77), 2) AS total_fuel_cost#1425, client_id#23, name#24 AS client_name#1426, country#25, state#26, outstanding_amount#27, state_tax#103, excise_tax#104, note#105, timestamp#106]\n+- Join Inner, (client_id#100 = client_id#23)\n   :- Join Inner, (fuel_id#101 = fuel_id#75)\n   :  :- Join Inner, (driver_id#99 = driver_id#50)\n   :  :  :- SubqueryAlias p\n   :  :  :  +- SubqueryAlias pump_loads\n   :  :  :     +- View (`pump_loads`, [load_id#98, driver_id#99, client_id#100, fuel_id#101, gallons_loaded#102, state_tax#103, excise_tax#104, note#105, timestamp#106])\n   :  :  :        +- Relation [load_id#98,driver_id#99,client_id#100,fuel_id#101,gallons_loaded#102,state_tax#103,excise_tax#104,note#105,timestamp#106] csv\n   :  :  +- SubqueryAlias d\n   :  :     +- SubqueryAlias drivers\n   :  :        +- View (`drivers`, [driver_id#50, name#51, license_number#52, experience_years#53])\n   :  :           +- Relation [driver_id#50,name#51,license_number#52,experience_years#53] csv\n   :  +- SubqueryAlias f\n   :     +- SubqueryAlias fuels\n   :        +- View (`fuels`, [fuel_id#75, fuel_type#76, price_per_gallon#77])\n   :           +- Relation [fuel_id#75,fuel_type#76,price_per_gallon#77] csv\n   +- SubqueryAlias c\n      +- SubqueryAlias clients\n         +- View (`clients`, [client_id#23, name#24, country#25, state#26, outstanding_amount#27])\n            +- Relation [client_id#23,name#24,country#25,state#26,outstanding_amount#27] csv\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fuel_report_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "  p.load_id,\n",
    "  d.driver_id,\n",
    "  d.name AS driver_name,\n",
    "  d.license_n,\n",
    "  d.experience_years,\n",
    "  f.fuel_type,\n",
    "  f.price_per_gallon,\n",
    "  p.gallons,\n",
    "  ROUND(p.gallons * f.price_per_gallon, 2) AS total_fuel_cost,\n",
    "  c.client_id,\n",
    "  c.name AS client_name,\n",
    "  c.country,\n",
    "  c.state,\n",
    "  c.outstanding_amount,\n",
    "  p.state_tax,\n",
    "  p.excise_tax,\n",
    "  p.note,\n",
    "  p.timestamp\n",
    "FROM pump_loads p\n",
    "JOIN drivers d ON p.driver_id = d.driver_id\n",
    "JOIN fuels f ON p.fuel_id = f.fuel_id\n",
    "JOIN clients c ON p.client_id = c.client_id\n",
    "\"\"\")\n",
    "\n",
    "fuel_report_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4ca6412-9e6f-48da-978d-64cf053fd428",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-7454861199995885>, line 1\u001b[0m\n",
       "\u001b[0;32m----> 1\u001b[0m fuel_report_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n",
       "\u001b[1;32m      2\u001b[0m \u001b[38;5;124mSELECT \u001b[39m\n",
       "\u001b[1;32m      3\u001b[0m \u001b[38;5;124m  p.load_id,\u001b[39m\n",
       "\u001b[1;32m      4\u001b[0m \u001b[38;5;124m  d.driver_id,\u001b[39m\n",
       "\u001b[1;32m      5\u001b[0m \u001b[38;5;124m  d.name AS driver_name,\u001b[39m\n",
       "\u001b[1;32m      6\u001b[0m \u001b[38;5;124m  d.license_number,\u001b[39m\n",
       "\u001b[1;32m      7\u001b[0m \u001b[38;5;124m  d.experience_years,\u001b[39m\n",
       "\u001b[1;32m      8\u001b[0m \u001b[38;5;124m  f.fuel_type,\u001b[39m\n",
       "\u001b[1;32m      9\u001b[0m \u001b[38;5;124m  f.price_per_gallon,\u001b[39m\n",
       "\u001b[1;32m     10\u001b[0m \u001b[38;5;124m  p.gallons,\u001b[39m\n",
       "\u001b[1;32m     11\u001b[0m \u001b[38;5;124m  ROUND(p.gallons * f.price_per_gallon, 2) AS total_fuel_cost,\u001b[39m\n",
       "\u001b[1;32m     12\u001b[0m \u001b[38;5;124m  c.client_id,\u001b[39m\n",
       "\u001b[1;32m     13\u001b[0m \u001b[38;5;124m  c.name AS client_name,\u001b[39m\n",
       "\u001b[1;32m     14\u001b[0m \u001b[38;5;124m  c.country,\u001b[39m\n",
       "\u001b[1;32m     15\u001b[0m \u001b[38;5;124m  c.state,\u001b[39m\n",
       "\u001b[1;32m     16\u001b[0m \u001b[38;5;124m  c.outstanding_amount,\u001b[39m\n",
       "\u001b[1;32m     17\u001b[0m \u001b[38;5;124m  p.state_tax,\u001b[39m\n",
       "\u001b[1;32m     18\u001b[0m \u001b[38;5;124m  p.excise_tax,\u001b[39m\n",
       "\u001b[1;32m     19\u001b[0m \u001b[38;5;124m  p.note,\u001b[39m\n",
       "\u001b[1;32m     20\u001b[0m \u001b[38;5;124m  p.timestamp\u001b[39m\n",
       "\u001b[1;32m     21\u001b[0m \u001b[38;5;124mFROM pump_loads p\u001b[39m\n",
       "\u001b[1;32m     22\u001b[0m \u001b[38;5;124mJOIN drivers d ON p.driver_id = d.driver_id\u001b[39m\n",
       "\u001b[1;32m     23\u001b[0m \u001b[38;5;124mJOIN fuels f ON p.fuel_id = f.fuel_id\u001b[39m\n",
       "\u001b[1;32m     24\u001b[0m \u001b[38;5;124mJOIN clients c ON p.client_id = c.client_id\u001b[39m\n",
       "\u001b[1;32m     25\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n",
       "\u001b[1;32m     27\u001b[0m fuel_report_df\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     45\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n",
       "\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m---> 47\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
       "\u001b[1;32m     48\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n",
       "\u001b[1;32m     49\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n",
       "\u001b[1;32m     50\u001b[0m     )\n",
       "\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/session.py:1830\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m   1825\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[1;32m   1826\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n",
       "\u001b[1;32m   1827\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINVALID_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
       "\u001b[1;32m   1828\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(args)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n",
       "\u001b[1;32m   1829\u001b[0m         )\n",
       "\u001b[0;32m-> 1830\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39msql(sqlQuery, litArgs), \u001b[38;5;28mself\u001b[39m)\n",
       "\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
       "\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1349\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1351\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1352\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n",
       "\u001b[1;32m   1354\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
       "\u001b[0;32m-> 1355\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n",
       "\u001b[1;32m   1356\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n",
       "\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
       "\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:261\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    257\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
       "\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n",
       "\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n",
       "\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n",
       "\u001b[0;32m--> 261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
       "\n",
       "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `p`.`gallons` cannot be resolved. Did you mean one of the following? [`p`.`note`, `p`.`fuel_id`, `p`.`load_id`, `c`.`name`, `d`.`name`]. SQLSTATE: 42703; line 10 pos 2;\n",
       "'Project [load_id#98, driver_id#50, name#51 AS driver_name#2750, license_number#52, experience_years#53, fuel_type#76, price_per_gallon#77, 'p.gallons, 'ROUND(('p.gallons * price_per_gallon#77), 2) AS total_fuel_cost#2751, client_id#23, name#24 AS client_name#2752, country#25, state#26, outstanding_amount#27, state_tax#103, excise_tax#104, note#105, timestamp#106]\n",
       "+- Join Inner, (client_id#100 = client_id#23)\n",
       "   :- Join Inner, (fuel_id#101 = fuel_id#75)\n",
       "   :  :- Join Inner, (driver_id#99 = driver_id#50)\n",
       "   :  :  :- SubqueryAlias p\n",
       "   :  :  :  +- SubqueryAlias pump_loads\n",
       "   :  :  :     +- View (`pump_loads`, [load_id#98, driver_id#99, client_id#100, fuel_id#101, gallons_loaded#102, state_tax#103, excise_tax#104, note#105, timestamp#106])\n",
       "   :  :  :        +- Relation [load_id#98,driver_id#99,client_id#100,fuel_id#101,gallons_loaded#102,state_tax#103,excise_tax#104,note#105,timestamp#106] csv\n",
       "   :  :  +- SubqueryAlias d\n",
       "   :  :     +- SubqueryAlias drivers\n",
       "   :  :        +- View (`drivers`, [driver_id#50, name#51, license_number#52, experience_years#53])\n",
       "   :  :           +- Relation [driver_id#50,name#51,license_number#52,experience_years#53] csv\n",
       "   :  +- SubqueryAlias f\n",
       "   :     +- SubqueryAlias fuels\n",
       "   :        +- View (`fuels`, [fuel_id#75, fuel_type#76, price_per_gallon#77])\n",
       "   :           +- Relation [fuel_id#75,fuel_type#76,price_per_gallon#77] csv\n",
       "   +- SubqueryAlias c\n",
       "      +- SubqueryAlias clients\n",
       "         +- View (`clients`, [client_id#23, name#24, country#25, state#26, outstanding_amount#27])\n",
       "            +- Relation [client_id#23,name#24,country#25,state#26,outstanding_amount#27] csv\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `p`.`gallons` cannot be resolved. Did you mean one of the following? [`p`.`note`, `p`.`fuel_id`, `p`.`load_id`, `c`.`name`, `d`.`name`]. SQLSTATE: 42703; line 10 pos 2;\n'Project [load_id#98, driver_id#50, name#51 AS driver_name#2750, license_number#52, experience_years#53, fuel_type#76, price_per_gallon#77, 'p.gallons, 'ROUND(('p.gallons * price_per_gallon#77), 2) AS total_fuel_cost#2751, client_id#23, name#24 AS client_name#2752, country#25, state#26, outstanding_amount#27, state_tax#103, excise_tax#104, note#105, timestamp#106]\n+- Join Inner, (client_id#100 = client_id#23)\n   :- Join Inner, (fuel_id#101 = fuel_id#75)\n   :  :- Join Inner, (driver_id#99 = driver_id#50)\n   :  :  :- SubqueryAlias p\n   :  :  :  +- SubqueryAlias pump_loads\n   :  :  :     +- View (`pump_loads`, [load_id#98, driver_id#99, client_id#100, fuel_id#101, gallons_loaded#102, state_tax#103, excise_tax#104, note#105, timestamp#106])\n   :  :  :        +- Relation [load_id#98,driver_id#99,client_id#100,fuel_id#101,gallons_loaded#102,state_tax#103,excise_tax#104,note#105,timestamp#106] csv\n   :  :  +- SubqueryAlias d\n   :  :     +- SubqueryAlias drivers\n   :  :        +- View (`drivers`, [driver_id#50, name#51, license_number#52, experience_years#53])\n   :  :           +- Relation [driver_id#50,name#51,license_number#52,experience_years#53] csv\n   :  +- SubqueryAlias f\n   :     +- SubqueryAlias fuels\n   :        +- View (`fuels`, [fuel_id#75, fuel_type#76, price_per_gallon#77])\n   :           +- Relation [fuel_id#75,fuel_type#76,price_per_gallon#77] csv\n   +- SubqueryAlias c\n      +- SubqueryAlias clients\n         +- View (`clients`, [client_id#23, name#24, country#25, state#26, outstanding_amount#27])\n            +- Relation [client_id#23,name#24,country#25,state#26,outstanding_amount#27] csv\n"
       },
       "metadata": {
        "errorSummary": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `p`.`gallons` cannot be resolved. Did you mean one of the following? [`p`.`note`, `p`.`fuel_id`, `p`.`load_id`, `c`.`name`, `d`.`name`]. SQLSTATE: 42703"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "UNRESOLVED_COLUMN.WITH_SUGGESTION",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "sqlState": "42703",
        "stackTrace": null,
        "startIndex": 143,
        "stopIndex": 151
       },
       "stackFrames": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
        "File \u001b[0;32m<command-7454861199995885>, line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fuel_report_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mSELECT \u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m  p.load_id,\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m  d.driver_id,\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m  d.name AS driver_name,\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m  d.license_number,\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m  d.experience_years,\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m  f.fuel_type,\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m  f.price_per_gallon,\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m  p.gallons,\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m  ROUND(p.gallons * f.price_per_gallon, 2) AS total_fuel_cost,\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m  c.client_id,\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m  c.name AS client_name,\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124m  c.country,\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124m  c.state,\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124m  c.outstanding_amount,\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124m  p.state_tax,\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124m  p.excise_tax,\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124m  p.note,\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124m  p.timestamp\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124mFROM pump_loads p\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124mJOIN drivers d ON p.driver_id = d.driver_id\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124mJOIN fuels f ON p.fuel_id = f.fuel_id\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124mJOIN clients c ON p.client_id = c.client_id\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m fuel_report_df\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     48\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n\u001b[1;32m     49\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n\u001b[1;32m     50\u001b[0m     )\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/sql/session.py:1830\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1825\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1826\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   1827\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINVALID_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1828\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(args)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   1829\u001b[0m         )\n\u001b[0;32m-> 1830\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39msql(sqlQuery, litArgs), \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
        "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1351\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1352\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1354\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1355\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1356\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:261\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    257\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
        "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `p`.`gallons` cannot be resolved. Did you mean one of the following? [`p`.`note`, `p`.`fuel_id`, `p`.`load_id`, `c`.`name`, `d`.`name`]. SQLSTATE: 42703; line 10 pos 2;\n'Project [load_id#98, driver_id#50, name#51 AS driver_name#2750, license_number#52, experience_years#53, fuel_type#76, price_per_gallon#77, 'p.gallons, 'ROUND(('p.gallons * price_per_gallon#77), 2) AS total_fuel_cost#2751, client_id#23, name#24 AS client_name#2752, country#25, state#26, outstanding_amount#27, state_tax#103, excise_tax#104, note#105, timestamp#106]\n+- Join Inner, (client_id#100 = client_id#23)\n   :- Join Inner, (fuel_id#101 = fuel_id#75)\n   :  :- Join Inner, (driver_id#99 = driver_id#50)\n   :  :  :- SubqueryAlias p\n   :  :  :  +- SubqueryAlias pump_loads\n   :  :  :     +- View (`pump_loads`, [load_id#98, driver_id#99, client_id#100, fuel_id#101, gallons_loaded#102, state_tax#103, excise_tax#104, note#105, timestamp#106])\n   :  :  :        +- Relation [load_id#98,driver_id#99,client_id#100,fuel_id#101,gallons_loaded#102,state_tax#103,excise_tax#104,note#105,timestamp#106] csv\n   :  :  +- SubqueryAlias d\n   :  :     +- SubqueryAlias drivers\n   :  :        +- View (`drivers`, [driver_id#50, name#51, license_number#52, experience_years#53])\n   :  :           +- Relation [driver_id#50,name#51,license_number#52,experience_years#53] csv\n   :  +- SubqueryAlias f\n   :     +- SubqueryAlias fuels\n   :        +- View (`fuels`, [fuel_id#75, fuel_type#76, price_per_gallon#77])\n   :           +- Relation [fuel_id#75,fuel_type#76,price_per_gallon#77] csv\n   +- SubqueryAlias c\n      +- SubqueryAlias clients\n         +- View (`clients`, [client_id#23, name#24, country#25, state#26, outstanding_amount#27])\n            +- Relation [client_id#23,name#24,country#25,state#26,outstanding_amount#27] csv\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fuel_report_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "  p.load_id,\n",
    "  d.driver_id,\n",
    "  d.name AS driver_name,\n",
    "  d.license_number,\n",
    "  d.experience_years,\n",
    "  f.fuel_type,\n",
    "  f.price_per_gallon,\n",
    "  p.gallons,\n",
    "  ROUND(p.gallons * f.price_per_gallon, 2) AS total_fuel_cost,\n",
    "  c.client_id,\n",
    "  c.name AS client_name,\n",
    "  c.country,\n",
    "  c.state,\n",
    "  c.outstanding_amount,\n",
    "  p.state_tax,\n",
    "  p.excise_tax,\n",
    "  p.note,\n",
    "  p.timestamp\n",
    "FROM pump_loads p\n",
    "JOIN drivers d ON p.driver_id = d.driver_id\n",
    "JOIN fuels f ON p.fuel_id = f.fuel_id\n",
    "JOIN clients c ON p.client_id = c.client_id\n",
    "\"\"\")\n",
    "\n",
    "fuel_report_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcbf70e1-8ded-44c2-aaa4-8f8989eadefd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-7454861199995886>, line 1\u001b[0m\n",
       "\u001b[0;32m----> 1\u001b[0m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/data/pump_loads.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mprintSchema()\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     45\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n",
       "\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m---> 47\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
       "\u001b[1;32m     48\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n",
       "\u001b[1;32m     49\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n",
       "\u001b[1;32m     50\u001b[0m     )\n",
       "\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:835\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n",
       "\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
       "\u001b[1;32m    834\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[0;32m--> 835\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n",
       "\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_remote_only():\n",
       "\u001b[1;32m    838\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrdd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RDD  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1349\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1351\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1352\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n",
       "\u001b[1;32m   1354\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
       "\u001b[0;32m-> 1355\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n",
       "\u001b[1;32m   1356\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n",
       "\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
       "\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:261\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    257\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
       "\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n",
       "\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n",
       "\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n",
       "\u001b[0;32m--> 261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
       "\n",
       "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: dbfs:/mnt/data/pump_loads.csv. SQLSTATE: 42K03"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[PATH_NOT_FOUND] Path does not exist: dbfs:/mnt/data/pump_loads.csv. SQLSTATE: 42K03"
       },
       "metadata": {
        "errorSummary": "[PATH_NOT_FOUND] Path does not exist: dbfs:/mnt/data/pump_loads.csv. SQLSTATE: 42K03"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "PATH_NOT_FOUND",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "sqlState": "42K03",
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
        "File \u001b[0;32m<command-7454861199995886>, line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/data/pump_loads.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mprintSchema()\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     48\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n\u001b[1;32m     49\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n\u001b[1;32m     50\u001b[0m     )\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:835\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    834\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 835\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_remote_only():\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrdd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RDD  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
        "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1351\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1352\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1354\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1355\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1356\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:261\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    257\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
        "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: dbfs:/mnt/data/pump_loads.csv. SQLSTATE: 42K03"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.read.csv(\"/mnt/data/pump_loads.csv\", header=True, inferSchema=True).printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fc0c84c-4435-4a07-9407-dfcbce9a9fd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-7454861199995887>, line 1\u001b[0m\n",
       "\u001b[0;32m----> 1\u001b[0m fuel_report_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n",
       "\u001b[1;32m      2\u001b[0m \u001b[38;5;124mSELECT \u001b[39m\n",
       "\u001b[1;32m      3\u001b[0m \u001b[38;5;124m  p.load_id,\u001b[39m\n",
       "\u001b[1;32m      4\u001b[0m \u001b[38;5;124m  d.driver_id,\u001b[39m\n",
       "\u001b[1;32m      5\u001b[0m \u001b[38;5;124m  d.name AS driver_name,\u001b[39m\n",
       "\u001b[1;32m      6\u001b[0m \u001b[38;5;124m  d.license_number,\u001b[39m\n",
       "\u001b[1;32m      7\u001b[0m \u001b[38;5;124m  d.experience_years,\u001b[39m\n",
       "\u001b[1;32m      8\u001b[0m \u001b[38;5;124m  f.fuel_type,\u001b[39m\n",
       "\u001b[1;32m      9\u001b[0m \u001b[38;5;124m  f.price_per_gallon,\u001b[39m\n",
       "\u001b[1;32m     10\u001b[0m \u001b[38;5;124m  p.gallons_l,\u001b[39m\n",
       "\u001b[1;32m     11\u001b[0m \u001b[38;5;124m  ROUND(p.gallons_l * f.price_per_gallon, 2) AS total_fuel_cost,\u001b[39m\n",
       "\u001b[1;32m     12\u001b[0m \u001b[38;5;124m  c.client_id,\u001b[39m\n",
       "\u001b[1;32m     13\u001b[0m \u001b[38;5;124m  c.name AS client_name,\u001b[39m\n",
       "\u001b[1;32m     14\u001b[0m \u001b[38;5;124m  c.country,\u001b[39m\n",
       "\u001b[1;32m     15\u001b[0m \u001b[38;5;124m  c.state,\u001b[39m\n",
       "\u001b[1;32m     16\u001b[0m \u001b[38;5;124m  c.outstanding_amount,\u001b[39m\n",
       "\u001b[1;32m     17\u001b[0m \u001b[38;5;124m  p.state_tax,\u001b[39m\n",
       "\u001b[1;32m     18\u001b[0m \u001b[38;5;124m  p.excise_tax,\u001b[39m\n",
       "\u001b[1;32m     19\u001b[0m \u001b[38;5;124m  p.note,\u001b[39m\n",
       "\u001b[1;32m     20\u001b[0m \u001b[38;5;124m  p.timestamp\u001b[39m\n",
       "\u001b[1;32m     21\u001b[0m \u001b[38;5;124mFROM pump_loads p\u001b[39m\n",
       "\u001b[1;32m     22\u001b[0m \u001b[38;5;124mJOIN drivers d ON p.driver_id = d.driver_id\u001b[39m\n",
       "\u001b[1;32m     23\u001b[0m \u001b[38;5;124mJOIN fuels f ON p.fuel_id = f.fuel_id\u001b[39m\n",
       "\u001b[1;32m     24\u001b[0m \u001b[38;5;124mJOIN clients c ON p.client_id = c.client_id\u001b[39m\n",
       "\u001b[1;32m     25\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n",
       "\u001b[1;32m     27\u001b[0m fuel_report_df\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     45\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n",
       "\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m---> 47\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
       "\u001b[1;32m     48\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n",
       "\u001b[1;32m     49\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n",
       "\u001b[1;32m     50\u001b[0m     )\n",
       "\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/session.py:1830\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m   1825\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[1;32m   1826\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n",
       "\u001b[1;32m   1827\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINVALID_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
       "\u001b[1;32m   1828\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(args)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n",
       "\u001b[1;32m   1829\u001b[0m         )\n",
       "\u001b[0;32m-> 1830\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39msql(sqlQuery, litArgs), \u001b[38;5;28mself\u001b[39m)\n",
       "\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
       "\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1349\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1351\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1352\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n",
       "\u001b[1;32m   1354\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
       "\u001b[0;32m-> 1355\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n",
       "\u001b[1;32m   1356\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n",
       "\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
       "\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:261\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    257\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
       "\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n",
       "\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n",
       "\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n",
       "\u001b[0;32m--> 261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
       "\n",
       "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `p`.`gallons_l` cannot be resolved. Did you mean one of the following? [`p`.`gallons_loaded`, `p`.`client_id`, `p`.`load_id`, `p`.`note`, `c`.`client_id`]. SQLSTATE: 42703; line 10 pos 2;\n",
       "'Project [load_id#98, driver_id#50, name#51 AS driver_name#2760, license_number#52, experience_years#53, fuel_type#76, price_per_gallon#77, 'p.gallons_l, 'ROUND(('p.gallons_l * price_per_gallon#77), 2) AS total_fuel_cost#2761, client_id#23, name#24 AS client_name#2762, country#25, state#26, outstanding_amount#27, state_tax#103, excise_tax#104, note#105, timestamp#106]\n",
       "+- Join Inner, (client_id#100 = client_id#23)\n",
       "   :- Join Inner, (fuel_id#101 = fuel_id#75)\n",
       "   :  :- Join Inner, (driver_id#99 = driver_id#50)\n",
       "   :  :  :- SubqueryAlias p\n",
       "   :  :  :  +- SubqueryAlias pump_loads\n",
       "   :  :  :     +- View (`pump_loads`, [load_id#98, driver_id#99, client_id#100, fuel_id#101, gallons_loaded#102, state_tax#103, excise_tax#104, note#105, timestamp#106])\n",
       "   :  :  :        +- Relation [load_id#98,driver_id#99,client_id#100,fuel_id#101,gallons_loaded#102,state_tax#103,excise_tax#104,note#105,timestamp#106] csv\n",
       "   :  :  +- SubqueryAlias d\n",
       "   :  :     +- SubqueryAlias drivers\n",
       "   :  :        +- View (`drivers`, [driver_id#50, name#51, license_number#52, experience_years#53])\n",
       "   :  :           +- Relation [driver_id#50,name#51,license_number#52,experience_years#53] csv\n",
       "   :  +- SubqueryAlias f\n",
       "   :     +- SubqueryAlias fuels\n",
       "   :        +- View (`fuels`, [fuel_id#75, fuel_type#76, price_per_gallon#77])\n",
       "   :           +- Relation [fuel_id#75,fuel_type#76,price_per_gallon#77] csv\n",
       "   +- SubqueryAlias c\n",
       "      +- SubqueryAlias clients\n",
       "         +- View (`clients`, [client_id#23, name#24, country#25, state#26, outstanding_amount#27])\n",
       "            +- Relation [client_id#23,name#24,country#25,state#26,outstanding_amount#27] csv\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `p`.`gallons_l` cannot be resolved. Did you mean one of the following? [`p`.`gallons_loaded`, `p`.`client_id`, `p`.`load_id`, `p`.`note`, `c`.`client_id`]. SQLSTATE: 42703; line 10 pos 2;\n'Project [load_id#98, driver_id#50, name#51 AS driver_name#2760, license_number#52, experience_years#53, fuel_type#76, price_per_gallon#77, 'p.gallons_l, 'ROUND(('p.gallons_l * price_per_gallon#77), 2) AS total_fuel_cost#2761, client_id#23, name#24 AS client_name#2762, country#25, state#26, outstanding_amount#27, state_tax#103, excise_tax#104, note#105, timestamp#106]\n+- Join Inner, (client_id#100 = client_id#23)\n   :- Join Inner, (fuel_id#101 = fuel_id#75)\n   :  :- Join Inner, (driver_id#99 = driver_id#50)\n   :  :  :- SubqueryAlias p\n   :  :  :  +- SubqueryAlias pump_loads\n   :  :  :     +- View (`pump_loads`, [load_id#98, driver_id#99, client_id#100, fuel_id#101, gallons_loaded#102, state_tax#103, excise_tax#104, note#105, timestamp#106])\n   :  :  :        +- Relation [load_id#98,driver_id#99,client_id#100,fuel_id#101,gallons_loaded#102,state_tax#103,excise_tax#104,note#105,timestamp#106] csv\n   :  :  +- SubqueryAlias d\n   :  :     +- SubqueryAlias drivers\n   :  :        +- View (`drivers`, [driver_id#50, name#51, license_number#52, experience_years#53])\n   :  :           +- Relation [driver_id#50,name#51,license_number#52,experience_years#53] csv\n   :  +- SubqueryAlias f\n   :     +- SubqueryAlias fuels\n   :        +- View (`fuels`, [fuel_id#75, fuel_type#76, price_per_gallon#77])\n   :           +- Relation [fuel_id#75,fuel_type#76,price_per_gallon#77] csv\n   +- SubqueryAlias c\n      +- SubqueryAlias clients\n         +- View (`clients`, [client_id#23, name#24, country#25, state#26, outstanding_amount#27])\n            +- Relation [client_id#23,name#24,country#25,state#26,outstanding_amount#27] csv\n"
       },
       "metadata": {
        "errorSummary": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `p`.`gallons_l` cannot be resolved. Did you mean one of the following? [`p`.`gallons_loaded`, `p`.`client_id`, `p`.`load_id`, `p`.`note`, `c`.`client_id`]. SQLSTATE: 42703"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "UNRESOLVED_COLUMN.WITH_SUGGESTION",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "sqlState": "42703",
        "stackTrace": null,
        "startIndex": 143,
        "stopIndex": 153
       },
       "stackFrames": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
        "File \u001b[0;32m<command-7454861199995887>, line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fuel_report_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mSELECT \u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m  p.load_id,\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m  d.driver_id,\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m  d.name AS driver_name,\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m  d.license_number,\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m  d.experience_years,\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m  f.fuel_type,\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m  f.price_per_gallon,\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m  p.gallons_l,\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m  ROUND(p.gallons_l * f.price_per_gallon, 2) AS total_fuel_cost,\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m  c.client_id,\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m  c.name AS client_name,\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124m  c.country,\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124m  c.state,\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124m  c.outstanding_amount,\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124m  p.state_tax,\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124m  p.excise_tax,\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124m  p.note,\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124m  p.timestamp\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124mFROM pump_loads p\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124mJOIN drivers d ON p.driver_id = d.driver_id\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124mJOIN fuels f ON p.fuel_id = f.fuel_id\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124mJOIN clients c ON p.client_id = c.client_id\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m fuel_report_df\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     48\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n\u001b[1;32m     49\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n\u001b[1;32m     50\u001b[0m     )\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/sql/session.py:1830\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1825\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1826\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   1827\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINVALID_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1828\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(args)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   1829\u001b[0m         )\n\u001b[0;32m-> 1830\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39msql(sqlQuery, litArgs), \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
        "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1351\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1352\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1354\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1355\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1356\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:261\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    257\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
        "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `p`.`gallons_l` cannot be resolved. Did you mean one of the following? [`p`.`gallons_loaded`, `p`.`client_id`, `p`.`load_id`, `p`.`note`, `c`.`client_id`]. SQLSTATE: 42703; line 10 pos 2;\n'Project [load_id#98, driver_id#50, name#51 AS driver_name#2760, license_number#52, experience_years#53, fuel_type#76, price_per_gallon#77, 'p.gallons_l, 'ROUND(('p.gallons_l * price_per_gallon#77), 2) AS total_fuel_cost#2761, client_id#23, name#24 AS client_name#2762, country#25, state#26, outstanding_amount#27, state_tax#103, excise_tax#104, note#105, timestamp#106]\n+- Join Inner, (client_id#100 = client_id#23)\n   :- Join Inner, (fuel_id#101 = fuel_id#75)\n   :  :- Join Inner, (driver_id#99 = driver_id#50)\n   :  :  :- SubqueryAlias p\n   :  :  :  +- SubqueryAlias pump_loads\n   :  :  :     +- View (`pump_loads`, [load_id#98, driver_id#99, client_id#100, fuel_id#101, gallons_loaded#102, state_tax#103, excise_tax#104, note#105, timestamp#106])\n   :  :  :        +- Relation [load_id#98,driver_id#99,client_id#100,fuel_id#101,gallons_loaded#102,state_tax#103,excise_tax#104,note#105,timestamp#106] csv\n   :  :  +- SubqueryAlias d\n   :  :     +- SubqueryAlias drivers\n   :  :        +- View (`drivers`, [driver_id#50, name#51, license_number#52, experience_years#53])\n   :  :           +- Relation [driver_id#50,name#51,license_number#52,experience_years#53] csv\n   :  +- SubqueryAlias f\n   :     +- SubqueryAlias fuels\n   :        +- View (`fuels`, [fuel_id#75, fuel_type#76, price_per_gallon#77])\n   :           +- Relation [fuel_id#75,fuel_type#76,price_per_gallon#77] csv\n   +- SubqueryAlias c\n      +- SubqueryAlias clients\n         +- View (`clients`, [client_id#23, name#24, country#25, state#26, outstanding_amount#27])\n            +- Relation [client_id#23,name#24,country#25,state#26,outstanding_amount#27] csv\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fuel_report_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "  p.load_id,\n",
    "  d.driver_id,\n",
    "  d.name AS driver_name,\n",
    "  d.license_number,\n",
    "  d.experience_years,\n",
    "  f.fuel_type,\n",
    "  f.price_per_gallon,\n",
    "  p.gallons_l,\n",
    "  ROUND(p.gallons_l * f.price_per_gallon, 2) AS total_fuel_cost,\n",
    "  c.client_id,\n",
    "  c.name AS client_name,\n",
    "  c.country,\n",
    "  c.state,\n",
    "  c.outstanding_amount,\n",
    "  p.state_tax,\n",
    "  p.excise_tax,\n",
    "  p.note,\n",
    "  p.timestamp\n",
    "FROM pump_loads p\n",
    "JOIN drivers d ON p.driver_id = d.driver_id\n",
    "JOIN fuels f ON p.fuel_id = f.fuel_id\n",
    "JOIN clients c ON p.client_id = c.client_id\n",
    "\"\"\")\n",
    "\n",
    "fuel_report_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87ab42ff-f6bd-46f0-bc83-e432574e3f36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fuel_report_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "  p.load_id,\n",
    "  d.driver_id,\n",
    "  d.name AS driver_name,\n",
    "  d.license_number,\n",
    "  d.experience_years,\n",
    "  f.fuel_type,\n",
    "  f.price_per_gallon,\n",
    "  p.gallons_loaded,\n",
    "  ROUND(p.gallons_loaded * f.price_per_gallon, 2) AS total_fuel_cost,\n",
    "  c.client_id,\n",
    "  c.name AS client_name,\n",
    "  c.country,\n",
    "  c.state,\n",
    "  c.outstanding_amount,\n",
    "  p.state_tax,\n",
    "  p.excise_tax,\n",
    "  p.note,\n",
    "  p.timestamp\n",
    "FROM pump_loads p\n",
    "JOIN drivers d ON p.driver_id = d.driver_id\n",
    "JOIN fuels f ON p.fuel_id = f.fuel_id\n",
    "JOIN clients c ON p.client_id = c.client_id\n",
    "\"\"\")\n",
    "\n",
    "fuel_report_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "678440c6-3639-49d8-8822-19081796c31b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define path to final output folder in Data Lake\n",
    "final_path = f\"/mnt/data/final/fuel_report.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dca77e5-7a66-4ccc-9532-7d9259901131",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Save DataFrame as Parquet\n",
    "fuel_report_df.write.mode(\"overwrite\").parquet(final_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee8e2680-947f-4ab4-826d-fe0812799768",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"âœ… fuel_report_df successfully written to Data Lake as Parquet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b005e3e-53a6-4499-abf8-425795342940",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.cp(\n",
    "  \"dbfs:/mnt/data/final/fuel_report.parquet\",\n",
    "  \"dbfs:/mnt/fueldata/final/fuel_report.parquet\",\n",
    "  recurse=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ea20d13-67b5-489c-9905-068cbcac582a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-6183947907640502>, line 1\u001b[0m\n",
       "\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/curated/top_drivers.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
       "\n",
       "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\nFile \u001b[0;32m<command-6183947907640502>, line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/curated/top_drivers.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\n\u001b[0;31mNameError\u001b[0m: name 'df' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'df' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"abfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/curated/top_drivers.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dfb67df-e551-4e56-967a-b746446c1613",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-6183947907640503>, line 1\u001b[0m\n",
       "\u001b[0;32m----> 1\u001b[0m top_drivers_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n",
       "\u001b[1;32m      2\u001b[0m \u001b[38;5;124m  SELECT \u001b[39m\n",
       "\u001b[1;32m      3\u001b[0m \u001b[38;5;124m    d.driver_id,\u001b[39m\n",
       "\u001b[1;32m      4\u001b[0m \u001b[38;5;124m    d.name AS driver_name,\u001b[39m\n",
       "\u001b[1;32m      5\u001b[0m \u001b[38;5;124m    SUM(p.gallons) AS total_gallons,\u001b[39m\n",
       "\u001b[1;32m      6\u001b[0m \u001b[38;5;124m    COUNT(p.load_id) AS deliveries\u001b[39m\n",
       "\u001b[1;32m      7\u001b[0m \u001b[38;5;124m  FROM pump_loads p\u001b[39m\n",
       "\u001b[1;32m      8\u001b[0m \u001b[38;5;124m  JOIN drivers d ON p.driver_id = d.driver_id\u001b[39m\n",
       "\u001b[1;32m      9\u001b[0m \u001b[38;5;124m  GROUP BY d.driver_id, d.name\u001b[39m\n",
       "\u001b[1;32m     10\u001b[0m \u001b[38;5;124m  ORDER BY total_gallons DESC\u001b[39m\n",
       "\u001b[1;32m     11\u001b[0m \u001b[38;5;124m  LIMIT 5\u001b[39m\n",
       "\u001b[1;32m     12\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n",
       "\u001b[1;32m     14\u001b[0m top_drivers_df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/curated/top_drivers.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n",
       "\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n",
       "\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n",
       "\u001b[1;32m     51\u001b[0m     )\n",
       "\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/session.py:1602\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m   1598\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[1;32m   1599\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n",
       "\u001b[1;32m   1600\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n",
       "\u001b[1;32m   1601\u001b[0m         )\n",
       "\u001b[0;32m-> 1602\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n",
       "\u001b[1;32m   1603\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
       "\u001b[1;32m   1604\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1349\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1351\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1352\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n",
       "\u001b[1;32m   1354\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
       "\u001b[0;32m-> 1355\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n",
       "\u001b[1;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
       "\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:194\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    190\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
       "\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n",
       "\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n",
       "\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n",
       "\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
       "\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
       "\n",
       "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `pump_loads` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
       "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
       "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 7 pos 7;\n",
       "'GlobalLimit 5\n",
       "+- 'LocalLimit 5\n",
       "   +- 'Sort ['total_gallons DESC NULLS LAST], true\n",
       "      +- 'Aggregate ['d.driver_id, 'd.name], ['d.driver_id, 'd.name AS driver_name#369, 'SUM('p.gallons) AS total_gallons#370, 'COUNT('p.load_id) AS deliveries#371]\n",
       "         +- 'Join Inner, ('p.driver_id = 'd.driver_id)\n",
       "            :- 'SubqueryAlias p\n",
       "            :  +- 'UnresolvedRelation [pump_loads], [], false\n",
       "            +- 'SubqueryAlias d\n",
       "               +- 'UnresolvedRelation [drivers], [], false\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\nFile \u001b[0;32m<command-6183947907640503>, line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m top_drivers_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124m  SELECT \u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m    d.driver_id,\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m    d.name AS driver_name,\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m    SUM(p.gallons) AS total_gallons,\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m    COUNT(p.load_id) AS deliveries\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m  FROM pump_loads p\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m  JOIN drivers d ON p.driver_id = d.driver_id\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m  GROUP BY d.driver_id, d.name\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m  ORDER BY total_gallons DESC\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m  LIMIT 5\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m top_drivers_df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/curated/top_drivers.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/sql/session.py:1602\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1598\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1600\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1601\u001b[0m         )\n\u001b[0;32m-> 1602\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1603\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1604\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\nFile \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1351\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1352\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1354\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1355\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:194\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    190\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\n\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `pump_loads` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 7 pos 7;\n'GlobalLimit 5\n+- 'LocalLimit 5\n   +- 'Sort ['total_gallons DESC NULLS LAST], true\n      +- 'Aggregate ['d.driver_id, 'd.name], ['d.driver_id, 'd.name AS driver_name#369, 'SUM('p.gallons) AS total_gallons#370, 'COUNT('p.load_id) AS deliveries#371]\n         +- 'Join Inner, ('p.driver_id = 'd.driver_id)\n            :- 'SubqueryAlias p\n            :  +- 'UnresolvedRelation [pump_loads], [], false\n            +- 'SubqueryAlias d\n               +- 'UnresolvedRelation [drivers], [], false\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `pump_loads` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 7 pos 7;\n'GlobalLimit 5\n+- 'LocalLimit 5\n   +- 'Sort ['total_gallons DESC NULLS LAST], true\n      +- 'Aggregate ['d.driver_id, 'd.name], ['d.driver_id, 'd.name AS driver_name#369, 'SUM('p.gallons) AS total_gallons#370, 'COUNT('p.load_id) AS deliveries#371]\n         +- 'Join Inner, ('p.driver_id = 'd.driver_id)\n            :- 'SubqueryAlias p\n            :  +- 'UnresolvedRelation [pump_loads], [], false\n            +- 'SubqueryAlias d\n               +- 'UnresolvedRelation [drivers], [], false\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_drivers_df = spark.sql(\"\"\"\n",
    "  SELECT \n",
    "    d.driver_id,\n",
    "    d.name AS driver_name,\n",
    "    SUM(p.gallons) AS total_gallons,\n",
    "    COUNT(p.load_id) AS deliveries\n",
    "  FROM pump_loads p\n",
    "  JOIN drivers d ON p.driver_id = d.driver_id\n",
    "  GROUP BY d.driver_id, d.name\n",
    "  ORDER BY total_gallons DESC\n",
    "  LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "top_drivers_df.write.mode(\"overwrite\").parquet(\"abfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/curated/top_drivers.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ec57175-4d7c-4578-aa9a-4a371330239e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-6183947907640504>, line 2\u001b[0m\n",
       "\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load data from Data Lake\u001b[39;00m\n",
       "\u001b[0;32m----> 2\u001b[0m drivers \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/drivers.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
       "\u001b[1;32m      3\u001b[0m pump_loads \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/pump_loads.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n",
       "\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n",
       "\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n",
       "\u001b[1;32m     51\u001b[0m     )\n",
       "\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n",
       "\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
       "\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
       "\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n",
       "\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1349\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1351\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1352\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n",
       "\u001b[1;32m   1354\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
       "\u001b[0;32m-> 1355\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n",
       "\u001b[1;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
       "\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:188\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
       "\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m--> 188\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
       "\u001b[1;32m    190\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n",
       "\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n",
       "\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n",
       "\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n",
       "\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
       "\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n",
       "\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n",
       "\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
       "\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
       "\n",
       "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o401.csv.\n",
       ": Failure to initialize configuration for storage account fuelintelligencestorage.dfs.core.windows.net: Invalid configuration value detected for fs.azure.account.keyInvalid configuration value detected for fs.azure.account.key\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:52)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AbfsConfiguration.getStorageAccountKey(AbfsConfiguration.java:686)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.initializeClient(AzureBlobFileSystemStore.java:2061)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.<init>(AzureBlobFileSystemStore.java:266)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.initialize(AzureBlobFileSystem.java:230)\n",
       "\tat com.databricks.common.filesystem.LokiABFS.initialize(LokiABFS.scala:36)\n",
       "\tat com.databricks.common.filesystem.LokiFileSystem$.$anonfun$getLokiFS$1(LokiFileSystem.scala:144)\n",
       "\tat com.databricks.common.filesystem.Cache.getOrCompute(Cache.scala:38)\n",
       "\tat com.databricks.common.filesystem.LokiFileSystem$.getLokiFS(LokiFileSystem.scala:141)\n",
       "\tat com.databricks.common.filesystem.LokiFileSystem.$anonfun$initialize$1(LokiFileSystem.scala:191)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.common.filesystem.LokiFileSystem.initialize(LokiFileSystem.scala:179)\n",
       "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n",
       "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)\n",
       "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
       "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)\n",
       "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:381)\n",
       "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:337)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:337)\n",
       "\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:740)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: Invalid configuration value detected for fs.azure.account.key\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.ConfigurationBasicValidator.validate(ConfigurationBasicValidator.java:49)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.Base64StringConfigurationBasicValidator.validate(Base64StringConfigurationBasicValidator.java:40)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.validateStorageAccountKey(SimpleKeyProvider.java:71)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:49)\n",
       "\t... 34 more\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\nFile \u001b[0;32m<command-6183947907640504>, line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load data from Data Lake\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m drivers \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/drivers.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m pump_loads \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/pump_loads.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n\nFile \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1351\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1352\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1354\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1355\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:188\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 188\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    190\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\nFile \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\n\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o401.csv.\n: Failure to initialize configuration for storage account fuelintelligencestorage.dfs.core.windows.net: Invalid configuration value detected for fs.azure.account.keyInvalid configuration value detected for fs.azure.account.key\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:52)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AbfsConfiguration.getStorageAccountKey(AbfsConfiguration.java:686)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.initializeClient(AzureBlobFileSystemStore.java:2061)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.<init>(AzureBlobFileSystemStore.java:266)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.initialize(AzureBlobFileSystem.java:230)\n\tat com.databricks.common.filesystem.LokiABFS.initialize(LokiABFS.scala:36)\n\tat com.databricks.common.filesystem.LokiFileSystem$.$anonfun$getLokiFS$1(LokiFileSystem.scala:144)\n\tat com.databricks.common.filesystem.Cache.getOrCompute(Cache.scala:38)\n\tat com.databricks.common.filesystem.LokiFileSystem$.getLokiFS(LokiFileSystem.scala:141)\n\tat com.databricks.common.filesystem.LokiFileSystem.$anonfun$initialize$1(LokiFileSystem.scala:191)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.common.filesystem.LokiFileSystem.initialize(LokiFileSystem.scala:179)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:381)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:337)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:337)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:740)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: Invalid configuration value detected for fs.azure.account.key\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.ConfigurationBasicValidator.validate(ConfigurationBasicValidator.java:49)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.Base64StringConfigurationBasicValidator.validate(Base64StringConfigurationBasicValidator.java:40)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.validateStorageAccountKey(SimpleKeyProvider.java:71)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:49)\n\t... 34 more\n",
       "errorSummary": "Failure to initialize configuration for storage account fuelintelligencestorage.dfs.core.windows.net: Invalid configuration value detected for fs.azure.account.keyInvalid configuration value detected for fs.azure.account.key",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data from Data Lake\n",
    "drivers = spark.read.option(\"header\", True).csv(\"abfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/drivers.csv\")\n",
    "pump_loads = spark.read.option(\"header\", True).csv(\"abfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/pump_loads.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aa677b2-36aa-4677-b911-d35ee11130e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set OAuth config for accessing Azure Data Lake Gen2\n",
    "spark.conf.set(\"fs.azure.account.auth.type.fuelintelligencestorage.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.fuelintelligencestorage.dfs.core.windows.net\", \n",
    "               \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.fuelintelligencestorage.dfs.core.windows.net\", \n",
    "               \"42415ee4-49b0-4c60-bc55-7ebb8a6bcce5\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.fuelintelligencestorage.dfs.core.windows.net\", \n",
    "               \"bD58Q~Ga_rtXiLOP31ZHbasGtm4~H0HO5fxfTaI8\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.fuelintelligencestorage.dfs.core.windows.net\", \n",
    "               \"https://login.microsoftonline.com/3ba40b32-cd04-4c04-950e-b9fdf6fc7d9e/oauth2/token\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa0c4a58-d56d-43db-b498-270a8efa6e99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-6183947907640506>, line 2\u001b[0m\n",
       "\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Validate connection to Azure Data Lake\u001b[39;00m\n",
       "\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/drivers.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
       "\u001b[1;32m      3\u001b[0m display(df)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n",
       "\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n",
       "\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n",
       "\u001b[1;32m     51\u001b[0m     )\n",
       "\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n",
       "\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
       "\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
       "\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n",
       "\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1349\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1351\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1352\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n",
       "\u001b[1;32m   1354\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
       "\u001b[0;32m-> 1355\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n",
       "\u001b[1;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
       "\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:188\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
       "\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m--> 188\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
       "\u001b[1;32m    190\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n",
       "\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n",
       "\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n",
       "\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n",
       "\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
       "\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n",
       "\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n",
       "\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
       "\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
       "\n",
       "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o419.csv.\n",
       ": Operation failed: \"This request is not authorized to perform this operation using this permission.\", 403, HEAD, https://fuelintelligencestorage.dfs.core.windows.net/fueldata/drivers.csv?upn=false&action=getStatus&timeout=90\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.completeExecute(AbfsRestOperation.java:264)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.lambda$execute$0(AbfsRestOperation.java:211)\n",
       "\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation(IOStatisticsBinding.java:464)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:209)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsClient.getPathStatus(AbfsClient.java:979)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getFileStatus(AzureBlobFileSystemStore.java:1125)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:901)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:891)\n",
       "\tat com.databricks.common.filesystem.LokiABFS.getFileStatusNoCache(LokiABFS.scala:52)\n",
       "\tat com.databricks.common.filesystem.LokiABFS.getFileStatus(LokiABFS.scala:42)\n",
       "\tat com.databricks.common.filesystem.LokiFileSystem.getFileStatus(LokiFileSystem.scala:263)\n",
       "\tat org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1777)\n",
       "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:60)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)\n",
       "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:381)\n",
       "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:337)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:337)\n",
       "\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:740)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\nFile \u001b[0;32m<command-6183947907640506>, line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Validate connection to Azure Data Lake\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/drivers.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m display(df)\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n\nFile \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1351\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1352\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1354\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1355\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:188\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 188\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    190\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\nFile \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\n\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o419.csv.\n: Operation failed: \"This request is not authorized to perform this operation using this permission.\", 403, HEAD, https://fuelintelligencestorage.dfs.core.windows.net/fueldata/drivers.csv?upn=false&action=getStatus&timeout=90\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.completeExecute(AbfsRestOperation.java:264)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.lambda$execute$0(AbfsRestOperation.java:211)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfInvocation(IOStatisticsBinding.java:464)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:209)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsClient.getPathStatus(AbfsClient.java:979)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getFileStatus(AzureBlobFileSystemStore.java:1125)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:901)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.getFileStatus(AzureBlobFileSystem.java:891)\n\tat com.databricks.common.filesystem.LokiABFS.getFileStatusNoCache(LokiABFS.scala:52)\n\tat com.databricks.common.filesystem.LokiABFS.getFileStatus(LokiABFS.scala:42)\n\tat com.databricks.common.filesystem.LokiFileSystem.getFileStatus(LokiFileSystem.scala:263)\n\tat org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1777)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:60)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:381)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:337)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:337)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:740)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\n",
       "errorSummary": "Operation failed: \"This request is not authorized to perform this operation using this permission.\", 403, HEAD, https://fuelintelligencestorage.dfs.core.windows.net/fueldata/drivers.csv?upn=false&action=getStatus&timeout=90",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Validate connection to Azure Data Lake\n",
    "df = spark.read.option(\"header\", True).csv(\"abfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/drivers.csv\")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6fdfb36-0385-41af-b91a-16885bb90ffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.azure.account.auth.type.fuelintelligencestorage.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.fuelintelligencestorage.dfs.core.windows.net\", \n",
    "               \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.fuelintelligencestorage.dfs.core.windows.net\", \n",
    "               \"42415ee4-49b0-4c60-bc55-7ebb8a6bcce5\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.fuelintelligencestorage.dfs.core.windows.net\", \n",
    "               \"bD58Q~Ga_rtXiLOP31ZHbasGtm4~H0HO5fxfTaI8\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.fuelintelligencestorage.dfs.core.windows.net\", \n",
    "               \"https://login.microsoftonline.com/3ba40b32-cd04-4c04-950e-b9fdf6fc7d9e/oauth2/token\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ef5ec66-f76e-4c88-909c-c4febc176bf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+--------------+----------------+\n",
      "|driver_id|              name|license_number|experience_years|\n",
      "+---------+------------------+--------------+----------------+\n",
      "|     D001| Mitchell Erickson|      uL533961|               2|\n",
      "|     D002|    Eric Maldonado|      Cf347080|               8|\n",
      "|     D003|       Jenny Kelly|      sa456491|              13|\n",
      "|     D004|    Ruben Gonzalez|      vF178322|              11|\n",
      "|     D005|    Robert Lambert|      pW399436|              19|\n",
      "|     D006|         Ana Baker|      vV188768|               2|\n",
      "|     D007|      Michael Lamb|      oY666058|               3|\n",
      "|     D008|   Christina Jones|      FG458396|              19|\n",
      "|     D009|   Lawrence Flores|      yD846849|              13|\n",
      "|     D010|     Jacob Flowers|      Qs404480|               7|\n",
      "|     D011|Alexander Anderson|      WG182160|               9|\n",
      "|     D012|     James Schmidt|      wl226169|              18|\n",
      "|     D013|        Ryan Myers|      EN382710|               6|\n",
      "|     D014|    Jennifer Allen|      jU336695|              20|\n",
      "|     D015|   Phillip Morales|      QO966263|              20|\n",
      "|     D016|    Tanner Morales|      vP248328|               3|\n",
      "|     D017|   Teresa Reynolds|      SK309094|              15|\n",
      "|     D018|     Carolyn Jones|      YZ016037|              14|\n",
      "|     D019|  Jared Howell Jr.|      ZP938622|              10|\n",
      "|     D020|      Vincent Carr|      Sk649981|              14|\n",
      "+---------+------------------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", True).csv(\"abfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/drivers.csv\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c149582c-6ade-40a6-a37a-f902efbf5627",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fuels = spark.read.option(\"header\", True).csv(\"abfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/fuels.csv\")\n",
    "clients = spark.read.option(\"header\", True).csv(\"abfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/clients.csv\")\n",
    "pump_loads = spark.read.option(\"header\", True).csv(\"abfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/pump_loads.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a290be9-26ad-435c-91e9-410e0e3f21c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;36m  File \u001b[0;32m<command-8947135195967925>, line 1\u001b[0;36m\u001b[0m\n",
       "\u001b[0;31m    abfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/curated/\u001b[0m\n",
       "\u001b[0m          ^\u001b[0m\n",
       "\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;36m  File \u001b[0;32m<command-8947135195967925>, line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    abfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/curated/\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n",
       "errorSummary": "<span class='ansi-red-fg'>SyntaxError</span>: invalid syntax (command-8947135195967925-3501607736, line 1)",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "abfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/curated/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d18a5a23-98d2-4a8d-aeaa-d8c3fe2e4655",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-8947135195967926>, line 1\u001b[0m\n",
       "\u001b[0;32m----> 1\u001b[0m top_drivers_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n",
       "\u001b[1;32m      2\u001b[0m \u001b[38;5;124mSELECT \u001b[39m\n",
       "\u001b[1;32m      3\u001b[0m \u001b[38;5;124m    d.driver_id,\u001b[39m\n",
       "\u001b[1;32m      4\u001b[0m \u001b[38;5;124m    d.name AS driver_name,\u001b[39m\n",
       "\u001b[1;32m      5\u001b[0m \u001b[38;5;124m    SUM(p.gallons_loaded) AS total_gallons,\u001b[39m\n",
       "\u001b[1;32m      6\u001b[0m \u001b[38;5;124m    COUNT(p.load_id) AS total_deliveries\u001b[39m\n",
       "\u001b[1;32m      7\u001b[0m \u001b[38;5;124mFROM pump_loads p\u001b[39m\n",
       "\u001b[1;32m      8\u001b[0m \u001b[38;5;124mJOIN drivers d ON d.driver_id = p.driver_id\u001b[39m\n",
       "\u001b[1;32m      9\u001b[0m \u001b[38;5;124mGROUP BY d.driver_id, d.name\u001b[39m\n",
       "\u001b[1;32m     10\u001b[0m \u001b[38;5;124mORDER BY total_gallons DESC\u001b[39m\n",
       "\u001b[1;32m     11\u001b[0m \u001b[38;5;124mLIMIT 5\u001b[39m\n",
       "\u001b[1;32m     12\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n",
       "\u001b[1;32m     14\u001b[0m top_drivers_df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/curated/top_5_drivers.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n",
       "\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n",
       "\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n",
       "\u001b[1;32m     51\u001b[0m     )\n",
       "\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/session.py:1602\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m   1598\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[1;32m   1599\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n",
       "\u001b[1;32m   1600\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n",
       "\u001b[1;32m   1601\u001b[0m         )\n",
       "\u001b[0;32m-> 1602\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n",
       "\u001b[1;32m   1603\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
       "\u001b[1;32m   1604\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1349\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1351\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1352\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n",
       "\u001b[1;32m   1354\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
       "\u001b[0;32m-> 1355\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n",
       "\u001b[1;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
       "\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:194\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    190\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
       "\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n",
       "\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n",
       "\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n",
       "\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
       "\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
       "\n",
       "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `pump_loads` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
       "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
       "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 7 pos 5;\n",
       "'GlobalLimit 5\n",
       "+- 'LocalLimit 5\n",
       "   +- 'Sort ['total_gallons DESC NULLS LAST], true\n",
       "      +- 'Aggregate ['d.driver_id, 'd.name], ['d.driver_id, 'd.name AS driver_name#134, 'SUM('p.gallons_loaded) AS total_gallons#135, 'COUNT('p.load_id) AS total_deliveries#136]\n",
       "         +- 'Join Inner, ('d.driver_id = 'p.driver_id)\n",
       "            :- 'SubqueryAlias p\n",
       "            :  +- 'UnresolvedRelation [pump_loads], [], false\n",
       "            +- 'SubqueryAlias d\n",
       "               +- 'UnresolvedRelation [drivers], [], false\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\nFile \u001b[0;32m<command-8947135195967926>, line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m top_drivers_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mSELECT \u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m    d.driver_id,\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m    d.name AS driver_name,\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m    SUM(p.gallons_loaded) AS total_gallons,\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m    COUNT(p.load_id) AS total_deliveries\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124mFROM pump_loads p\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124mJOIN drivers d ON d.driver_id = p.driver_id\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124mGROUP BY d.driver_id, d.name\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124mORDER BY total_gallons DESC\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124mLIMIT 5\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m top_drivers_df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/curated/top_5_drivers.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/sql/session.py:1602\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1598\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1600\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1601\u001b[0m         )\n\u001b[0;32m-> 1602\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1603\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1604\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\nFile \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1351\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1352\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1354\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1355\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:194\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    190\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\n\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `pump_loads` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 7 pos 5;\n'GlobalLimit 5\n+- 'LocalLimit 5\n   +- 'Sort ['total_gallons DESC NULLS LAST], true\n      +- 'Aggregate ['d.driver_id, 'd.name], ['d.driver_id, 'd.name AS driver_name#134, 'SUM('p.gallons_loaded) AS total_gallons#135, 'COUNT('p.load_id) AS total_deliveries#136]\n         +- 'Join Inner, ('d.driver_id = 'p.driver_id)\n            :- 'SubqueryAlias p\n            :  +- 'UnresolvedRelation [pump_loads], [], false\n            +- 'SubqueryAlias d\n               +- 'UnresolvedRelation [drivers], [], false\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `pump_loads` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 7 pos 5;\n'GlobalLimit 5\n+- 'LocalLimit 5\n   +- 'Sort ['total_gallons DESC NULLS LAST], true\n      +- 'Aggregate ['d.driver_id, 'd.name], ['d.driver_id, 'd.name AS driver_name#134, 'SUM('p.gallons_loaded) AS total_gallons#135, 'COUNT('p.load_id) AS total_deliveries#136]\n         +- 'Join Inner, ('d.driver_id = 'p.driver_id)\n            :- 'SubqueryAlias p\n            :  +- 'UnresolvedRelation [pump_loads], [], false\n            +- 'SubqueryAlias d\n               +- 'UnresolvedRelation [drivers], [], false\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_drivers_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    d.driver_id,\n",
    "    d.name AS driver_name,\n",
    "    SUM(p.gallons_loaded) AS total_gallons,\n",
    "    COUNT(p.load_id) AS total_deliveries\n",
    "FROM pump_loads p\n",
    "JOIN drivers d ON d.driver_id = p.driver_id\n",
    "GROUP BY d.driver_id, d.name\n",
    "ORDER BY total_gallons DESC\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "top_drivers_df.write.mode(\"overwrite\").parquet(\"abfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/curated/top_5_drivers.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cab50b5b-6cb4-4a8d-8f2d-53058cbe18c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-8947135195967927>, line 1\u001b[0m\n",
       "\u001b[0;32m----> 1\u001b[0m top_drivers_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n",
       "\u001b[1;32m      2\u001b[0m \u001b[38;5;124mSELECT \u001b[39m\n",
       "\u001b[1;32m      3\u001b[0m \u001b[38;5;124m    d.driver_id,\u001b[39m\n",
       "\u001b[1;32m      4\u001b[0m \u001b[38;5;124m    d.name AS driver_name,\u001b[39m\n",
       "\u001b[1;32m      5\u001b[0m \u001b[38;5;124m    SUM(p.gallons_loaded) AS total_gallons,\u001b[39m\n",
       "\u001b[1;32m      6\u001b[0m \u001b[38;5;124m    COUNT(p.load_id) AS total_deliveries\u001b[39m\n",
       "\u001b[1;32m      7\u001b[0m \u001b[38;5;124mFROM pump_loads p\u001b[39m\n",
       "\u001b[1;32m      8\u001b[0m \u001b[38;5;124mJOIN drivers d ON d.driver_id = p.driver_id\u001b[39m\n",
       "\u001b[1;32m      9\u001b[0m \u001b[38;5;124mGROUP BY d.driver_id, d.name\u001b[39m\n",
       "\u001b[1;32m     10\u001b[0m \u001b[38;5;124mORDER BY total_gallons DESC\u001b[39m\n",
       "\u001b[1;32m     11\u001b[0m \u001b[38;5;124mLIMIT 5\u001b[39m\n",
       "\u001b[1;32m     12\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n",
       "\u001b[1;32m     14\u001b[0m top_drivers_df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/curated/top_5_drivers.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n",
       "\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n",
       "\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n",
       "\u001b[1;32m     51\u001b[0m     )\n",
       "\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/session.py:1602\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m   1598\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[1;32m   1599\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n",
       "\u001b[1;32m   1600\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n",
       "\u001b[1;32m   1601\u001b[0m         )\n",
       "\u001b[0;32m-> 1602\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n",
       "\u001b[1;32m   1603\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
       "\u001b[1;32m   1604\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1349\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1351\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1352\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n",
       "\u001b[1;32m   1354\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
       "\u001b[0;32m-> 1355\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n",
       "\u001b[1;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
       "\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:194\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    190\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
       "\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n",
       "\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n",
       "\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n",
       "\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
       "\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
       "\n",
       "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `pump_loads` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
       "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
       "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 7 pos 5;\n",
       "'GlobalLimit 5\n",
       "+- 'LocalLimit 5\n",
       "   +- 'Sort ['total_gallons DESC NULLS LAST], true\n",
       "      +- 'Aggregate ['d.driver_id, 'd.name], ['d.driver_id, 'd.name AS driver_name#322, 'SUM('p.gallons_loaded) AS total_gallons#323, 'COUNT('p.load_id) AS total_deliveries#324]\n",
       "         +- 'Join Inner, ('d.driver_id = 'p.driver_id)\n",
       "            :- 'SubqueryAlias p\n",
       "            :  +- 'UnresolvedRelation [pump_loads], [], false\n",
       "            +- 'SubqueryAlias d\n",
       "               +- 'UnresolvedRelation [drivers], [], false\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\nFile \u001b[0;32m<command-8947135195967927>, line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m top_drivers_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mSELECT \u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m    d.driver_id,\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m    d.name AS driver_name,\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m    SUM(p.gallons_loaded) AS total_gallons,\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m    COUNT(p.load_id) AS total_deliveries\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124mFROM pump_loads p\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124mJOIN drivers d ON d.driver_id = p.driver_id\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124mGROUP BY d.driver_id, d.name\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124mORDER BY total_gallons DESC\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124mLIMIT 5\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m top_drivers_df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/curated/top_5_drivers.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/sql/session.py:1602\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1598\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1600\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1601\u001b[0m         )\n\u001b[0;32m-> 1602\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1603\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1604\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\nFile \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1351\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1352\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1354\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1355\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:194\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    190\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\n\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `pump_loads` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 7 pos 5;\n'GlobalLimit 5\n+- 'LocalLimit 5\n   +- 'Sort ['total_gallons DESC NULLS LAST], true\n      +- 'Aggregate ['d.driver_id, 'd.name], ['d.driver_id, 'd.name AS driver_name#322, 'SUM('p.gallons_loaded) AS total_gallons#323, 'COUNT('p.load_id) AS total_deliveries#324]\n         +- 'Join Inner, ('d.driver_id = 'p.driver_id)\n            :- 'SubqueryAlias p\n            :  +- 'UnresolvedRelation [pump_loads], [], false\n            +- 'SubqueryAlias d\n               +- 'UnresolvedRelation [drivers], [], false\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `pump_loads` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 7 pos 5;\n'GlobalLimit 5\n+- 'LocalLimit 5\n   +- 'Sort ['total_gallons DESC NULLS LAST], true\n      +- 'Aggregate ['d.driver_id, 'd.name], ['d.driver_id, 'd.name AS driver_name#322, 'SUM('p.gallons_loaded) AS total_gallons#323, 'COUNT('p.load_id) AS total_deliveries#324]\n         +- 'Join Inner, ('d.driver_id = 'p.driver_id)\n            :- 'SubqueryAlias p\n            :  +- 'UnresolvedRelation [pump_loads], [], false\n            +- 'SubqueryAlias d\n               +- 'UnresolvedRelation [drivers], [], false\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_drivers_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    d.driver_id,\n",
    "    d.name AS driver_name,\n",
    "    SUM(p.gallons_loaded) AS total_gallons,\n",
    "    COUNT(p.load_id) AS total_deliveries\n",
    "FROM pump_loads p\n",
    "JOIN drivers d ON d.driver_id = p.driver_id\n",
    "GROUP BY d.driver_id, d.name\n",
    "ORDER BY total_gallons DESC\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "top_drivers_df.write.mode(\"overwrite\").parquet(\"abfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/curated/top_5_drivers.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d4ead32-65e7-429e-8eb5-63b83eeb04b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load CSVs from Data Lake\n",
    "drivers = spark.read.option(\"header\", True).csv(\"abfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/drivers.csv\")\n",
    "pump_loads = spark.read.option(\"header\", True).csv(\"abfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/pump_loads.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08bd4629-8b8d-4c3c-ac46-ab259aa5ddb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register as temporary views\n",
    "drivers.createOrReplaceTempView(\"drivers\")\n",
    "pump_loads.createOrReplaceTempView(\"pump_loads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3acff398-928e-49ed-a781-9c4f3939fc89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_drivers_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    d.driver_id,\n",
    "    d.name AS driver_name,\n",
    "    SUM(p.gallons_loaded) AS total_gallons,\n",
    "    COUNT(p.load_id) AS total_deliveries\n",
    "FROM pump_loads p\n",
    "JOIN drivers d ON d.driver_id = p.driver_id\n",
    "GROUP BY d.driver_id, d.name\n",
    "ORDER BY total_gallons DESC\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "top_drivers_df.write.mode(\"overwrite\").parquet(\"abfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/curated/top_5_drivers.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b388d537-e8d2-499c-b93e-932ff99d293e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-8947135195967931>, line 1\u001b[0m\n",
       "\u001b[0;32m----> 1\u001b[0m fuel_cost_per_client \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n",
       "\u001b[1;32m      2\u001b[0m \u001b[38;5;124mSELECT \u001b[39m\n",
       "\u001b[1;32m      3\u001b[0m \u001b[38;5;124m    c.client_id,\u001b[39m\n",
       "\u001b[1;32m      4\u001b[0m \u001b[38;5;124m    c.name AS client_name,\u001b[39m\n",
       "\u001b[1;32m      5\u001b[0m \u001b[38;5;124m    SUM(p.total_fuel_cost) AS total_spent,\u001b[39m\n",
       "\u001b[1;32m      6\u001b[0m \u001b[38;5;124m    COUNT(p.load_id) AS total_deliveries\u001b[39m\n",
       "\u001b[1;32m      7\u001b[0m \u001b[38;5;124mFROM pump_loads p\u001b[39m\n",
       "\u001b[1;32m      8\u001b[0m \u001b[38;5;124mJOIN clients c ON c.client_id = p.client_id\u001b[39m\n",
       "\u001b[1;32m      9\u001b[0m \u001b[38;5;124mGROUP BY c.client_id, c.name\u001b[39m\n",
       "\u001b[1;32m     10\u001b[0m \u001b[38;5;124mORDER BY total_spent DESC\u001b[39m\n",
       "\u001b[1;32m     11\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n",
       "\u001b[1;32m     13\u001b[0m fuel_cost_per_client\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/curated/fuel_cost_per_client.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n",
       "\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n",
       "\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n",
       "\u001b[1;32m     51\u001b[0m     )\n",
       "\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/session.py:1602\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m   1598\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[1;32m   1599\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n",
       "\u001b[1;32m   1600\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n",
       "\u001b[1;32m   1601\u001b[0m         )\n",
       "\u001b[0;32m-> 1602\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n",
       "\u001b[1;32m   1603\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
       "\u001b[1;32m   1604\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n",
       "\u001b[1;32m   1349\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1351\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n",
       "\u001b[1;32m   1352\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n",
       "\u001b[1;32m   1354\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
       "\u001b[0;32m-> 1355\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n",
       "\u001b[1;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
       "\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
       "\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:194\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n",
       "\u001b[1;32m    190\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
       "\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n",
       "\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n",
       "\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n",
       "\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
       "\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
       "\n",
       "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `clients` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
       "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
       "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 8 pos 5;\n",
       "'Sort ['total_spent DESC NULLS LAST], true\n",
       "+- 'Aggregate ['c.client_id, 'c.name], ['c.client_id, 'c.name AS client_name#428, 'SUM('p.total_fuel_cost) AS total_spent#429, 'COUNT('p.load_id) AS total_deliveries#430]\n",
       "   +- 'Join Inner, ('c.client_id = 'p.client_id)\n",
       "      :- SubqueryAlias p\n",
       "      :  +- SubqueryAlias pump_loads\n",
       "      :     +- View (`pump_loads`, [load_id#367,driver_id#368,client_id#369,fuel_id#370,gallons_loaded#371,state_tax#372,excise_tax#373,note#374,timestamp#375])\n",
       "      :        +- Relation [load_id#367,driver_id#368,client_id#369,fuel_id#370,gallons_loaded#371,state_tax#372,excise_tax#373,note#374,timestamp#375] csv\n",
       "      +- 'SubqueryAlias c\n",
       "         +- 'UnresolvedRelation [clients], [], false\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\nFile \u001b[0;32m<command-8947135195967931>, line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fuel_cost_per_client \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mSELECT \u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m    c.client_id,\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m    c.name AS client_name,\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m    SUM(p.total_fuel_cost) AS total_spent,\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m    COUNT(p.load_id) AS total_deliveries\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124mFROM pump_loads p\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124mJOIN clients c ON c.client_id = p.client_id\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124mGROUP BY c.client_id, c.name\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124mORDER BY total_spent DESC\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m fuel_cost_per_client\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/curated/fuel_cost_per_client.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001b[0m, in \u001b[0;36m_wrap_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog_success(\n\u001b[1;32m     50\u001b[0m         module_name, class_name, function_name, time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start, signature\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/sql/session.py:1602\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1598\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1600\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1601\u001b[0m         )\n\u001b[0;32m-> 1602\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1603\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1604\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\nFile \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1351\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1352\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1354\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1355\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:194\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    190\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\n\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `clients` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 8 pos 5;\n'Sort ['total_spent DESC NULLS LAST], true\n+- 'Aggregate ['c.client_id, 'c.name], ['c.client_id, 'c.name AS client_name#428, 'SUM('p.total_fuel_cost) AS total_spent#429, 'COUNT('p.load_id) AS total_deliveries#430]\n   +- 'Join Inner, ('c.client_id = 'p.client_id)\n      :- SubqueryAlias p\n      :  +- SubqueryAlias pump_loads\n      :     +- View (`pump_loads`, [load_id#367,driver_id#368,client_id#369,fuel_id#370,gallons_loaded#371,state_tax#372,excise_tax#373,note#374,timestamp#375])\n      :        +- Relation [load_id#367,driver_id#368,client_id#369,fuel_id#370,gallons_loaded#371,state_tax#372,excise_tax#373,note#374,timestamp#375] csv\n      +- 'SubqueryAlias c\n         +- 'UnresolvedRelation [clients], [], false\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `clients` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 8 pos 5;\n'Sort ['total_spent DESC NULLS LAST], true\n+- 'Aggregate ['c.client_id, 'c.name], ['c.client_id, 'c.name AS client_name#428, 'SUM('p.total_fuel_cost) AS total_spent#429, 'COUNT('p.load_id) AS total_deliveries#430]\n   +- 'Join Inner, ('c.client_id = 'p.client_id)\n      :- SubqueryAlias p\n      :  +- SubqueryAlias pump_loads\n      :     +- View (`pump_loads`, [load_id#367,driver_id#368,client_id#369,fuel_id#370,gallons_loaded#371,state_tax#372,excise_tax#373,note#374,timestamp#375])\n      :        +- Relation [load_id#367,driver_id#368,client_id#369,fuel_id#370,gallons_loaded#371,state_tax#372,excise_tax#373,note#374,timestamp#375] csv\n      +- 'SubqueryAlias c\n         +- 'UnresolvedRelation [clients], [], false\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fuel_cost_per_client = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    c.client_id,\n",
    "    c.name AS client_name,\n",
    "    SUM(p.total_fuel_cost) AS total_spent,\n",
    "    COUNT(p.load_id) AS total_deliveries\n",
    "FROM pump_loads p\n",
    "JOIN clients c ON c.client_id = p.client_id\n",
    "GROUP BY c.client_id, c.name\n",
    "ORDER BY total_spent DESC\n",
    "\"\"\")\n",
    "\n",
    "fuel_cost_per_client.write.mode(\"overwrite\").parquet(\"abfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/curated/fuel_cost_per_client.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e95c9fd-9334-48d6-b318-4020355c128d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load CSVs\n",
    "clients = spark.read.option(\"header\", True).csv(\"abfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/clients.csv\")\n",
    "fuels = spark.read.option(\"header\", True).csv(\"abfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/fuels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f41fc98c-2e1c-42bd-9f33-ef8a1e255576",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register as temp views\n",
    "clients.createOrReplaceTempView(\"clients\")\n",
    "fuels.createOrReplaceTempView(\"fuels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a5c4d11-073e-4b84-bb8e-deb676474289",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fuel_cost_per_client = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    c.client_id,\n",
    "    c.name AS client_name,\n",
    "    SUM(p.gallons_loaded * f.price_per_gallon) AS total_spent,\n",
    "    COUNT(p.load_id) AS total_deliveries\n",
    "FROM pump_loads p\n",
    "JOIN clients c ON c.client_id = p.client_id\n",
    "JOIN fuels f ON f.fuel_id = p.fuel_id\n",
    "GROUP BY c.client_id, c.name\n",
    "ORDER BY total_spent DESC\n",
    "\"\"\")\n",
    "\n",
    "fuel_cost_per_client.write.mode(\"overwrite\").parquet(\"abfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/curated/fuel_cost_per_client.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50810445-55df-4f5d-9a33-9b62c066e47a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fuel_type_usage = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    f.fuel_type,\n",
    "    COUNT(*) AS load_count,\n",
    "    SUM(p.gallons_loaded) AS total_gallons,\n",
    "    AVG(f.price_per_gallon) AS avg_price\n",
    "FROM pump_loads p\n",
    "JOIN fuels f ON f.fuel_id = p.fuel_id\n",
    "GROUP BY f.fuel_type\n",
    "ORDER BY total_gallons DESC\n",
    "\"\"\")\n",
    "\n",
    "fuel_type_usage.write.mode(\"overwrite\").parquet(\"abfss://fueldata@fuelintelligencestorage.dfs.core.windows.net/curated/fuel_type_usage.parquet\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1_ingest_to_datalake",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
